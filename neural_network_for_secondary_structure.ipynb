{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A feed-forward Neural Network for secondary structure prediction\n",
    "This notebook uses a Neural Network based on code from Stevens and Boucher (2014, Python for Biology CUP). A Predictive network is trained using data on known secondary structure of k-mers of 5 amino-acids taken from a set of PDB structures. Three secondary structure states are defined: H,C, and S. H and S are helix and strand respectively while C is for coil which is a range of structures not with regular H-bonding pattern. \n",
    "\n",
    "Secondary structure is much more complicated than indicated by the simple classification of this data - full details are available from the analysis of the H-bonding arrangements. The program DSSP (https://swift.cmbi.umcn.nl/gv/dssp/DSSP_3.html) is a well-tested approach to this problem. This produces a description of the secondary structure in a known protein structure. For historical reasons DSSP uses E for Strands. A related DSSR program gives RNA secondary structure.\n",
    "\n",
    "It is useful to be able to predict the secondary structure of a protein for which there is only sequence available. One approach would be to align it with a known homologous structure. The approach here is to use the sequence in the neighbourhood of a residue as a basis for a neural network prediction. \n",
    "\n",
    "The network here is a simple three layer feed-forward one. The number of nodes in the hidden layer can be defined by the programmer. But the number of input and output nodes is defined by the sizes of the input and output data vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run this cell to import numpy \n",
    "from numpy import tanh, ones, append, array, zeros, random, sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network function takes input data for the first layer of Network nodes, applies the the first weighted connections to pass the signal to the hidden layer of nodes, then applies the second weights to produce output. \n",
    "\n",
    "The output may not be optimized as the function also operates on the weighting during training. However after training the function gives predictions so takes its name from that. \n",
    "\n",
    "The weightsIn values define the strength of connection between the input nodes and the hidden nodes. Similarly weightsOut define the strengths of connection between the hidden and the output nodes. \n",
    "\n",
    "The wieghts are given as matrices with the rows indexing the nodes in a layer and the columns indexing the nodes in the other layer. \n",
    "\n",
    "The signalIn vector is the input features and an extra value of 1.0. This additional value is called the bias node which is used to tune the baseline response of the network. The baseline is the level without a meaningful signal. \n",
    "\n",
    "Setting the bias node value happens during training to adapt to the values in the input data. This means the input data don't need to pre-prepared with a mean of zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run this cell to define the function\n",
    "def neuralNetPredict(inputVec, weightsIn, weightsOut):\n",
    "    \"\"\" uses the current weights in a neural network\n",
    "    to make a prediction from an input vector\n",
    "    all input and output are numpy data structures\"\"\" \n",
    "    signalIn = append(inputVec, 1.0) # input layer\n",
    "\n",
    "    prod = signalIn * weightsIn.T\n",
    "    sums = sum(prod, axis=1)\n",
    "    signalHid = tanh(sums)    # hidden    layer\n",
    "\n",
    "    prod = signalHid * weightsOut.T\n",
    "    sums = sum(prod, axis=1)\n",
    "    signalOut = tanh(sums)    # output    layer\n",
    "\n",
    "    return signalIn, signalHid, signalOut\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .T function gives the transpose of a matrix - that is the matrix with the columns turned into rows and the rows turned into columns. \n",
    "\n",
    "This is used so that the input signal gets applied to all the hidden nodes.\n",
    "\n",
    "The network applies the hyperbolic tangent function (tanh) to get the signal output from all the nodes in layer. Hyperbolic tan is a sigmoidal function that varies from -1 to 1, so it is much better than ordinary tan that runs off to infinity. The tanh function defines the output of that node given an input or set of inputs. As a nod to the output of neurones, which depends on an activation level across their cell membrane, the output function is called the *Activation* function.\n",
    "\n",
    "In operation only the signalOut from the output layer is of interest. But during training the response signals from the other layers also need to adjust the weighting scheme.\n",
    "\n",
    "### Training \n",
    "The weighting scheme (and gain) will be optimized by using a training dataset. \n",
    "\n",
    "The training data will be an input feature vector and a known output vector. The order of the data will be randomly shuffled to avoid bias. The number of hidden nodes needs to be specified and the number of optimization cycles. \n",
    "\n",
    "After each cycle the 'error' between the output signal of the network and the known training set output is used to adjust the network weights. The difference is combined with the *gradient* in the signal values - calculated from the tanh activation function (conveniently the gradient of tanh(sig) is 1-sig^2 or 1 - (sig x sig)). \n",
    "\n",
    "Early in training large difference can make the network go haywire so the speed of weight changing is damped down by a 'rate' and 'momentum' multipliers (usually the default values of 0.5 and 0.2 are good enough. \n",
    "\n",
    "More damping would mean that many more cycles would be needed for the weights to converge. \n",
    "\n",
    "The training will work back from the value of the error to adjust the weighting scheme of the network. This is called *back propagation*. \n",
    "\n",
    "The use of the gradient is crucial as it means initial adjustments will be large but then finer adjustments will be made as the optimum is approached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run this cell to define the function\n",
    "def neuralNetTrain(trainData, numHid, steps=100, rate=0.5, momentum=0.2):\n",
    "    \"\"\" uses training data to set the weights in a simple\n",
    "    neural network, number of hidden nodes is specified\"\"\"\n",
    "    numInp = len(trainData[0][0])\n",
    "    numOut = len(trainData[0][1])\n",
    "    numInp += 1\n",
    "    minError = None\n",
    "\n",
    "    sigInp = ones(numInp)\n",
    "    sigHid = ones(numHid)\n",
    "    sigOut = ones(numOut)\n",
    "\n",
    "    wInp = random.random((numInp, numHid))-0.5\n",
    "    wOut = random.random((numHid, numOut))-0.5\n",
    "    bestWeightMatrices = (wInp, wOut)\n",
    "\n",
    "    cInp = zeros((numInp, numHid))\n",
    "    cOut = zeros((numHid, numOut))\n",
    "\n",
    "    for x, (inputs, knownOut) in enumerate(trainData):\n",
    "        trainData[x] = (array(inputs), array(knownOut))\n",
    " \n",
    "    for step in range(steps):  \n",
    "        random.shuffle(trainData) # Important to avoid bias\n",
    "        error = 0.0\n",
    " \n",
    "        for inputs, knownOut in trainData:\n",
    "            sigIn, sigHid, sigOut = neuralNetPredict(inputs, wInp, wOut)\n",
    "\n",
    "            diff = knownOut - sigOut\n",
    "            error += sum(diff * diff)\n",
    "\n",
    "            gradient = ones(numOut) - (sigOut*sigOut)\n",
    "            outAdjust = gradient * diff \n",
    "\n",
    "            diff = sum(outAdjust * wOut, axis=1)\n",
    "            gradient = ones(numHid) - (sigHid*sigHid)\n",
    "            hidAdjust = gradient * diff \n",
    "\n",
    "            # update output \n",
    "            change = outAdjust * sigHid.reshape(numHid, 1)\n",
    "            wOut += (rate * change) + (momentum * cOut)\n",
    "            cOut = change\n",
    " \n",
    "            # update input \n",
    "            change = hidAdjust * sigIn.reshape(numInp, 1)\n",
    "            wInp += (rate * change) + (momentum * cInp)\n",
    "            cInp = change\n",
    " \n",
    "        if (minError is None) or (error < minError):\n",
    "            minError = error\n",
    "            bestWeightMatrices = (wInp.copy(), wOut.copy())\n",
    "            print(\"Step: %d Error: %f\" % (step, error))\n",
    "    \n",
    "    return bestWeightMatrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the functions\n",
    "Simple data to test a network can be binary input vectors with the desired output being an 'exclusive OR' (EOR) response. This responds True if any input is true but False is both are together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run this cell to define the test data\n",
    "testEORdata = [[[0,0], [0]],[[0,1], [1]], [[1,0], [1]], [[1,1], [0]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network test uses two hidden nodes - in real use several values would be tried to find the best performance.\n",
    "Run the cell below to see if the training converges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Error: 1.662945\n",
      "Step: 1 Error: 1.216083\n",
      "Step: 3 Error: 1.159888\n",
      "Step: 10 Error: 1.136167\n",
      "Step: 20 Error: 1.102602\n",
      "Step: 21 Error: 1.025619\n",
      "Step: 22 Error: 0.999384\n",
      "Step: 25 Error: 0.934159\n",
      "Step: 28 Error: 0.933513\n",
      "Step: 30 Error: 0.910591\n",
      "Step: 31 Error: 0.892805\n",
      "Step: 39 Error: 0.887306\n",
      "Step: 41 Error: 0.869018\n",
      "Step: 45 Error: 0.849426\n",
      "Step: 52 Error: 0.839845\n",
      "Step: 54 Error: 0.833483\n",
      "Step: 59 Error: 0.832764\n",
      "Step: 60 Error: 0.818643\n",
      "Step: 61 Error: 0.805045\n",
      "Step: 64 Error: 0.799074\n",
      "Step: 66 Error: 0.784809\n",
      "Step: 67 Error: 0.773271\n",
      "Step: 70 Error: 0.754645\n",
      "Step: 71 Error: 0.705676\n",
      "Step: 76 Error: 0.672309\n",
      "Step: 77 Error: 0.649118\n",
      "Step: 81 Error: 0.606411\n",
      "Step: 82 Error: 0.593129\n",
      "Step: 86 Error: 0.536180\n",
      "Step: 88 Error: 0.419359\n",
      "Step: 90 Error: 0.378517\n",
      "Step: 96 Error: 0.348409\n",
      "Step: 99 Error: 0.337254\n",
      "Step: 101 Error: 0.331303\n",
      "Step: 102 Error: 0.263926\n",
      "Step: 103 Error: 0.215745\n",
      "Step: 105 Error: 0.200334\n",
      "Step: 107 Error: 0.184302\n",
      "Step: 108 Error: 0.153578\n",
      "Step: 110 Error: 0.134130\n",
      "Step: 112 Error: 0.093575\n",
      "Step: 122 Error: 0.092292\n",
      "Step: 124 Error: 0.083346\n",
      "Step: 127 Error: 0.054596\n",
      "Step: 134 Error: 0.048636\n",
      "Step: 135 Error: 0.048605\n",
      "Step: 136 Error: 0.033930\n",
      "Step: 151 Error: 0.028907\n",
      "Step: 157 Error: 0.021296\n",
      "Step: 171 Error: 0.019041\n",
      "Step: 184 Error: 0.013928\n",
      "Step: 193 Error: 0.013851\n",
      "Step: 199 Error: 0.010472\n",
      "Step: 201 Error: 0.009603\n",
      "Step: 217 Error: 0.009386\n",
      "Step: 221 Error: 0.009143\n",
      "Step: 224 Error: 0.008642\n",
      "Step: 226 Error: 0.007217\n",
      "Step: 240 Error: 0.007155\n",
      "Step: 243 Error: 0.007149\n",
      "Step: 244 Error: 0.006805\n",
      "Step: 248 Error: 0.006125\n",
      "Step: 253 Error: 0.006030\n",
      "Step: 258 Error: 0.005584\n",
      "Step: 261 Error: 0.005251\n",
      "Step: 268 Error: 0.004557\n",
      "Step: 286 Error: 0.004236\n",
      "Step: 298 Error: 0.004085\n",
      "Step: 303 Error: 0.003995\n",
      "Step: 304 Error: 0.003826\n",
      "Step: 307 Error: 0.003765\n",
      "Step: 313 Error: 0.003655\n",
      "Step: 323 Error: 0.003304\n",
      "Step: 326 Error: 0.003157\n",
      "Step: 332 Error: 0.003000\n",
      "Step: 339 Error: 0.002919\n",
      "Step: 351 Error: 0.002804\n",
      "Step: 366 Error: 0.002642\n",
      "Step: 377 Error: 0.002603\n",
      "Step: 380 Error: 0.002414\n",
      "Step: 384 Error: 0.002402\n",
      "Step: 387 Error: 0.002338\n",
      "Step: 406 Error: 0.002300\n",
      "Step: 408 Error: 0.002285\n",
      "Step: 413 Error: 0.002230\n",
      "Step: 414 Error: 0.002219\n",
      "Step: 419 Error: 0.002098\n",
      "Step: 423 Error: 0.002050\n",
      "Step: 443 Error: 0.001975\n",
      "Step: 448 Error: 0.001967\n",
      "Step: 449 Error: 0.001905\n",
      "Step: 454 Error: 0.001862\n",
      "Step: 455 Error: 0.001852\n",
      "Step: 460 Error: 0.001766\n",
      "Step: 472 Error: 0.001603\n",
      "Step: 479 Error: 0.001576\n",
      "Step: 506 Error: 0.001445\n",
      "Step: 515 Error: 0.001425\n",
      "Step: 544 Error: 0.001410\n",
      "Step: 547 Error: 0.001395\n",
      "Step: 549 Error: 0.001381\n",
      "Step: 551 Error: 0.001371\n",
      "Step: 555 Error: 0.001355\n",
      "Step: 560 Error: 0.001260\n",
      "Step: 563 Error: 0.001241\n",
      "Step: 577 Error: 0.001228\n",
      "Step: 614 Error: 0.001172\n",
      "Step: 618 Error: 0.001150\n",
      "Step: 622 Error: 0.001127\n",
      "Step: 629 Error: 0.001097\n",
      "Step: 651 Error: 0.001072\n",
      "Step: 653 Error: 0.001003\n",
      "Step: 682 Error: 0.000997\n",
      "Step: 684 Error: 0.000964\n",
      "Step: 711 Error: 0.000932\n",
      "Step: 721 Error: 0.000905\n",
      "Step: 739 Error: 0.000882\n",
      "Step: 753 Error: 0.000881\n",
      "Step: 756 Error: 0.000868\n",
      "Step: 757 Error: 0.000845\n",
      "Step: 759 Error: 0.000840\n",
      "Step: 775 Error: 0.000836\n",
      "Step: 777 Error: 0.000830\n",
      "Step: 784 Error: 0.000812\n",
      "Step: 793 Error: 0.000801\n",
      "Step: 795 Error: 0.000780\n",
      "Step: 828 Error: 0.000778\n",
      "Step: 829 Error: 0.000754\n",
      "Step: 851 Error: 0.000728\n",
      "Step: 860 Error: 0.000717\n",
      "Step: 869 Error: 0.000699\n",
      "Step: 892 Error: 0.000691\n",
      "Step: 896 Error: 0.000683\n",
      "Step: 924 Error: 0.000638\n",
      "Step: 934 Error: 0.000634\n",
      "Step: 971 Error: 0.000633\n",
      "Step: 973 Error: 0.000632\n",
      "Step: 975 Error: 0.000624\n",
      "Step: 981 Error: 0.000596\n"
     ]
    }
   ],
   "source": [
    "##Run this cell to train the network\n",
    "wMatrixIn, wMatrixOut = neuralNetTrain(testEORdata, 2, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here quite good convergence has occurred and there is no oscillation. Perhaps you can see that the initial steps are giving large changes in the error while later on there are smaller and smaller changes. This is owing to the effect of the gradient calculation. The changes in the actual weights are not printed, but will follow the same trend.\n",
    "\n",
    "The output weight matrices can then be run on test data for evaluation. Test data should be inputs with known output but which were not included in the training set. \n",
    "\n",
    "Obviously it is not possible to give any new data for the EOR function as the training set covered all possible responses!\n",
    "\n",
    "But the trained network should be able to do a reasonable job on the training set.\n",
    "Run the following cell to compare the output of the network with the actual values of the training set outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] 0.9834937071829389\n",
      "[1] 0.9833967025093591\n",
      "[0] 0.011231149515516878\n",
      "[0] -0.00048119258000472944\n"
     ]
    }
   ],
   "source": [
    "##Run this cell to test the network\n",
    "for inputs, knownOut in testEORdata:\n",
    "    sIn, sHid, sOut =    neuralNetPredict(array(inputs), wMatrixIn, wMatrixOut)\n",
    "    print(knownOut, sOut[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple feature vectors for sequence data\n",
    "A simple numbering scheme is used to convert to the sequence alphabet to a numeric form as an input vector. For proteins that is number from 1 to 20 from the list of one-letter codes.\n",
    "\n",
    "k-mers with k=5 are used. Only the output for the middle residue is required but the network will use the neighbours to predict the secondary structure of the middle one.  \n",
    "\n",
    "Although static k-mer are used for training in practice a prediction in a moving 5-mer window could be implemented. \n",
    "\n",
    "The possible outputs are also coded as integers for the more restricted alphabet of H, C, and S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run this cell to define the dictionaries for the vectors\n",
    "aminoAcids = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "aaIndexDict = {}\n",
    "for i, aa in enumerate(aminoAcids):\n",
    "        aaIndexDict[aa] = i\n",
    "\n",
    "ssIndexDict = {}\n",
    "ssCodes = 'HCS'\n",
    "for i, code in enumerate(ssCodes):\n",
    "        ssIndexDict[code] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a very limited set of training data. It shows the raw format which is a 5-mer string and the secondary structure that was observed for the central residue of this in at least one PDB structure. \n",
    "\n",
    "The actual structure is a simplified output from the DSSP program mentioned in the introduction. DSSP acutally distinguishes more structures that the three here - for example there are other kinds of helix. But these complications are not dealt with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run this cell to define the training set\n",
    "seqSecStrucData = [('ADTLL','S'),\n",
    "                         ('DTLLI','S'),\n",
    "                         ('TLLIL','S'),\n",
    "                         ('LLILG','S'),\n",
    "                         ('LILGD','S'),\n",
    "                         ('ILGDS','S'),\n",
    "                         ('LGDSL','C'),\n",
    "                         ('GDSLS','H'),\n",
    "                         ('DSLSA','H'),\n",
    "                         ('SLSAG','H'),\n",
    "                         ('LSAGY','H'),\n",
    "                         ('SAGYR','C'),\n",
    "                         ('AGYRM','C'),\n",
    "                         ('GYRMS','C'),\n",
    "                         ('YRMSA','C'),\n",
    "                         ('RMSAS','C')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data has to be converted to the numerical code. Here is a function to to that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run this cell to define the function\n",
    "def convertSeqToVector(seq, indexDict):\n",
    "    \"\"\"converts a one-letter sequence to numerical\n",
    "    coding for neural network calculations\"\"\"   \n",
    "    numLetters = len(indexDict)\n",
    "    vector = [0.0] * len(seq) * numLetters\n",
    "\n",
    "    for pos, letter in enumerate(seq):\n",
    "        index = pos * numLetters + indexDict[letter]    \n",
    "        vector[index] = 1.0\n",
    "\n",
    "    return vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data is prepared with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run this cell to create the training data\n",
    "trainingData = []\n",
    "for seq, ss in seqSecStrucData:\n",
    " \n",
    "        inputVec = convertSeqToVector(seq, aaIndexDict)\n",
    "        outputVec = convertSeqToVector(ss, ssIndexDict)\n",
    " \n",
    "        trainingData.append( (inputVec, outputVec) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then the network is trained. Here there are 3 hidden nodes specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Error: 18.544195\n",
      "Step: 1 Error: 13.273331\n",
      "Step: 2 Error: 10.049923\n",
      "Step: 3 Error: 8.081151\n",
      "Step: 4 Error: 6.679041\n",
      "Step: 5 Error: 4.146071\n",
      "Step: 6 Error: 3.434692\n",
      "Step: 7 Error: 1.841507\n",
      "Step: 9 Error: 0.771946\n",
      "Step: 10 Error: 0.557903\n",
      "Step: 11 Error: 0.451036\n",
      "Step: 12 Error: 0.264221\n",
      "Step: 13 Error: 0.189702\n",
      "Step: 15 Error: 0.142910\n",
      "Step: 18 Error: 0.138171\n",
      "Step: 20 Error: 0.112035\n",
      "Step: 22 Error: 0.083567\n",
      "Step: 24 Error: 0.058639\n",
      "Step: 26 Error: 0.053782\n",
      "Step: 31 Error: 0.047820\n",
      "Step: 33 Error: 0.041850\n",
      "Step: 37 Error: 0.041696\n",
      "Step: 41 Error: 0.034369\n",
      "Step: 42 Error: 0.024654\n",
      "Step: 62 Error: 0.016462\n",
      "Step: 80 Error: 0.011997\n",
      "Step: 110 Error: 0.008466\n",
      "Step: 111 Error: 0.007629\n",
      "Step: 116 Error: 0.006890\n",
      "Step: 146 Error: 0.006702\n",
      "Step: 148 Error: 0.005243\n",
      "Step: 164 Error: 0.004507\n",
      "Step: 198 Error: 0.004341\n",
      "Step: 199 Error: 0.004154\n",
      "Step: 200 Error: 0.003424\n",
      "Step: 250 Error: 0.003408\n",
      "Step: 281 Error: 0.003361\n",
      "Step: 287 Error: 0.003207\n",
      "Step: 299 Error: 0.002994\n",
      "Step: 350 Error: 0.002833\n",
      "Step: 353 Error: 0.001769\n",
      "Step: 375 Error: 0.001680\n",
      "Step: 376 Error: 0.001459\n",
      "Step: 381 Error: 0.001285\n",
      "Step: 455 Error: 0.001240\n",
      "Step: 456 Error: 0.001076\n",
      "Step: 457 Error: 0.001040\n",
      "Step: 459 Error: 0.000939\n",
      "Step: 467 Error: 0.000924\n",
      "Step: 480 Error: 0.000864\n",
      "Step: 495 Error: 0.000783\n",
      "Step: 535 Error: 0.000782\n",
      "Step: 539 Error: 0.000725\n",
      "Step: 593 Error: 0.000653\n",
      "Step: 603 Error: 0.000653\n",
      "Step: 605 Error: 0.000602\n",
      "Step: 606 Error: 0.000595\n",
      "Step: 625 Error: 0.000578\n",
      "Step: 626 Error: 0.000562\n",
      "Step: 635 Error: 0.000554\n",
      "Step: 692 Error: 0.000507\n",
      "Step: 700 Error: 0.000506\n",
      "Step: 705 Error: 0.000497\n",
      "Step: 890 Error: 0.000497\n",
      "Step: 901 Error: 0.000458\n",
      "Step: 905 Error: 0.000420\n",
      "Step: 910 Error: 0.000417\n",
      "Step: 922 Error: 0.000394\n",
      "Step: 926 Error: 0.000367\n",
      "Step: 936 Error: 0.000367\n",
      "Step: 940 Error: 0.000359\n"
     ]
    }
   ],
   "source": [
    "   wMatrixIn, wMatrixOut = neuralNetTrain(trainingData, 3, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see that this training has converged nicely. The only problem is that it was for a very restricted set of sequence data. \n",
    "\n",
    "There are 3 x 20 = 60 theoretical combinations of amino acid residues with secondary structure states. But for particular residues some of these are favoured and others disfavoured. \n",
    "\n",
    "For each residue there will be 20^4 = 160 000 different contexts that then could possibly occur in. Although some of the resulting 5-mers are actually quite rare in structured proteins. \n",
    "\n",
    "All the same, it would be good to have a larger training set. It is better to have some rare examples of residue state combinations although, of course, the network will not have to predict them frequently.\n",
    "\n",
    "One thing to remember is to retain some test examples - where the answer is known but which are not in the training set. \n",
    "\n",
    "In its current, poorly-trained state, the network is still able to make a reasonable predictions. But only if the test is clearly related to examples that it has seen. testSecStrucSeq here is very similar to examples in the seqSecStrucData training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test prediction: S\n"
     ]
    }
   ],
   "source": [
    "##Run this cell to test for a new 5-mer\n",
    "testSecStrucSeq = 'DLLSA'\n",
    "testSecStrucVec = convertSeqToVector(testSecStrucSeq, aaIndexDict)\n",
    "testSecStrucArray = array( [testSecStrucVec,] )\n",
    "\n",
    "sIn, sHid, sOut =    neuralNetPredict(testSecStrucArray, wMatrixIn, wMatrixOut)\n",
    "index = sOut.argmax()\n",
    "print(\"Test prediction: %s\" % ssCodes[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inporting a larger training set\n",
    "A much larger training set is provided as a comma separated file called:\n",
    "\"PDB_protein_secondary_5mers.csv\". \n",
    "\n",
    "    from csv import reader #may help here\n",
    "Can you create a training set vector data structure from this and use it to train the network?\n",
    "If you would like some examples of test data, here are some examples from the recently-determined PDB 6aam.pdb\n",
    "\n",
    "    S: EMVAV, KVSKY, YKGCC\n",
    "    H: LAQLL, ICEGM, ASVDW\n",
    "    C: ERLPR, GDFGL, YKFYY\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
