{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A feed-forward Neural Network for secondary structure prediction\n",
    "This notebook looks at a Neural Network based on code from \n",
    "[Stevens and Boucher (2014, Python Programming for Biology CUP)](https://www.amazon.co.uk/Python-Programming-Biology-Bioinformatics-Beyond/dp/0521720095). The aim is to predict the secondary structure of a protein from its sequence. \n",
    "A Predictive network is trained using data on known secondary structure of k-mers of 5 amino-acids taken from a set of PDB structures. Three secondary structure states are defined: H, C, and S. H and S are helix and strand respectively while C is for coil which is a range of structures not with regular H-bonding pattern. In practice, secondary structure prediction has many uses, for instance in helping in the identification of functional domains ([Drozdetskiy et al., 2015](https://doi.org/10.1093/nar/gkv332)) and can be easily acheived using the JPRED4 server http://www.compbio.dundee.ac.uk/jpred4\n",
    "\n",
    "Secondary structure is much more complicated than indicated by the simple classification of this data - full details are available from the analysis of the H-bonding arrangements. The program DSSP (https://swift.cmbi.umcn.nl/gv/dssp/DSSP_3.html) is a well-tested approach to this problem. This produces a description of the secondary structure in a known protein structure. For historical reasons DSSP uses E for Strands. A related DSSR program gives RNA secondary structure.\n",
    "\n",
    "It is useful to be able to predict the secondary structure of a protein for which there is only sequence available. One approach would be to align it with homologous sequences where the structure is known. The approach here is to use the sequence in the neighbourhood of a residue as a basis for a neural network prediction. \n",
    "\n",
    "The network here is a simple three layer feed-forward one. The number of nodes in the hidden layer can be defined by the programmer. But the number of input and output nodes is defined by the sizes of the input and output data vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run this cell to import numpy \n",
    "from numpy import tanh, ones, append, array, zeros, random, sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network function takes input data for the first layer of Network nodes, applies the the first weighted connections to pass the signal to the hidden layer of nodes, then applies the second weights to produce output. \n",
    "\n",
    "The output may not be optimized as the function also operates on the weighting during training. However after training the function gives predictions so takes its name from that. \n",
    "\n",
    "The weightsIn values define the strength of connection between the input nodes and the hidden nodes. Similarly weightsOut define the strengths of connection between the hidden and the output nodes. \n",
    "\n",
    "The weights are given as matrices with the rows indexing the nodes in a layer and the columns indexing the nodes in the other layer. \n",
    "\n",
    "The signalIn vector is the input features and an extra value of 1.0. This additional value is called the bias node which is used to tune the baseline response of the network. The baseline is the level without a meaningful signal. \n",
    "\n",
    "Setting the bias node value happens during training to adapt to the values in the input data. This means the input data don't need to pre-prepared with a mean of zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this cell to define the function\n",
    "def neuralNetPredict(inputVec, weightsIn, weightsOut):\n",
    "    \"\"\" uses the current weights in a neural network\n",
    "    to make a prediction from an input vector\n",
    "    all input and output are numpy data structures\"\"\" \n",
    "    signalIn = append(inputVec, 1.0) # input layer\n",
    "\n",
    "    prod = signalIn * weightsIn.T\n",
    "    sums = sum(prod, axis=1)\n",
    "    signalHid = tanh(sums)    # hidden    layer\n",
    "\n",
    "    prod = signalHid * weightsOut.T\n",
    "    sums = sum(prod, axis=1)\n",
    "    signalOut = tanh(sums)    # output    layer\n",
    "\n",
    "    return signalIn, signalHid, signalOut\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the numpy `.T` methodn gives the transpose of a matrix - that is the matrix with the columns turned into rows and the rows turned into columns. \n",
    "\n",
    "This is used so that the input signal gets applied to all the hidden nodes.\n",
    "\n",
    "The network applies the hyperbolic tangent function (tanh) to get the signal output from all the nodes in layer. Hyperbolic tan is a sigmoidal function that varies from -1 to 1, so it is much better than ordinary tan that runs off to infinity. \n",
    "\n",
    "<img src=\"https://mathworld.wolfram.com/images/interactive/TanhReal.gif\" width=300></img> \n",
    "\n",
    "The tanh function defines the output of that node given an input or set of inputs. As a nod to the output of neurones, which depends on an activation level across their cell membrane, the output function is called the *Activation* function.\n",
    "\n",
    "In operation only the signalOut from the output layer is of interest. But during training the response signals from the other layers are also needed to adjust the weighting scheme.\n",
    "\n",
    "### Training \n",
    "\n",
    "The weighting scheme (and gain) will be optimized by using a training dataset. \n",
    "\n",
    "The training data will be an input feature vector and a known output vector. The order of the data will be randomly shuffled to avoid bias. The number of hidden nodes needs to be specified and the number of optimization cycles. \n",
    "\n",
    "After each cycle the 'error' between the output signal of the network and the known training set output is used to adjust the network weights. The difference is combined with the *gradient* in the signal values - calculated from the tanh activation function (conveniently the gradient of tanh(sig) is 1-sig<sup>2</sup> or 1 - {sig x sig}). \n",
    "\n",
    "Early in training large difference can make the network go haywire so the speed of weight changing is damped down by a 'rate' and 'momentum' multipliers (usually the default values of 0.5 and 0.2 are good enough). \n",
    "\n",
    "More damping would mean that many more cycles would be needed for the weights to converge. \n",
    "\n",
    "The training will work back from the value of the error to adjust the weighting scheme of the network. This is called *back propagation*. \n",
    "\n",
    "The use of the gradient is crucial as it means initial adjustments will be large but then finer adjustments will be made as the optimum is approached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this cell to define the neuralNetTrain function\n",
    "def neuralNetTrain(trainData, numHid, steps=100, rate=0.5, momentum=0.2, wInp=None, wOut=None):\n",
    "    \"\"\" uses training data to set the weights in a simple\n",
    "    neural network, number of hidden nodes is specified\"\"\"\n",
    "    numInp = len(trainData[0][0])\n",
    "    numOut = len(trainData[0][1])\n",
    "    numInp += 1\n",
    "    minError = None\n",
    "\n",
    "    sigInp = ones(numInp)\n",
    "    sigHid = ones(numHid)\n",
    "    sigOut = ones(numOut)\n",
    "    \n",
    "    if wInp is None:\n",
    "        wInp = random.random((numInp, numHid))-0.5\n",
    "    \n",
    "    if wOut is None:\n",
    "        wOut = random.random((numHid, numOut))-0.5\n",
    "    bestWeightMatrices = (wInp, wOut)\n",
    "\n",
    "    cInp = zeros((numInp, numHid))\n",
    "    cOut = zeros((numHid, numOut))\n",
    "\n",
    "    for x, (inputs, knownOut) in enumerate(trainData):\n",
    "        trainData[x] = (array(inputs), array(knownOut))\n",
    " \n",
    "    for step in range(steps):  \n",
    "        random.shuffle(trainData) # Important to avoid bias\n",
    "        error = 0.0\n",
    " \n",
    "        for inputs, knownOut in trainData:\n",
    "            sigIn, sigHid, sigOut = neuralNetPredict(inputs, wInp, wOut)\n",
    "\n",
    "            diff = knownOut - sigOut\n",
    "            error += sum(diff * diff)\n",
    "\n",
    "            gradient = ones(numOut) - (sigOut*sigOut)\n",
    "            outAdjust = gradient * diff \n",
    "\n",
    "            diff = sum(outAdjust * wOut, axis=1)\n",
    "            gradient = ones(numHid) - (sigHid*sigHid)\n",
    "            hidAdjust = gradient * diff \n",
    "\n",
    "            # update output \n",
    "            change = outAdjust * sigHid.reshape(numHid, 1)\n",
    "            wOut += (rate * change) + (momentum * cOut)\n",
    "            cOut = change\n",
    " \n",
    "            # update input \n",
    "            change = hidAdjust * sigIn.reshape(numInp, 1)\n",
    "            wInp += (rate * change) + (momentum * cInp)\n",
    "            cInp = change\n",
    " \n",
    "        if (minError is None) or (error < minError):\n",
    "            minError = error\n",
    "            bestWeightMatrices = (wInp.copy(), wOut.copy())\n",
    "            print(\"Step: %d Error: %f\" % (step, error))\n",
    "    \n",
    "    return bestWeightMatrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the functions\n",
    "Simple data to test a network can be binary input vectors with the desired output being an 'exclusive OR' (EOR) response https://en.wikipedia.org/wiki/Exclusive_or. This responds True if any input is true but False is both are together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run this cell to define the test data\n",
    "testEORdata = [[[0,0], [0]],\n",
    "               [[0,1], [1]], \n",
    "               [[1,0], [1]], \n",
    "               [[1,1], [0]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network test uses two hidden nodes - in real use several values would be tried to find the best performance.\n",
    "Run the cell below to see if the training converges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Error: 2.047457\n",
      "Step: 1 Error: 1.496867\n",
      "Step: 2 Error: 1.470226\n",
      "Step: 3 Error: 1.270359\n",
      "Step: 4 Error: 1.136371\n",
      "Step: 11 Error: 1.115328\n",
      "Step: 20 Error: 1.114759\n",
      "Step: 22 Error: 1.056885\n",
      "Step: 27 Error: 1.054385\n",
      "Step: 29 Error: 1.034736\n",
      "Step: 30 Error: 1.025347\n",
      "Step: 31 Error: 0.973878\n",
      "Step: 32 Error: 0.953984\n",
      "Step: 34 Error: 0.897854\n",
      "Step: 36 Error: 0.859145\n",
      "Step: 40 Error: 0.853927\n",
      "Step: 42 Error: 0.832885\n",
      "Step: 43 Error: 0.820952\n",
      "Step: 47 Error: 0.801974\n",
      "Step: 51 Error: 0.782796\n",
      "Step: 54 Error: 0.776446\n",
      "Step: 55 Error: 0.751814\n",
      "Step: 60 Error: 0.738176\n",
      "Step: 62 Error: 0.735615\n",
      "Step: 65 Error: 0.721428\n",
      "Step: 66 Error: 0.708045\n",
      "Step: 67 Error: 0.707911\n",
      "Step: 71 Error: 0.701907\n",
      "Step: 72 Error: 0.660396\n",
      "Step: 73 Error: 0.657192\n",
      "Step: 76 Error: 0.655137\n",
      "Step: 77 Error: 0.557145\n",
      "Step: 80 Error: 0.415191\n",
      "Step: 85 Error: 0.247516\n",
      "Step: 94 Error: 0.234249\n",
      "Step: 95 Error: 0.225726\n",
      "Step: 98 Error: 0.184929\n",
      "Step: 101 Error: 0.144817\n",
      "Step: 104 Error: 0.130381\n",
      "Step: 106 Error: 0.105220\n",
      "Step: 109 Error: 0.075125\n",
      "Step: 113 Error: 0.047095\n",
      "Step: 128 Error: 0.041979\n",
      "Step: 132 Error: 0.038448\n",
      "Step: 135 Error: 0.033169\n",
      "Step: 141 Error: 0.025566\n",
      "Step: 144 Error: 0.018320\n",
      "Step: 148 Error: 0.017276\n",
      "Step: 160 Error: 0.016760\n",
      "Step: 165 Error: 0.016504\n",
      "Step: 166 Error: 0.015058\n",
      "Step: 173 Error: 0.014843\n",
      "Step: 174 Error: 0.014244\n",
      "Step: 180 Error: 0.010774\n",
      "Step: 190 Error: 0.008220\n",
      "Step: 207 Error: 0.007309\n",
      "Step: 220 Error: 0.007157\n",
      "Step: 224 Error: 0.006010\n",
      "Step: 240 Error: 0.005198\n",
      "Step: 246 Error: 0.004661\n",
      "Step: 256 Error: 0.004634\n",
      "Step: 263 Error: 0.004428\n",
      "Step: 269 Error: 0.004397\n",
      "Step: 275 Error: 0.004375\n",
      "Step: 276 Error: 0.003955\n",
      "Step: 295 Error: 0.003810\n",
      "Step: 300 Error: 0.003627\n",
      "Step: 304 Error: 0.003125\n",
      "Step: 330 Error: 0.002818\n",
      "Step: 352 Error: 0.002800\n",
      "Step: 353 Error: 0.002624\n",
      "Step: 358 Error: 0.002575\n",
      "Step: 373 Error: 0.002526\n",
      "Step: 375 Error: 0.002506\n",
      "Step: 376 Error: 0.002282\n",
      "Step: 395 Error: 0.002181\n",
      "Step: 399 Error: 0.002085\n",
      "Step: 405 Error: 0.001986\n",
      "Step: 430 Error: 0.001949\n",
      "Step: 436 Error: 0.001854\n",
      "Step: 442 Error: 0.001788\n",
      "Step: 447 Error: 0.001764\n",
      "Step: 455 Error: 0.001618\n",
      "Step: 474 Error: 0.001610\n",
      "Step: 477 Error: 0.001606\n",
      "Step: 484 Error: 0.001594\n",
      "Step: 488 Error: 0.001486\n",
      "Step: 509 Error: 0.001375\n",
      "Step: 525 Error: 0.001372\n",
      "Step: 532 Error: 0.001351\n",
      "Step: 538 Error: 0.001331\n",
      "Step: 549 Error: 0.001298\n",
      "Step: 559 Error: 0.001282\n",
      "Step: 582 Error: 0.001277\n",
      "Step: 591 Error: 0.001228\n",
      "Step: 593 Error: 0.001199\n",
      "Step: 595 Error: 0.001189\n",
      "Step: 633 Error: 0.001186\n",
      "Step: 635 Error: 0.001149\n",
      "Step: 637 Error: 0.001058\n",
      "Step: 652 Error: 0.001052\n",
      "Step: 656 Error: 0.001042\n",
      "Step: 670 Error: 0.001004\n",
      "Step: 673 Error: 0.000961\n",
      "Step: 694 Error: 0.000954\n",
      "Step: 713 Error: 0.000932\n",
      "Step: 716 Error: 0.000926\n",
      "Step: 737 Error: 0.000848\n",
      "Step: 746 Error: 0.000837\n",
      "Step: 765 Error: 0.000829\n",
      "Step: 773 Error: 0.000803\n",
      "Step: 835 Error: 0.000796\n",
      "Step: 836 Error: 0.000780\n",
      "Step: 837 Error: 0.000767\n",
      "Step: 838 Error: 0.000738\n",
      "Step: 854 Error: 0.000717\n",
      "Step: 876 Error: 0.000709\n",
      "Step: 877 Error: 0.000696\n",
      "Step: 893 Error: 0.000692\n",
      "Step: 897 Error: 0.000690\n",
      "Step: 901 Error: 0.000686\n",
      "Step: 907 Error: 0.000675\n",
      "Step: 912 Error: 0.000648\n",
      "Step: 933 Error: 0.000641\n",
      "Step: 955 Error: 0.000634\n",
      "Step: 968 Error: 0.000607\n",
      "Step: 992 Error: 0.000599\n"
     ]
    }
   ],
   "source": [
    "## Run this cell to train the network\n",
    "wMatrixIn, wMatrixOut = neuralNetTrain(testEORdata, 2, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here quite good convergence has occurred and there is no oscillation. Perhaps you can see that the initial steps are giving large changes in the error while later on there are smaller and smaller changes. This is owing to the effect of the gradient calculation. The changes in the actual weights are not printed, but will follow the same trend.\n",
    "\n",
    "The output weight matrices can then be run on test data for evaluation. Test data should be inputs with known output but which were not included in the training set. \n",
    "\n",
    "Obviously it is not possible to give any new data for the EOR function as the training set covered all possible responses!\n",
    "\n",
    "But the trained network should be able to do a reasonable job on the training set.\n",
    "Run the following cell to compare the output of the network with the actual values of the training set outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input [1 0]  should have output  [1] actual output 0.983\n",
      "input [0 1]  should have output  [1] actual output 0.983\n",
      "input [0 0]  should have output  [0] actual output -0.002\n",
      "input [1 1]  should have output  [0] actual output 0.010\n"
     ]
    }
   ],
   "source": [
    "##Run this cell to test the network\n",
    "for inputs, knownOut in testEORdata:\n",
    "    sIn, sHid, sOut =    neuralNetPredict(array(inputs), wMatrixIn, wMatrixOut)\n",
    "    print('input', inputs, ' should have output ', knownOut, 'actual output {:.3f}'.format(sOut[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple feature vectors for sequence data\n",
    "A simple numbering scheme is used to convert to the sequence alphabet to a numeric form as an input vector. For proteins that is number from 1 to 20 from the list of one-letter codes.\n",
    "\n",
    "k-mers with k=5 are used. Only the output for the middle residue is required but the network will use the neighbours to predict the secondary structure of the middle one.  \n",
    "\n",
    "Although static k-mer are used for training in practice a prediction in a moving 5-mer window could be implemented. \n",
    "\n",
    "The possible outputs are also coded as integers for the more restricted alphabet of H, C, and S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run this cell to define the dictionaries for the vectors\n",
    "aminoAcids = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "aaIndexDict = {}\n",
    "for i, aa in enumerate(aminoAcids):\n",
    "        aaIndexDict[aa] = i\n",
    "\n",
    "ssIndexDict = {}\n",
    "ssCodes = 'HCS'\n",
    "for i, code in enumerate(ssCodes):\n",
    "        ssIndexDict[code] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a very limited set of training data. It shows the raw format which is a 5-mer string and the secondary structure that was observed for the central residue of this in at least one PDB structure. \n",
    "\n",
    "The actual structure is a simplified output from the DSSP program mentioned in the introduction. DSSP acutally distinguishes more structures that the three here - for example there are other kinds of helix. But these complications are not dealt with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run this cell to define the training set\n",
    "small_training_set = [('ADTLL','S'),\n",
    "                      ('DTLLI','S'),\n",
    "                      ('TLLIL','S'),\n",
    "                      ('LLILG','S'),\n",
    "                      ('LILGD','S'),\n",
    "                      ('ILGDS','S'),\n",
    "                      ('LGDSL','C'),\n",
    "                      ('GDSLS','H'),\n",
    "                      ('DSLSA','H'),\n",
    "                      ('SLSAG','H'),\n",
    "                      ('LSAGY','H'),\n",
    "                      ('SAGYR','C'),\n",
    "                      ('AGYRM','C'),\n",
    "                      ('GYRMS','C'),\n",
    "                      ('YRMSA','C'),\n",
    "                      ('RMSAS','C')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data has to be converted to the numerical code. Here is a function to to that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run this cell to define the function\n",
    "def convertSeqToVector(seq, indexDict):\n",
    "    \"\"\"converts a one-letter sequence to numerical\n",
    "    coding for neural network calculations\"\"\"   \n",
    "    numLetters = len(indexDict)\n",
    "    vector = [0.0] * len(seq) * numLetters\n",
    "\n",
    "    for pos, letter in enumerate(seq):\n",
    "        index = pos * numLetters + indexDict[letter]    \n",
    "        vector[index] = 1.0\n",
    "\n",
    "    return vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data is prepared with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run this cell to create the training data\n",
    "small_training_vector = []\n",
    "for seq, ss in small_training_set:\n",
    " \n",
    "        inputVec = convertSeqToVector(seq, aaIndexDict)\n",
    "        outputVec = convertSeqToVector(ss, ssIndexDict)\n",
    "\n",
    "        small_training_vector.append( (inputVec, outputVec) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then the network is trained. Here there are 3 hidden nodes specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Error: 20.651445\n",
      "Step: 1 Error: 11.056185\n",
      "Step: 2 Error: 10.190612\n",
      "Step: 3 Error: 9.980795\n",
      "Step: 4 Error: 6.978679\n",
      "Step: 5 Error: 6.137538\n",
      "Step: 6 Error: 5.204809\n",
      "Step: 11 Error: 4.641291\n",
      "Step: 12 Error: 4.395981\n",
      "Step: 13 Error: 1.864322\n",
      "Step: 15 Error: 1.122902\n",
      "Step: 16 Error: 0.518139\n",
      "Step: 17 Error: 0.256837\n",
      "Step: 19 Error: 0.241855\n",
      "Step: 23 Error: 0.151086\n",
      "Step: 27 Error: 0.148091\n",
      "Step: 31 Error: 0.126893\n",
      "Step: 40 Error: 0.088898\n",
      "Step: 43 Error: 0.069470\n",
      "Step: 47 Error: 0.067209\n",
      "Step: 48 Error: 0.065510\n",
      "Step: 50 Error: 0.064267\n",
      "Step: 52 Error: 0.049810\n",
      "Step: 53 Error: 0.037619\n",
      "Step: 65 Error: 0.031603\n",
      "Step: 80 Error: 0.016909\n",
      "Step: 100 Error: 0.016783\n",
      "Step: 101 Error: 0.013760\n",
      "Step: 129 Error: 0.012409\n",
      "Step: 135 Error: 0.006590\n",
      "Step: 169 Error: 0.006210\n",
      "Step: 194 Error: 0.006120\n",
      "Step: 195 Error: 0.005638\n",
      "Step: 240 Error: 0.004162\n",
      "Step: 248 Error: 0.004063\n",
      "Step: 260 Error: 0.003323\n",
      "Step: 277 Error: 0.003254\n",
      "Step: 278 Error: 0.002845\n",
      "Step: 360 Error: 0.002118\n",
      "Step: 472 Error: 0.002041\n",
      "Step: 494 Error: 0.001691\n",
      "Step: 517 Error: 0.001372\n",
      "Step: 528 Error: 0.001105\n",
      "Step: 548 Error: 0.000930\n",
      "Step: 679 Error: 0.000822\n",
      "Step: 762 Error: 0.000722\n",
      "Step: 809 Error: 0.000644\n",
      "Step: 905 Error: 0.000619\n",
      "Step: 918 Error: 0.000615\n",
      "Step: 924 Error: 0.000563\n",
      "Step: 931 Error: 0.000528\n",
      "Step: 969 Error: 0.000503\n"
     ]
    }
   ],
   "source": [
    "wMatrixIn, wMatrixOut = neuralNetTrain(small_training_vector, 3, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see that this training has converged nicely. The only problem is that it was for a very restricted set of sequence data. \n",
    "\n",
    "There are 3 x 20 = 60 theoretical combinations of amino acid residues with secondary structure states. But for particular residues some of these are favoured and others disfavoured. \n",
    "\n",
    "For each residue there will be 20^4 = 160 000 different contexts that then could possibly occur in. Although some of the resulting 5-mers are actually quite rare in structured proteins. \n",
    "\n",
    "All the same, it would be good to have a larger training set. It is better to have some rare examples of residue state combinations although, of course, the network will not have to predict them frequently.\n",
    "\n",
    "One thing to remember is to retain some test examples - where the answer is known but which are not in the training set. \n",
    "\n",
    "In its current, poorly-trained state, the network is still able to make a reasonable predictions. But only if the test is clearly related to examples that it has seen. testSecStrucSeq here is very similar to examples in the seqSecStrucData training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to define \n",
    "def predict_5_mer(seq_5_mer):\n",
    "    \"\"\"\n",
    "    returns a prediction either 'H', 'S' or 'C' for the input sequence of 5 amino acids\n",
    "    \"\"\"\n",
    "    global wMatrixIn  # produced by previous training\n",
    "    global wMatrixOut\n",
    "    vector_5_mer = convertSeqToVector(seq_5_mer, indexDict=aaIndexDict)\n",
    "    array_5_mer = array([vector_5_mer,])\n",
    "    sIn, sHid, sOut =    neuralNetPredict(array_5_mer, wMatrixIn, wMatrixOut)\n",
    "    index = sOut.argmax()\n",
    "    return ssCodes[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADTLL input  S  predict  S\n",
      "DTLLI input  S  predict  S\n",
      "TLLIL input  S  predict  S\n",
      "LLILG input  S  predict  S\n",
      "LILGD input  S  predict  S\n",
      "ILGDS input  S  predict  S\n",
      "LGDSL input  C  predict  C\n",
      "GDSLS input  H  predict  H\n",
      "DSLSA input  H  predict  H\n",
      "SLSAG input  H  predict  H\n",
      "LSAGY input  H  predict  H\n",
      "SAGYR input  C  predict  C\n",
      "AGYRM input  C  predict  C\n",
      "GYRMS input  C  predict  C\n",
      "YRMSA input  C  predict  C\n",
      "RMSAS input  C  predict  C\n",
      "success \"prediction\" for training data is 100.0%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for test_5_mer, code in small_training_set:\n",
    "    predict = predict_5_mer(test_5_mer)\n",
    "    if predict == code:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "    print(test_5_mer, 'input ', code, ' predict ', predict)\n",
    "print('success \"prediction\" for training data is {:.1f}%'.format(100.*correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction for DLLSA is H\n"
     ]
    }
   ],
   "source": [
    "test_expect_h = 'DLLSA'\n",
    "print('prediction for', test_expect_h, 'is', predict_5_mer(test_expect_h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data from PDB structure 6aam, divided into 5-mers\n",
    "test_data_6aam = [('DPTVF', 'C'), ('HKRYL', 'C'), ('KKIRD', 'S'), ('LGEGH', 'C'), \n",
    "                  ('FGKVS', 'S'), ('LYCYD', 'S'), ('PTNDG', 'C'), ('TGEMV', 'S'), \n",
    "                  ('AVKAL', 'S'), ('KADAG', 'C'), ('PQHRS', 'H'), ('GWKQE', 'H'), \n",
    "                  ('IDILR', 'H'), ('TLYHE', 'C'), ('HIIKY', 'C'), ('KGCCE', 'S'), \n",
    "                  ('DAGAA', 'C'), ('SLQLV', 'S'), ('MEYVP', 'C'), ('LGSLR', 'S'), \n",
    "                  ('DYLPR', 'C'), ('HSIGL', 'C'), ('AQLLL', 'H'), ('FAQQI', 'H'), \n",
    "                  ('CEGMA', 'H'), ('YLHAQ', 'H'), ('HYIHR', 'S'), ('NLAAR', 'S'), \n",
    "                  ('NVLLD', 'S'), ('NDRLV', 'C'), ('KIGDF', 'C'), ('GLAKA', 'C'), \n",
    "                  ('VPEGH', 'C'), ('EYYRV', 'C'), ('REDGD', 'C'), ('SPVFW', 'C'), \n",
    "                  ('YAPEC', 'H'), ('LKEYK', 'H'), ('FYYAS', 'H'), ('DVWSF', 'H'), \n",
    "                  ('GVTLY', 'H'), ('ELLTH', 'H'), ('CDSSQ', 'C'), ('SPPTK', 'H'), \n",
    "                  ('FLELI', 'H'), ('GLAQG', 'C'), ('QMTVL', 'H'), ('RLTEL', 'H'), \n",
    "                  ('LERGE', 'C'), ('RLPRP', 'C'), ('DKCPA', 'C'), ('EVYHL', 'H'), \n",
    "                  ('MKNCW', 'H'), ('ETEAS', 'S'), ('FRPTF', 'C'), ('ENLIP', 'H'), \n",
    "                  ('ILKTV', 'H'), ('HEKYQ', 'H'), ('GQAPS', 'C')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success prediction for test data is 22.0%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for test_5_mer, code in test_data_6aam:\n",
    "    predict = predict_5_mer(test_5_mer)\n",
    "    if predict == code:\n",
    "        correct += 1\n",
    "print('success prediction for test data is {:.1f}%'.format(100.*correct/len(test_data_6aam)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# making a prediction of the secondary structure for PDB 6aam\n",
    "\n",
    "Lets use the recent PDB structure 6AAM \"Non-receptor tyrosine-protein kinase TYK2\" \n",
    "https://www.ebi.ac.uk/pdbe/entry/pdb/6aam/\n",
    "    \n",
    "as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6aam sequence from https://www.ebi.ac.uk/pdbe/entry/pdb/6aam/protein/1\n",
    "sequence_6aam = ('GPGDPTVFHKRYLKKIRDLGEGHFGKVSLYCYDPTNDGTGEMVAVKALKADAGP'\n",
    "                 'QHRSGWKQEIDILRTLYHEHIIKYKGCCEDAGAASLQLVMEYVPLGSLRDYLPR'\n",
    "                 'HSIGLAQLLLFAQQICEGMAYLHAQHYIHRNLAARNVLLDNDRLVKIGDFGLAK'\n",
    "                 'AVPEGHEYYRVREDGDSPVFWYAPECLKEYKFYYASDVWSFGVTLYELLTHCDS'\n",
    "                 'SQSPPTKFLELIGLAQGQMTVLRLTELLERGERLPRPDKCPAEVYHLMKNCWET'\n",
    "                 'EASFRPTFENLIPILKTVHEKYQGQAPS')\n",
    "print(len(sequence_6aam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://cdn.rcsb.org/etl/kabschSander/ss_dis.txt.gz\n",
    "dssp_result_for_6aam  = \"\"\">6AAM:A:secstr\n",
    "      B  GGGEEEEEE       EEEEEEE TT     EEEEEEE      TTHHHHHHHHHHHHHH   TTB\n",
    "  EEEEEEEGGGTEEEEEEE  TT BHHHHGGGS   HHHHHHHHHHHHHHHHHHHHTTEE S  SGGGEEEEET\n",
    "TEEEE   TT EE                 GGG  HHHHHH    HHHHHHHHHHHHHHHHTTT GGGSHHHHHH\n",
    "HHH S  TT HHHHHHHHHHTT      TT  HHHHHHHHHHT SSGGGS  HHHHHHHHHHHHHHHH     \n",
    "\"\"\"\n",
    "dssp_result_for_6aam = dssp_result_for_6aam.splitlines()\n",
    "dssp_result_for_6aam.pop(0)\n",
    "dssp_result_for_6aam = ''.join(dssp_result_for_6aam)\n",
    "# need to convert DSSP code to the 3-category helix, strand, coil.\n",
    "# Use mapping \n",
    "# helices H, C, I go to H\n",
    "# strands E & bridges B go to S\n",
    "# everything else got to C\n",
    "translation = str.maketrans('HCIEB GT', 'HHHSSCCC')\n",
    "dssp_result_for_6aam = dssp_result_for_6aam.translate(translation)\n",
    "print(len(dssp_result_for_6aam))\n",
    "print(dssp_result_for_6aam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_from_6aam = []\n",
    "for ires, dssp in enumerate(dssp_result_for_6aam):\n",
    "    if ires>1 and ires%5 == 0:\n",
    "        fivemer = sequence_6aam[ires-2:ires+3]\n",
    "        test_data_from_6aam.append((fivemer, dssp))\n",
    "print(test_data_from_6aam)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_for_sequence(sequence):\n",
    "    prediction = []\n",
    "    for ires, residue in enumerate(sequence):\n",
    "        if ires < 2 or ires > len(sequence) - 3:\n",
    "            this_prediction = '.'\n",
    "        else:\n",
    "            fivemer = sequence[ires-2:ires+3]\n",
    "            testSecStrucVec = convertSeqToVector(fivemer, aaIndexDict)\n",
    "            testSecStrucArray = array( [testSecStrucVec,] )\n",
    "            sIn, sHid, sOut =    neuralNetPredict(testSecStrucArray, wMatrixIn, wMatrixOut)\n",
    "            index = sOut.argmax()\n",
    "            this_prediction = ssCodes[index]\n",
    "        prediction.append(this_prediction)\n",
    "    return ''.join(prediction)\n",
    "\n",
    "initial_predict_6aam = predict_for_sequence(sequence_6aam)\n",
    "print(initial_predict_6aam)\n",
    "print(len(initial_predict_6aam))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_line(first_seq, second_seq):\n",
    "    \"\"\" \n",
    "    for the two sequences returns a line where matching letters are \n",
    "    highlighted with | except if the letter are a gap\n",
    "    \"\"\"\n",
    "    joins = ['|' if a == b and a != '-' else ' ' for a, b in zip(first_seq, second_seq)]\n",
    "    return ''.join(joins)\n",
    "def print_alignment(seq_a, seq_b):\n",
    "    len_split = 50\n",
    "    n_splits = len(seq_a)//len_split + 1\n",
    "    for i_split in range(n_splits):\n",
    "        start = len_split*i_split\n",
    "        end = start + len_split\n",
    "        part_a = seq_a[start:end]\n",
    "        part_b = seq_b[start:end]\n",
    "        print(part_a)\n",
    "        print(highlight_line(part_a, part_b))\n",
    "        print(part_b)\n",
    "        print()\n",
    "\n",
    "print_alignment(dssp_result_for_6aam, initial_predict_6aam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def success(seq_a, seq_b):\n",
    "    same = 0\n",
    "    different = 0\n",
    "    for let_a, let_b in zip(seq_a, seq_b):\n",
    "        if let_a == let_b:\n",
    "            same += 1\n",
    "        else:\n",
    "            different += 1\n",
    "    return same/(same + different)\n",
    "print('percentage correct predictions: {:.1f}%'.format(100*success(dssp_result_for_6aam, initial_predict_6aam)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inporting a larger training set\n",
    "A much larger training set is provided as a comma separated file called:\n",
    "\"PDB_protein_secondary_5mers.csv\". \n",
    "\n",
    "    from csv import reader #may help here\n",
    "Can you create a training set vector data structure from this and use it to train the network?\n",
    "If you would like some examples of test data, here are some examples from the recently-determined PDB 6aam.pdb\n",
    "\n",
    "    S: EMVAV, KVSKY, YKGCC\n",
    "    H: LAQLL, ICEGM, ASVDW\n",
    "    C: ERLPR, GDFGL, YKFYY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have read 26242 lines from csv file\n",
      "first 5 lines [['MGKMY', 'S'], ['YGIPQ', 'C'], ['KMWTY', 'H'], ['YRLRK', 'H'], ['NSVSV', 'S']]\n"
     ]
    }
   ],
   "source": [
    "# answer\n",
    "from csv import reader\n",
    "with open('PDB_protein_secondary_5mers.csv') as csv_file:\n",
    "    csv_reader = reader(csv_file, delimiter=',')\n",
    "    data = list(csv_reader)\n",
    "print('have read', len(data), 'lines from csv file')\n",
    "print('first 5 lines', data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = small_training_set + data[:400]\n",
    "##Run this cell to create the training data\n",
    "training_vector = []\n",
    "for seq, ss in training_set:\n",
    "        inputVec = convertSeqToVector(seq, aaIndexDict)\n",
    "        outputVec = convertSeqToVector(ss, ssIndexDict)\n",
    "        training_vector.append( (inputVec, outputVec) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Error: 333.045502\n",
      "Step: 8 Error: 322.953090\n",
      "Step: 77 Error: 311.129678\n",
      "Step: 557 Error: 295.125128\n"
     ]
    }
   ],
   "source": [
    "wMatrixIn, wMatrixOut = neuralNetTrain(training_vector, 3, 1000, wInp=wMatrixIn, wOut=wMatrixOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success \"prediction\" for training data is 55.0%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for test_5_mer, code in training_set:\n",
    "    predict = predict_5_mer(test_5_mer)\n",
    "    if predict == code:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "print('success \"prediction\" for training data is {:.1f}%'.format(100.*correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success prediction for test data is 49.2%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for test_5_mer, code in test_data_6aam:\n",
    "    predict = predict_5_mer(test_5_mer)\n",
    "    if predict == code:\n",
    "        correct += 1\n",
    "print('success prediction for test data is {:.1f}%'.format(100.*correct/len(test_data_6aam)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
