{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A feed-forward Neural Network for secondary structure prediction\n",
    "This notebook looks at a Neural Network based on code from \n",
    "[Stevens and Boucher (2014, Python Programming for Biology CUP)](https://www.amazon.co.uk/Python-Programming-Biology-Bioinformatics-Beyond/dp/0521720095). The aim is to predict the secondary structure of a protein from its sequence. \n",
    "A Predictive network is trained using data on known secondary structure of k-mers of 5 amino-acids taken from a set of PDB structures. Three secondary structure states are defined: H, C, and S. H and S are helix and strand respectively while C is for coil which is a range of structures not with regular H-bonding pattern. In practice, secondary structure prediction has many uses, for instance in helping in the identification of functional domains ([Drozdetskiy et al., 2015](https://doi.org/10.1093/nar/gkv332)) and can be easily acheived using the JPRED4 server http://www.compbio.dundee.ac.uk/jpred4\n",
    "\n",
    "Secondary structure is much more complicated than indicated by the simple classification of this data - full details are available from the analysis of the H-bonding arrangements. The program DSSP (https://swift.cmbi.umcn.nl/gv/dssp/DSSP_3.html) is a well-tested approach to this problem. This produces a description of the secondary structure in a known protein structure. For historical reasons DSSP uses E for Strands. A related DSSR program gives RNA secondary structure.\n",
    "\n",
    "It is useful to be able to predict the secondary structure of a protein for which there is only sequence available. One approach would be to align it with homologous sequences where the structure is known. The approach here is to use the sequence in the neighbourhood of a residue as a basis for a neural network prediction. \n",
    "\n",
    "The network here is a simple three layer feed-forward one. The number of nodes in the hidden layer can be defined by the programmer. But the number of input and output nodes is defined by the sizes of the input and output data vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run this cell to import numpy \n",
    "from numpy import tanh, ones, append, array, zeros, random, sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network function takes input data for the first layer of Network nodes, applies the the first weighted connections to pass the signal to the hidden layer of nodes, then applies the second weights to produce output. \n",
    "\n",
    "The output may not be optimized as the function also operates on the weighting during training. However after training the function gives predictions so takes its name from that. \n",
    "\n",
    "The weightsIn values define the strength of connection between the input nodes and the hidden nodes. Similarly weightsOut define the strengths of connection between the hidden and the output nodes. \n",
    "\n",
    "The weights are given as matrices with the rows indexing the nodes in a layer and the columns indexing the nodes in the other layer. \n",
    "\n",
    "The signalIn vector is the input features and an extra value of 1.0. This additional value is called the bias node which is used to tune the baseline response of the network. The baseline is the level without a meaningful signal. \n",
    "\n",
    "Setting the bias node value happens during training to adapt to the values in the input data. This means the input data don't need to pre-prepared with a mean of zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this cell to define the function\n",
    "def neuralNetPredict(inputVec, weightsIn, weightsOut):\n",
    "    \"\"\" uses the current weights in a neural network\n",
    "    to make a prediction from an input vector\n",
    "    all input and output are numpy data structures\"\"\" \n",
    "    signalIn = append(inputVec, 1.0) # input layer\n",
    "\n",
    "    prod = signalIn * weightsIn.T\n",
    "    sums = sum(prod, axis=1)\n",
    "    signalHid = tanh(sums)    # hidden    layer\n",
    "\n",
    "    prod = signalHid * weightsOut.T\n",
    "    sums = sum(prod, axis=1)\n",
    "    signalOut = tanh(sums)    # output    layer\n",
    "\n",
    "    return signalIn, signalHid, signalOut\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the numpy `.T` methodn gives the transpose of a matrix - that is the matrix with the columns turned into rows and the rows turned into columns. \n",
    "\n",
    "This is used so that the input signal gets applied to all the hidden nodes.\n",
    "\n",
    "The network applies the hyperbolic tangent function (tanh) to get the signal output from all the nodes in layer. Hyperbolic tan is a sigmoidal function that varies from -1 to 1, so it is much better than ordinary tan that runs off to infinity. \n",
    "\n",
    "<img src=\"https://mathworld.wolfram.com/images/interactive/TanhReal.gif\" width=300></img> \n",
    "\n",
    "The tanh function defines the output of that node given an input or set of inputs. As a nod to the output of neurones, which depends on an activation level across their cell membrane, the output function is called the *Activation* function.\n",
    "\n",
    "In operation only the signalOut from the output layer is of interest. But during training the response signals from the other layers are also needed to adjust the weighting scheme.\n",
    "\n",
    "### Training \n",
    "\n",
    "The weighting scheme (and gain) will be optimized by using a training dataset. \n",
    "\n",
    "The training data will be an input feature vector and a known output vector. The order of the data will be randomly shuffled to avoid bias. The number of hidden nodes needs to be specified and the number of optimization cycles. \n",
    "\n",
    "After each cycle the 'error' between the output signal of the network and the known training set output is used to adjust the network weights. The difference is combined with the *gradient* in the signal values - calculated from the tanh activation function (conveniently the gradient of tanh(sig) is 1-sig<sup>2</sup> or 1 - {sig x sig}). \n",
    "\n",
    "Early in training large difference can make the network go haywire so the speed of weight changing is damped down by a 'rate' and 'momentum' multipliers (usually the default values of 0.5 and 0.2 are good enough). \n",
    "\n",
    "More damping would mean that many more cycles would be needed for the weights to converge. \n",
    "\n",
    "The training will work back from the value of the error to adjust the weighting scheme of the network. This is called *back propagation*. \n",
    "\n",
    "The use of the gradient is crucial as it means initial adjustments will be large but then finer adjustments will be made as the optimum is approached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this cell to define the neuralNetTrain function\n",
    "def neuralNetTrain(trainData, numHid, steps=100, rate=0.5, momentum=0.2, wInp=None, wOut=None):\n",
    "    \"\"\" uses training data to set the weights in a simple\n",
    "    neural network, number of hidden nodes is specified\"\"\"\n",
    "    numInp = len(trainData[0][0])\n",
    "    numOut = len(trainData[0][1])\n",
    "    numInp += 1\n",
    "    minError = None\n",
    "\n",
    "    sigInp = ones(numInp)\n",
    "    sigHid = ones(numHid)\n",
    "    sigOut = ones(numOut)\n",
    "    \n",
    "    if wInp is None:\n",
    "        wInp = random.random((numInp, numHid))-0.5\n",
    "    \n",
    "    if wOut is None:\n",
    "        wOut = random.random((numHid, numOut))-0.5\n",
    "    bestWeightMatrices = (wInp, wOut)\n",
    "\n",
    "    cInp = zeros((numInp, numHid))\n",
    "    cOut = zeros((numHid, numOut))\n",
    "\n",
    "    for x, (inputs, knownOut) in enumerate(trainData):\n",
    "        trainData[x] = (array(inputs), array(knownOut))\n",
    " \n",
    "    for step in range(steps):  \n",
    "        random.shuffle(trainData) # Important to avoid bias\n",
    "        error = 0.0\n",
    " \n",
    "        for inputs, knownOut in trainData:\n",
    "            sigIn, sigHid, sigOut = neuralNetPredict(inputs, wInp, wOut)\n",
    "\n",
    "            diff = knownOut - sigOut\n",
    "            error += sum(diff * diff)\n",
    "\n",
    "            gradient = ones(numOut) - (sigOut*sigOut)\n",
    "            outAdjust = gradient * diff \n",
    "\n",
    "            diff = sum(outAdjust * wOut, axis=1)\n",
    "            gradient = ones(numHid) - (sigHid*sigHid)\n",
    "            hidAdjust = gradient * diff \n",
    "\n",
    "            # update output \n",
    "            change = outAdjust * sigHid.reshape(numHid, 1)\n",
    "            wOut += (rate * change) + (momentum * cOut)\n",
    "            cOut = change\n",
    " \n",
    "            # update input \n",
    "            change = hidAdjust * sigIn.reshape(numInp, 1)\n",
    "            wInp += (rate * change) + (momentum * cInp)\n",
    "            cInp = change\n",
    " \n",
    "        if (minError is None) or (error < minError):\n",
    "            minError = error\n",
    "            bestWeightMatrices = (wInp.copy(), wOut.copy())\n",
    "            print(\"Step: %d Error: %f\" % (step, error))\n",
    "    \n",
    "    return bestWeightMatrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the functions\n",
    "Simple data to test a network can be binary input vectors with the desired output being an 'exclusive OR' (EOR) response https://en.wikipedia.org/wiki/Exclusive_or. This responds True if any input is true but False is both are together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run this cell to define the test data\n",
    "testEORdata = [[[0,0], [0]],\n",
    "               [[0,1], [1]], \n",
    "               [[1,0], [1]], \n",
    "               [[1,1], [0]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network test uses two hidden nodes - in real use several values would be tried to find the best performance.\n",
    "Run the cell below to see if the training converges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Error: 1.784879\n",
      "Step: 1 Error: 1.290609\n",
      "Step: 2 Error: 1.211001\n",
      "Step: 3 Error: 1.201999\n",
      "Step: 5 Error: 1.098999\n",
      "Step: 9 Error: 1.047462\n",
      "Step: 11 Error: 1.034072\n",
      "Step: 12 Error: 0.994271\n",
      "Step: 13 Error: 0.894744\n",
      "Step: 15 Error: 0.884094\n",
      "Step: 16 Error: 0.880256\n",
      "Step: 17 Error: 0.853571\n",
      "Step: 21 Error: 0.830573\n",
      "Step: 24 Error: 0.803426\n",
      "Step: 29 Error: 0.777918\n",
      "Step: 30 Error: 0.764454\n",
      "Step: 32 Error: 0.763145\n",
      "Step: 35 Error: 0.742660\n",
      "Step: 39 Error: 0.727669\n",
      "Step: 41 Error: 0.720235\n",
      "Step: 45 Error: 0.681146\n",
      "Step: 46 Error: 0.662529\n",
      "Step: 47 Error: 0.614150\n",
      "Step: 50 Error: 0.557497\n",
      "Step: 52 Error: 0.436446\n",
      "Step: 60 Error: 0.365064\n",
      "Step: 63 Error: 0.222438\n",
      "Step: 69 Error: 0.212613\n",
      "Step: 70 Error: 0.139005\n",
      "Step: 79 Error: 0.097076\n",
      "Step: 83 Error: 0.060757\n",
      "Step: 93 Error: 0.054632\n",
      "Step: 100 Error: 0.050386\n",
      "Step: 103 Error: 0.047377\n",
      "Step: 110 Error: 0.032306\n",
      "Step: 113 Error: 0.030242\n",
      "Step: 117 Error: 0.028321\n",
      "Step: 128 Error: 0.024120\n",
      "Step: 140 Error: 0.023153\n",
      "Step: 141 Error: 0.021031\n",
      "Step: 145 Error: 0.019311\n",
      "Step: 151 Error: 0.013827\n",
      "Step: 157 Error: 0.012991\n",
      "Step: 164 Error: 0.012927\n",
      "Step: 170 Error: 0.011279\n",
      "Step: 173 Error: 0.011013\n",
      "Step: 184 Error: 0.010536\n",
      "Step: 185 Error: 0.010395\n",
      "Step: 188 Error: 0.010068\n",
      "Step: 194 Error: 0.009771\n",
      "Step: 195 Error: 0.007410\n",
      "Step: 213 Error: 0.007321\n",
      "Step: 214 Error: 0.007283\n",
      "Step: 215 Error: 0.007157\n",
      "Step: 223 Error: 0.006288\n",
      "Step: 225 Error: 0.005940\n",
      "Step: 229 Error: 0.005916\n",
      "Step: 233 Error: 0.005691\n",
      "Step: 244 Error: 0.005075\n",
      "Step: 246 Error: 0.004984\n",
      "Step: 250 Error: 0.004249\n",
      "Step: 271 Error: 0.003785\n",
      "Step: 286 Error: 0.003781\n",
      "Step: 291 Error: 0.003698\n",
      "Step: 294 Error: 0.003638\n",
      "Step: 295 Error: 0.003150\n",
      "Step: 315 Error: 0.003050\n",
      "Step: 321 Error: 0.002735\n",
      "Step: 333 Error: 0.002692\n",
      "Step: 344 Error: 0.002682\n",
      "Step: 352 Error: 0.002622\n",
      "Step: 353 Error: 0.002555\n",
      "Step: 360 Error: 0.002482\n",
      "Step: 361 Error: 0.002293\n",
      "Step: 371 Error: 0.002171\n",
      "Step: 395 Error: 0.002139\n",
      "Step: 398 Error: 0.001928\n",
      "Step: 413 Error: 0.001919\n",
      "Step: 424 Error: 0.001835\n",
      "Step: 429 Error: 0.001765\n",
      "Step: 436 Error: 0.001747\n",
      "Step: 440 Error: 0.001640\n",
      "Step: 459 Error: 0.001627\n",
      "Step: 461 Error: 0.001612\n",
      "Step: 465 Error: 0.001611\n",
      "Step: 468 Error: 0.001607\n",
      "Step: 472 Error: 0.001560\n",
      "Step: 475 Error: 0.001533\n",
      "Step: 481 Error: 0.001532\n",
      "Step: 482 Error: 0.001492\n",
      "Step: 494 Error: 0.001391\n",
      "Step: 511 Error: 0.001336\n",
      "Step: 529 Error: 0.001318\n",
      "Step: 534 Error: 0.001263\n",
      "Step: 538 Error: 0.001248\n",
      "Step: 543 Error: 0.001242\n",
      "Step: 557 Error: 0.001222\n",
      "Step: 567 Error: 0.001209\n",
      "Step: 576 Error: 0.001177\n",
      "Step: 591 Error: 0.001154\n",
      "Step: 593 Error: 0.001128\n",
      "Step: 595 Error: 0.001103\n",
      "Step: 608 Error: 0.001079\n",
      "Step: 610 Error: 0.001073\n",
      "Step: 631 Error: 0.001051\n",
      "Step: 635 Error: 0.000983\n",
      "Step: 647 Error: 0.000979\n",
      "Step: 654 Error: 0.000944\n",
      "Step: 686 Error: 0.000897\n",
      "Step: 707 Error: 0.000891\n",
      "Step: 719 Error: 0.000874\n",
      "Step: 722 Error: 0.000869\n",
      "Step: 727 Error: 0.000866\n",
      "Step: 734 Error: 0.000861\n",
      "Step: 735 Error: 0.000853\n",
      "Step: 738 Error: 0.000850\n",
      "Step: 739 Error: 0.000832\n",
      "Step: 752 Error: 0.000831\n",
      "Step: 759 Error: 0.000816\n",
      "Step: 761 Error: 0.000800\n",
      "Step: 765 Error: 0.000791\n",
      "Step: 779 Error: 0.000775\n",
      "Step: 783 Error: 0.000756\n",
      "Step: 797 Error: 0.000748\n",
      "Step: 819 Error: 0.000711\n",
      "Step: 848 Error: 0.000711\n",
      "Step: 851 Error: 0.000702\n",
      "Step: 853 Error: 0.000690\n",
      "Step: 874 Error: 0.000673\n",
      "Step: 890 Error: 0.000668\n",
      "Step: 891 Error: 0.000664\n",
      "Step: 893 Error: 0.000664\n",
      "Step: 903 Error: 0.000651\n",
      "Step: 910 Error: 0.000646\n",
      "Step: 917 Error: 0.000639\n",
      "Step: 920 Error: 0.000637\n",
      "Step: 930 Error: 0.000624\n",
      "Step: 946 Error: 0.000596\n",
      "Step: 966 Error: 0.000589\n",
      "Step: 970 Error: 0.000589\n",
      "Step: 991 Error: 0.000579\n"
     ]
    }
   ],
   "source": [
    "## Run this cell to train the network\n",
    "wMatrixIn, wMatrixOut = neuralNetTrain(testEORdata, 2, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here quite good convergence has occurred and there is no oscillation. Perhaps you can see that the initial steps are giving large changes in the error while later on there are smaller and smaller changes. This is owing to the effect of the gradient calculation. The changes in the actual weights are not printed, but will follow the same trend.\n",
    "\n",
    "The output weight matrices can then be run on test data for evaluation. Test data should be inputs with known output but which were not included in the training set. \n",
    "\n",
    "Obviously it is not possible to give any new data for the EOR function as the training set covered all possible responses!\n",
    "\n",
    "But the trained network should be able to do a reasonable job on the training set.\n",
    "Run the following cell to compare the output of the network with the actual values of the training set outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input [0 0]  should have output  [0] actual output -0.001\n",
      "input [0 1]  should have output  [1] actual output 0.983\n",
      "input [1 1]  should have output  [0] actual output 0.003\n",
      "input [1 0]  should have output  [1] actual output 0.983\n"
     ]
    }
   ],
   "source": [
    "##Run this cell to test the network\n",
    "for inputs, knownOut in testEORdata:\n",
    "    sIn, sHid, sOut =    neuralNetPredict(array(inputs), wMatrixIn, wMatrixOut)\n",
    "    print('input', inputs, ' should have output ', knownOut, 'actual output {:.3f}'.format(sOut[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple feature vectors for sequence data\n",
    "A simple numbering scheme is used to convert to the sequence alphabet to a numeric form as an input vector. For proteins that is number from 1 to 20 from the list of one-letter codes.\n",
    "\n",
    "k-mers with k=5 are used. Only the output for the middle residue is required but the network will use the neighbours to predict the secondary structure of the middle one.  \n",
    "\n",
    "Although static k-mer are used for training in practice a prediction in a moving 5-mer window could be implemented. \n",
    "\n",
    "The possible outputs are also coded as integers for the more restricted alphabet of H, C, and S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run this cell to define the dictionaries for the vectors\n",
    "aminoAcids = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "aaIndexDict = {}\n",
    "for i, aa in enumerate(aminoAcids):\n",
    "        aaIndexDict[aa] = i\n",
    "\n",
    "ssIndexDict = {}\n",
    "ssCodes = 'HCS'\n",
    "for i, code in enumerate(ssCodes):\n",
    "        ssIndexDict[code] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a very limited set of training data. It shows the raw format which is a 5-mer string and the secondary structure that was observed for the central residue of this in at least one PDB structure. \n",
    "\n",
    "The actual structure is a simplified output from the DSSP program mentioned in the introduction. DSSP acutally distinguishes more structures that the three here - for example there are other kinds of helix. But these complications are not dealt with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run this cell to define the training set\n",
    "small_training_set = [('ADTLL','S'),\n",
    "                      ('DTLLI','S'),\n",
    "                      ('TLLIL','S'),\n",
    "                      ('LLILG','S'),\n",
    "                      ('LILGD','S'),\n",
    "                      ('ILGDS','S'),\n",
    "                      ('LGDSL','C'),\n",
    "                      ('GDSLS','H'),\n",
    "                      ('DSLSA','H'),\n",
    "                      ('SLSAG','H'),\n",
    "                      ('LSAGY','H'),\n",
    "                      ('SAGYR','C'),\n",
    "                      ('AGYRM','C'),\n",
    "                      ('GYRMS','C'),\n",
    "                      ('YRMSA','C'),\n",
    "                      ('RMSAS','C')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data has to be converted to the numerical code. Here is a function to to that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run this cell to define the function\n",
    "def convertSeqToVector(seq, indexDict):\n",
    "    \"\"\"converts a one-letter sequence to numerical\n",
    "    coding for neural network calculations\"\"\"   \n",
    "    numLetters = len(indexDict)\n",
    "    vector = [0.0] * len(seq) * numLetters\n",
    "\n",
    "    for pos, letter in enumerate(seq):\n",
    "        index = pos * numLetters + indexDict[letter]    \n",
    "        vector[index] = 1.0\n",
    "\n",
    "    return vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data is prepared with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run this cell to create the training data\n",
    "small_training_vector = []\n",
    "for seq, ss in small_training_set:\n",
    " \n",
    "        inputVec = convertSeqToVector(seq, aaIndexDict)\n",
    "        outputVec = convertSeqToVector(ss, ssIndexDict)\n",
    "\n",
    "        small_training_vector.append( (inputVec, outputVec) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then the network is trained. Here there are 3 hidden nodes specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Error: 16.884136\n",
      "Step: 2 Error: 11.528012\n",
      "Step: 3 Error: 6.486692\n",
      "Step: 5 Error: 3.891270\n",
      "Step: 6 Error: 2.404141\n",
      "Step: 8 Error: 2.116589\n",
      "Step: 11 Error: 1.255028\n",
      "Step: 12 Error: 0.543938\n",
      "Step: 15 Error: 0.339015\n",
      "Step: 16 Error: 0.324979\n",
      "Step: 17 Error: 0.230384\n",
      "Step: 19 Error: 0.191600\n",
      "Step: 25 Error: 0.125040\n",
      "Step: 30 Error: 0.106283\n",
      "Step: 31 Error: 0.076249\n",
      "Step: 41 Error: 0.067321\n",
      "Step: 66 Error: 0.057802\n",
      "Step: 67 Error: 0.048827\n",
      "Step: 76 Error: 0.021699\n",
      "Step: 146 Error: 0.018714\n",
      "Step: 153 Error: 0.012945\n",
      "Step: 185 Error: 0.010329\n",
      "Step: 187 Error: 0.009675\n",
      "Step: 213 Error: 0.007237\n",
      "Step: 294 Error: 0.006668\n",
      "Step: 345 Error: 0.006321\n",
      "Step: 411 Error: 0.005723\n",
      "Step: 450 Error: 0.005056\n",
      "Step: 459 Error: 0.004171\n",
      "Step: 468 Error: 0.002074\n",
      "Step: 674 Error: 0.001772\n",
      "Step: 848 Error: 0.001646\n",
      "Step: 915 Error: 0.001432\n"
     ]
    }
   ],
   "source": [
    "wMatrixIn, wMatrixOut = neuralNetTrain(small_training_vector, 3, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see that this training has converged nicely. The only problem is that it was for a very restricted set of sequence data. \n",
    "\n",
    "There are 3 x 20 = 60 theoretical combinations of amino acid residues with secondary structure states. But for particular residues some of these are favoured and others disfavoured. \n",
    "\n",
    "For each residue there will be 20^4 = 160 000 different contexts that then could possibly occur in. Although some of the resulting 5-mers are actually quite rare in structured proteins. \n",
    "\n",
    "All the same, it would be good to have a larger training set. It is better to have some rare examples of residue state combinations although, of course, the network will not have to predict them frequently.\n",
    "\n",
    "One thing to remember is to retain some test examples - where the answer is known but which are not in the training set. \n",
    "\n",
    "In its current, poorly-trained state, the network is still able to make a reasonable predictions. But only if the test is clearly related to examples that it has seen. testSecStrucSeq here is very similar to examples in the seqSecStrucData training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to define \n",
    "def predict_seq(seq, w_matrix_in, w_matrix_out):\n",
    "    \"\"\"\n",
    "    returns a prediction either 'H', 'S' or 'C' for the input sequence\n",
    "    \"\"\"\n",
    "    vector_seq = convertSeqToVector(seq, indexDict=aaIndexDict)\n",
    "    array_seq = array([vector_seq,])\n",
    "    sIn, sHid, sOut =    neuralNetPredict(array_seq, w_matrix_in, w_matrix_out)\n",
    "    index = sOut.argmax()\n",
    "    return ssCodes[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADTLL input  S  predict  S\n",
      "DTLLI input  S  predict  S\n",
      "TLLIL input  S  predict  S\n",
      "LLILG input  S  predict  S\n",
      "LILGD input  S  predict  S\n",
      "ILGDS input  S  predict  S\n",
      "LGDSL input  C  predict  C\n",
      "GDSLS input  H  predict  H\n",
      "DSLSA input  H  predict  H\n",
      "SLSAG input  H  predict  H\n",
      "LSAGY input  H  predict  H\n",
      "SAGYR input  C  predict  C\n",
      "AGYRM input  C  predict  C\n",
      "GYRMS input  C  predict  C\n",
      "YRMSA input  C  predict  C\n",
      "RMSAS input  C  predict  C\n",
      "success \"prediction\" for training data is 100.0%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for test_5_mer, code in small_training_set:\n",
    "    predict = predict_seq(test_5_mer, wMatrixIn, wMatrixOut)\n",
    "    if predict == code:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "    print(test_5_mer, 'input ', code, ' predict ', predict)\n",
    "print('success \"prediction\" for training data is {:.1f}%'.format(100.*correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction for DLLSA is S\n"
     ]
    }
   ],
   "source": [
    "test_expect_h = 'DLLSA'\n",
    "print('prediction for', test_expect_h, 'is', predict_seq(test_expect_h, wMatrixIn, wMatrixOut))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data from PDB structure 6aam, divided into 5-mers\n",
    "test_data_6aam = [('DPTVF', 'C'), ('HKRYL', 'C'), ('KKIRD', 'S'), ('LGEGH', 'C'), \n",
    "                  ('FGKVS', 'S'), ('LYCYD', 'S'), ('PTNDG', 'C'), ('TGEMV', 'S'), \n",
    "                  ('AVKAL', 'S'), ('KADAG', 'C'), ('PQHRS', 'H'), ('GWKQE', 'H'), \n",
    "                  ('IDILR', 'H'), ('TLYHE', 'C'), ('HIIKY', 'C'), ('KGCCE', 'S'), \n",
    "                  ('DAGAA', 'C'), ('SLQLV', 'S'), ('MEYVP', 'C'), ('LGSLR', 'S'), \n",
    "                  ('DYLPR', 'C'), ('HSIGL', 'C'), ('AQLLL', 'H'), ('FAQQI', 'H'), \n",
    "                  ('CEGMA', 'H'), ('YLHAQ', 'H'), ('HYIHR', 'S'), ('NLAAR', 'S'), \n",
    "                  ('NVLLD', 'S'), ('NDRLV', 'C'), ('KIGDF', 'C'), ('GLAKA', 'C'), \n",
    "                  ('VPEGH', 'C'), ('EYYRV', 'C'), ('REDGD', 'C'), ('SPVFW', 'C'), \n",
    "                  ('YAPEC', 'H'), ('LKEYK', 'H'), ('FYYAS', 'H'), ('DVWSF', 'H'), \n",
    "                  ('GVTLY', 'H'), ('ELLTH', 'H'), ('CDSSQ', 'C'), ('SPPTK', 'H'), \n",
    "                  ('FLELI', 'H'), ('GLAQG', 'C'), ('QMTVL', 'H'), ('RLTEL', 'H'), \n",
    "                  ('LERGE', 'C'), ('RLPRP', 'C'), ('DKCPA', 'C'), ('EVYHL', 'H'), \n",
    "                  ('MKNCW', 'H'), ('ETEAS', 'S'), ('FRPTF', 'C'), ('ENLIP', 'H'), \n",
    "                  ('ILKTV', 'H'), ('HEKYQ', 'H'), ('GQAPS', 'C')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success prediction for 6AAM test data is 32.2%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for test_5_mer, code in test_data_6aam:\n",
    "    predict = predict_seq(test_5_mer, wMatrixIn, wMatrixOut)\n",
    "    if predict == code:\n",
    "        correct += 1\n",
    "print('success prediction for 6AAM test data is {:.1f}%'.format(100.*correct/len(test_data_6aam)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# making a prediction of the secondary structure for PDB 6aam\n",
    "\n",
    "Lets use the recent PDB structure 6AAM \"Non-receptor tyrosine-protein kinase TYK2\" \n",
    "https://www.ebi.ac.uk/pdbe/entry/pdb/6aam/\n",
    "    \n",
    "as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6aam sequence from https://www.ebi.ac.uk/pdbe/entry/pdb/6aam/protein/1\n",
    "sequence_6aam = ('GPGDPTVFHKRYLKKIRDLGEGHFGKVSLYCYDPTNDGTGEMVAVKALKADAGP'\n",
    "                 'QHRSGWKQEIDILRTLYHEHIIKYKGCCEDAGAASLQLVMEYVPLGSLRDYLPR'\n",
    "                 'HSIGLAQLLLFAQQICEGMAYLHAQHYIHRNLAARNVLLDNDRLVKIGDFGLAK'\n",
    "                 'AVPEGHEYYRVREDGDSPVFWYAPECLKEYKFYYASDVWSFGVTLYELLTHCDS'\n",
    "                 'SQSPPTKFLELIGLAQGQMTVLRLTELLERGERLPRPDKCPAEVYHLMKNCWET'\n",
    "                 'EASFRPTFENLIPILKTVHEKYQGQAPS')\n",
    "print(len(sequence_6aam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://cdn.rcsb.org/etl/kabschSander/ss_dis.txt.gz\n",
    "dssp_result_for_6aam  = \"\"\">6AAM:A:secstr\n",
    "      B  GGGEEEEEE       EEEEEEE TT     EEEEEEE      TTHHHHHHHHHHHHHH   TTB\n",
    "  EEEEEEEGGGTEEEEEEE  TT BHHHHGGGS   HHHHHHHHHHHHHHHHHHHHTTEE S  SGGGEEEEET\n",
    "TEEEE   TT EE                 GGG  HHHHHH    HHHHHHHHHHHHHHHHTTT GGGSHHHHHH\n",
    "HHH S  TT HHHHHHHHHHTT      TT  HHHHHHHHHHT SSGGGS  HHHHHHHHHHHHHHHH     \n",
    "\"\"\"\n",
    "dssp_result_for_6aam = dssp_result_for_6aam.splitlines()\n",
    "dssp_result_for_6aam.pop(0)\n",
    "dssp_result_for_6aam = ''.join(dssp_result_for_6aam)\n",
    "# need to convert DSSP code to the 3-category helix, strand, coil.\n",
    "# Use mapping \n",
    "# helices H, C, I go to H\n",
    "# strands E & bridges B go to S\n",
    "# everything else got to C\n",
    "translation = str.maketrans('HCIEB GT', 'HHHSSCCC')\n",
    "dssp_result_for_6aam = dssp_result_for_6aam.translate(translation)\n",
    "print(len(dssp_result_for_6aam))\n",
    "print(dssp_result_for_6aam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_from_6aam = []\n",
    "for ires, dssp in enumerate(dssp_result_for_6aam):\n",
    "    if ires>1 and ires%5 == 0:\n",
    "        fivemer = sequence_6aam[ires-2:ires+3]\n",
    "        test_data_from_6aam.append((fivemer, dssp))\n",
    "print(test_data_from_6aam)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_for_sequence(sequence):\n",
    "    prediction = []\n",
    "    for ires, residue in enumerate(sequence):\n",
    "        if ires < 2 or ires > len(sequence) - 3:\n",
    "            this_prediction = '.'\n",
    "        else:\n",
    "            fivemer = sequence[ires-2:ires+3]\n",
    "            testSecStrucVec = convertSeqToVector(fivemer, aaIndexDict)\n",
    "            testSecStrucArray = array( [testSecStrucVec,] )\n",
    "            sIn, sHid, sOut =    neuralNetPredict(testSecStrucArray, wMatrixIn, wMatrixOut)\n",
    "            index = sOut.argmax()\n",
    "            this_prediction = ssCodes[index]\n",
    "        prediction.append(this_prediction)\n",
    "    return ''.join(prediction)\n",
    "\n",
    "initial_predict_6aam = predict_for_sequence(sequence_6aam)\n",
    "print(initial_predict_6aam)\n",
    "print(len(initial_predict_6aam))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_line(first_seq, second_seq):\n",
    "    \"\"\" \n",
    "    for the two sequences returns a line where matching letters are \n",
    "    highlighted with | except if the letter are a gap\n",
    "    \"\"\"\n",
    "    joins = ['|' if a == b and a != '-' else ' ' for a, b in zip(first_seq, second_seq)]\n",
    "    return ''.join(joins)\n",
    "def print_alignment(seq_a, seq_b):\n",
    "    len_split = 50\n",
    "    n_splits = len(seq_a)//len_split + 1\n",
    "    for i_split in range(n_splits):\n",
    "        start = len_split*i_split\n",
    "        end = start + len_split\n",
    "        part_a = seq_a[start:end]\n",
    "        part_b = seq_b[start:end]\n",
    "        print(part_a)\n",
    "        print(highlight_line(part_a, part_b))\n",
    "        print(part_b)\n",
    "        print()\n",
    "\n",
    "print_alignment(dssp_result_for_6aam, initial_predict_6aam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def success(seq_a, seq_b):\n",
    "    same = 0\n",
    "    different = 0\n",
    "    for let_a, let_b in zip(seq_a, seq_b):\n",
    "        if let_a == let_b:\n",
    "            same += 1\n",
    "        else:\n",
    "            different += 1\n",
    "    return same/(same + different)\n",
    "print('percentage correct predictions: {:.1f}%'.format(100*success(dssp_result_for_6aam, initial_predict_6aam)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inporting a larger training set\n",
    "A much larger training set is provided as a comma separated file called:\n",
    "\"PDB_protein_secondary_5mers.csv\". \n",
    "\n",
    "    from csv import reader #may help here\n",
    "Can you create a training set vector data structure from this and use it to train the network?\n",
    "If you would like some examples of test data, here are some examples from the recently-determined PDB 6aam.pdb\n",
    "\n",
    "    S: EMVAV, KVSKY, YKGCC\n",
    "    H: LAQLL, ICEGM, ASVDW\n",
    "    C: ERLPR, GDFGL, YKFYY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have read 26242 lines from csv file\n",
      "first 5 lines [['MGKMY', 'S'], ['YGIPQ', 'C'], ['KMWTY', 'H'], ['YRLRK', 'H'], ['NSVSV', 'S']]\n"
     ]
    }
   ],
   "source": [
    "# answer\n",
    "from csv import reader\n",
    "with open('PDB_protein_secondary_5mers.csv') as csv_file:\n",
    "    csv_reader = reader(csv_file, delimiter=',')\n",
    "    data = list(csv_reader)\n",
    "print('have read', len(data), 'lines from csv file')\n",
    "print('first 5 lines', data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "shuffled = data[:]\n",
    "shuffle(shuffled)\n",
    "training_set = small_training_set + data[:400]\n",
    "##Run this cell to create the training data\n",
    "training_vector = []\n",
    "for seq, ss in training_set:\n",
    "        inputVec = convertSeqToVector(seq, aaIndexDict)\n",
    "        outputVec = convertSeqToVector(ss, ssIndexDict)\n",
    "        training_vector.append( (inputVec, outputVec) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Error: 341.594060\n",
      "Step: 1 Error: 319.603416\n",
      "Step: 2 Error: 301.404242\n",
      "Step: 3 Error: 273.661071\n",
      "Step: 5 Error: 265.416957\n",
      "Step: 6 Error: 262.273802\n",
      "Step: 7 Error: 259.922359\n",
      "Step: 8 Error: 259.370990\n",
      "Step: 9 Error: 249.706879\n",
      "Step: 10 Error: 237.085130\n",
      "Step: 13 Error: 228.858834\n",
      "Step: 15 Error: 223.070449\n",
      "Step: 18 Error: 215.953852\n",
      "Step: 22 Error: 215.476783\n",
      "Step: 23 Error: 214.466913\n",
      "Step: 26 Error: 199.090300\n",
      "Step: 29 Error: 196.353325\n",
      "Step: 31 Error: 193.363168\n",
      "Step: 34 Error: 188.348423\n",
      "Step: 37 Error: 186.696155\n",
      "Step: 38 Error: 186.312441\n",
      "Step: 41 Error: 184.288669\n",
      "Step: 42 Error: 179.659380\n",
      "Step: 43 Error: 176.120485\n",
      "Step: 47 Error: 175.584359\n",
      "Step: 48 Error: 169.666680\n",
      "Step: 54 Error: 166.831194\n",
      "Step: 62 Error: 163.882696\n",
      "Step: 70 Error: 158.097456\n",
      "Step: 71 Error: 157.732503\n",
      "Step: 78 Error: 157.359282\n",
      "Step: 81 Error: 152.324292\n",
      "Step: 93 Error: 150.910565\n",
      "Step: 94 Error: 147.931380\n",
      "Step: 101 Error: 147.610147\n",
      "Step: 103 Error: 147.237361\n",
      "Step: 111 Error: 142.572788\n",
      "Step: 130 Error: 139.867259\n",
      "Step: 139 Error: 139.000322\n",
      "Step: 157 Error: 134.442084\n",
      "Step: 176 Error: 132.859406\n",
      "Step: 181 Error: 132.780986\n",
      "Step: 211 Error: 131.607650\n",
      "Step: 231 Error: 126.907649\n",
      "Step: 264 Error: 123.398958\n",
      "Step: 305 Error: 122.151862\n",
      "Step: 348 Error: 121.420252\n",
      "Step: 441 Error: 121.240776\n",
      "Step: 473 Error: 120.846738\n",
      "Step: 544 Error: 119.674072\n",
      "Step: 639 Error: 119.455380\n",
      "Step: 728 Error: 117.690227\n",
      "Step: 836 Error: 117.633786\n",
      "Step: 916 Error: 117.482049\n"
     ]
    }
   ],
   "source": [
    "wMatrixIn, wMatrixOut = neuralNetTrain(training_vector, 3, 1000, rate=0.125, momentum=0.2)\n",
    "#\n",
    "#wMatrixIn, wMatrixOut = neuralNetTrain(training_vector, 3, 1000, wInp=wMatrixIn, wOut=wMatrixOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADTLL input  S  predict  H\n",
      "DTLLI input  S  predict  S\n",
      "TLLIL input  S  predict  S\n",
      "LLILG input  S  predict  S\n",
      "LILGD input  S  predict  S\n",
      "ILGDS input  S  predict  C\n",
      "LGDSL input  C  predict  C\n",
      "GDSLS input  H  predict  C\n",
      "DSLSA input  H  predict  H\n",
      "SLSAG input  H  predict  H\n",
      "LSAGY input  H  predict  H\n",
      "SAGYR input  C  predict  C\n",
      "AGYRM input  C  predict  S\n",
      "GYRMS input  C  predict  C\n",
      "YRMSA input  C  predict  C\n",
      "RMSAS input  C  predict  C\n",
      "success \"prediction\" for training data is 75.0%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for test_5_mer, code in small_training_set:\n",
    "    predict = predict_seq(test_5_mer, wMatrixIn, wMatrixOut)\n",
    "    if predict == code:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "    print(test_5_mer, 'input ', code, ' predict ', predict)\n",
    "print('success \"prediction\" for training data is {:.1f}%'.format(100.*correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success prediction for 6AAM test data is 49.2%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for test_5_mer, code in test_data_6aam:\n",
    "    predict = predict_seq(test_5_mer, wMatrixIn, wMatrixOut)\n",
    "    if predict == code:\n",
    "        correct += 1\n",
    "print('success prediction for 6AAM test data is {:.1f}%'.format(100.*correct/len(test_data_6aam)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All coil prediction for 6AAM test data has success 42.4%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for test_5_mer, code in test_data_6aam:\n",
    "    if code == 'C':\n",
    "        correct += 1\n",
    "print('All coil prediction for 6AAM test data has success {:.1f}%'.format(100.*correct/len(test_data_6aam)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chou & Fasman\n",
    "\n",
    "Server http://www.biogem.org/tool/chou-fasman/index.php\n",
    "\n",
    "Produces result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from server http://www.biogem.org/tool/chou-fasman/index.php using\n",
    "# using fasta from\n",
    "chou_fasman = \"\"\"\n",
    "Struc 1   CCCTCCCCHHHHHHHHHHCCTTCHHCEEEEECCTCCTCCCHHHHHHHHHHHCCTCCCTHHHHHEEEEEEH 70 \n",
    "Struc 71  HHHHHHHHCCCCHHHHHHHEEHHHEEEHCCCCCCCCCCCCEHHHEEHHHHEHEHHHHHHHHHHHHHCTCH 140 \n",
    "Struc 141 HHHHHHHHTHHHHHHHHHHHHHHHHHCCCCCCCCTCCCTCEEEHHTHHHHHEEEEEEEEEEEEEEEHHHE 210 \n",
    "Struc 211 EEECCTCCCTCEHHHHEEHHHHTEEEEEHHHHHHHTHHCCCTCTHHHHHEEHHHETEHHHHHHCCTEHHE 280 \n",
    "Struc 281 EEEEEEHHHHHHHTHCCC 298 \n",
    "\"\"\"\n",
    "import re\n",
    "chou_fasman = re.sub(r'[^HTEC]+', '', chou_fasman)\n",
    "chou_fasman = chou_fasman.replace('T', 'S')\n",
    "chou_fasman = chou_fasman.replace('E', 'S')\n",
    "assert len(chou_fasman) == 298  # must be as long as 6aam\n",
    "assert set(chou_fasman) == set('HSC')  # must have only H, S or C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "298\n"
     ]
    }
   ],
   "source": [
    "# 6aam sequence from https://www.ebi.ac.uk/pdbe/entry/pdb/6aam/protein/1\n",
    "sequence_6aam = ('GPGDPTVFHKRYLKKIRDLGEGHFGKVSLYCYDPTNDGTGEMVAVKALKADAGP'\n",
    "                 'QHRSGWKQEIDILRTLYHEHIIKYKGCCEDAGAASLQLVMEYVPLGSLRDYLPR'\n",
    "                 'HSIGLAQLLLFAQQICEGMAYLHAQHYIHRNLAARNVLLDNDRLVKIGDFGLAK'\n",
    "                 'AVPEGHEYYRVREDGDSPVFWYAPECLKEYKFYYASDVWSFGVTLYELLTHCDS'\n",
    "                 'SQSPPTKFLELIGLAQGQMTVLRLTELLERGERLPRPDKCPAEVYHLMKNCWET'\n",
    "                 'EASFRPTFENLIPILKTVHEKYQGQAPS')\n",
    "print(len(sequence_6aam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DPTVF C C\n",
      "HKRYL C H\n",
      "KKIRD S H\n",
      "LGEGH C S\n",
      "FGKVS S C\n",
      "LYCYD S S\n",
      "PTNDG C C\n",
      "TGEMV S H\n",
      "AVKAL S H\n",
      "KADAG C H\n",
      "PQHRS H C\n",
      "GWKQE H H\n",
      "IDILR H S\n",
      "TLYHE C H\n",
      "HIIKY C H\n",
      "KGCCE S C\n",
      "DAGAA C H\n",
      "SLQLV S S\n",
      "MEYVP C S\n",
      "LGSLR S C\n",
      "DYLPR C C\n",
      "HSIGL C S\n",
      "AQLLL H S\n",
      "FAQQI H S\n",
      "CEGMA H H\n",
      "YLHAQ H H\n",
      "HYIHR S H\n",
      "NLAAR S H\n",
      "NVLLD S H\n",
      "NDRLV C H\n",
      "KIGDF C H\n",
      "GLAKA C H\n",
      "VPEGH C H\n",
      "EYYRV C C\n",
      "REDGD C C\n",
      "SPVFW C S\n",
      "YAPEC H S\n",
      "LKEYK H H\n",
      "FYYAS H S\n",
      "DVWSF H S\n",
      "GVTLY H S\n",
      "ELLTH H S\n",
      "CDSSQ C S\n",
      "SPPTK H C\n",
      "FLELI H H\n",
      "GLAQG C H\n",
      "QMTVL H S\n",
      "RLTEL H H\n",
      "LERGE C S\n",
      "RLPRP C C\n",
      "DKCPA C H\n",
      "EVYHL H S\n",
      "MKNCW H S\n",
      "ETEAS S H\n",
      "FRPTF C S\n",
      "ENLIP H S\n",
      "ILKTV H S\n",
      "HEKYQ H H\n",
      "GQAPS C C\n",
      "Success CF prediction for 6AAM test data is 27.1%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for test_5_mer, code in test_data_6aam:\n",
    "    where_in_6aam = sequence_6aam.find(test_5_mer)\n",
    "    cf_predict = chou_fasman[where_in_6aam+2]\n",
    "    print(test_5_mer, code, cf_predict)\n",
    "    if code == cf_predict:\n",
    "        correct += 1\n",
    "print('Success CF prediction for 6AAM test data is {:.1f}%'.format(100.*correct/len(test_data_6aam)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# jPred4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "jnetpred = '-,-,-,-,-,-,-,-,-,-,-,-,-,E,E,-,-,-,-,-,-,-,-,-,-,E,E,E,E,E,E,E,E,-,-,-,-,-,-,-,E,E,E,E,E,E,E,E,-,-,-,-,-,H,H,H,H,H,H,H,H,H,H,H,H,H,H,H,H,-,-,-,-,-,-,E,E,E,E,E,E,E,E,E,-,-,-,-,E,E,E,E,E,E,E,-,-,-,-,-,-,-,H,H,H,-,-,-,-,-,-,-,H,H,H,H,H,H,H,H,H,H,H,H,H,H,H,H,H,H,-,-,-,-,-,E,E,E,-,-,-,-,-,-,E,-,-,-,-,-,-,E,E,E,E,-,-,-,-,-,-,-,-,-,-,-,-,-,E,E,E,E,E,E,-,-,-,E,-,-,-,-,-,-,-,H,H,H,H,-,-,-,-,-,-,-,-,-,-,H,H,H,H,H,H,H,H,H,H,H,H,H,-,-,-,-,-,-,-,-,-,-,-,-,-,E,H,H,H,H,-,-,-,-,-,-,H,H,H,H,H,H,H,H,H,-,-,-,-,-,-,-,-,-,-,-,H,H,H,H,H,H,H,H,H,H,H,H,H,-,-,-,-,-,-,-,-,H,H,H,H,H,H,H,H,H,H,H,H,H,H,-,-,-,-,-,-,-,'\n",
    "translation = str.maketrans('HE-', 'HSC', ',')\n",
    "jnetpred = jnetpred.translate(translation)\n",
    "assert len(jnetpred) == 298  # must be as long as 6aam\n",
    "assert set(jnetpred) == set('HSC')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success JPred4 prediction for 6AAM test data is 78.0%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for test_5_mer, code in test_data_6aam:\n",
    "    where_in_6aam = sequence_6aam.find(test_5_mer)\n",
    "    jp_predict = jnetpred[where_in_6aam+2]\n",
    "    #print(test_5_mer, code, jp_predict)\n",
    "    if code == jp_predict:\n",
    "        correct += 1\n",
    "print('Success JPred4 prediction for 6AAM test data is {:.1f}%'.format(100.*correct/len(test_data_6aam)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maybe try 3-mer rather than 5mer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run this cell to define the training set\n",
    "small_training_set = [('ADTLL','S'),\n",
    "                      ('DTLLI','S'),\n",
    "                      ('TLLIL','S'),\n",
    "                      ('LLILG','S'),\n",
    "                      ('LILGD','S'),\n",
    "                      ('ILGDS','S'),\n",
    "                      ('LGDSL','C'),\n",
    "                      ('GDSLS','H'),\n",
    "                      ('DSLSA','H'),\n",
    "                      ('SLSAG','H'),\n",
    "                      ('LSAGY','H'),\n",
    "                      ('SAGYR','C'),\n",
    "                      ('AGYRM','C'),\n",
    "                      ('GYRMS','C'),\n",
    "                      ('YRMSA','C'),\n",
    "                      ('RMSAS','C')]\n",
    "small_training_set_3 = []\n",
    "for k, v in small_training_set:\n",
    "    small_training_set_3.append((k[1:4],v))\n",
    "print(small_training_set_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run this cell to create the training data\n",
    "small_training_vector = []\n",
    "for seq, ss in small_training_set_3:\n",
    " \n",
    "        inputVec = convertSeqToVector(seq, aaIndexDict)\n",
    "        outputVec = convertSeqToVector(ss, ssIndexDict)\n",
    "\n",
    "        small_training_vector.append( (inputVec, outputVec) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wMatrixIn, wMatrixOut = neuralNetTrain(small_training_vector, 3, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to define \n",
    "def predict_3_mer(seq):\n",
    "    \"\"\"\n",
    "    returns a prediction either 'H', 'S' or 'C' for the input sequence of 5 amino acids\n",
    "    \"\"\"\n",
    "    global wMatrixIn  # produced by previous training\n",
    "    global wMatrixOut\n",
    "    vector_seq = convertSeqToVector(seq, indexDict=aaIndexDict)\n",
    "    array_seq = array([vector_seq,])\n",
    "    sIn, sHid, sOut =    neuralNetPredict(array_seq, wMatrixIn, wMatrixOut)\n",
    "    index = sOut.argmax()\n",
    "    return ssCodes[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for test_3_mer, code in small_training_set_3:\n",
    "    predict = predict_3_mer(test_3_mer)\n",
    "    if predict == code:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "    print(test_3_mer, 'input ', code, ' predict ', predict)\n",
    "print('success \"prediction\" for training data is {:.1f}%'.format(100.*correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = small_training_set + data[:100]\n",
    "##Run this cell to create the training data\n",
    "training_set_3 = [(k[1:4], v) for (k, v) in training_set]\n",
    "training_vector = []\n",
    "for seq, ss in training_set_3:\n",
    "        inputVec = convertSeqToVector(seq, aaIndexDict)\n",
    "        outputVec = convertSeqToVector(ss, ssIndexDict)\n",
    "        training_vector.append( (inputVec, outputVec) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MatrixIn, wMatrixOut = neuralNetTrain(training_vector, 4, 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for test_3_mer, code in training_set_3:\n",
    "    #print(test_3_mer, code)\n",
    "    predict = predict_3_mer(test_3_mer)\n",
    "    if predict == code:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "print('success \"prediction\" for training data is {:.1f}%'.format(100.*correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for test_5_mer, code in test_data_6aam:\n",
    "    test_3_mer = test_5_mer[1:4]\n",
    "    predict = predict_5_mer(test_3_mer)\n",
    "    if predict == code:\n",
    "        correct += 1\n",
    "print('success prediction for 6aan test data is {:.1f}%'.format(100.*correct/len(test_data_6aam)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lookup closest\n",
    "\n",
    "idea lookup 'closest' value in large dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_match_5 = {}\n",
    "match_central_3 = {}\n",
    "match_central_1 = {}\n",
    "for k, v in small_training_set + data:\n",
    "    exact_match_5[k] = v\n",
    "    match_central_3[k[1:4]] = v\n",
    "    match_central_1[k[3]] = v\n",
    "    \n",
    "def predict_closest(seq5):\n",
    "    if seq5 in exact_match_5:\n",
    "        print('match 5')\n",
    "        return exact_match_5[seq5]\n",
    "    if seq5[1:4] in match_central_3:\n",
    "        print('match 3')\n",
    "        return match_central_3[seq5[1:4]]\n",
    "    if seq5[3] in match_central_1:\n",
    "        print('match 1')\n",
    "        return match_central_1[seq5[3]]\n",
    "    print('no match')\n",
    "    return 'C'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for test_5_mer, code in small_training_set:\n",
    "    predict = predict_closest(test_5_mer)\n",
    "    if predict == code:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "    print(test_5_mer, 'input ', code, ' predict ', predict)\n",
    "print('success \"prediction\" for training data is {:.1f}%'.format(100.*correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for test_5_mer, code in test_data_6aam:\n",
    "    predict = predict_closest(test_5_mer)\n",
    "    if predict == code:\n",
    "        correct += 1\n",
    "print('success prediction for 6AAM test data is {:.1f}%'.format(100.*correct/len(test_data_6aam)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = {}\n",
    "for k, v in small_training_set + data:\n",
    "    if k not in values:\n",
    "        values[k] = [v]\n",
    "    else:\n",
    "        values[k] = values[k] + [v]\n",
    "for k, v in values.items():\n",
    "    if len(v) > 4:\n",
    "        print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
