{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A feed-forward Neural Network for secondary structure prediction\n",
    "This notebook looks at a Neural Network based on code from \n",
    "[Stevens and Boucher (2014, Python Programming for Biology CUP)](https://www.amazon.co.uk/Python-Programming-Biology-Bioinformatics-Beyond/dp/0521720095). We will look at how a neural network is trained and how we can assess whether after training it has useful predictive power.\n",
    "\n",
    "In this case the neural network will be used  to predict the secondary structure of a protein from its sequence. \n",
    "It is useful to be able to predict the secondary structure of a protein for which there is only sequence available. In practice, secondary structure prediction has many uses, for instance in helping in the identification of functional domains ([Drozdetskiy et al., 2015](https://doi.org/10.1093/nar/gkv332)) and can be easily acheived using the JPRED4 server http://www.compbio.dundee.ac.uk/jpred4 (we will run this to compare results). \n",
    "\n",
    "It should be noted that the best approach to predicting a secondary structure is to use BLAST (or PSI-BLAST) to align the sequence to sequences of proteins with experimental 3D structures (those in PDB Protein Data Bank). If homologous sequences are found then the secondary structure can be expected to be same for areas with significant sequence identify. JPRED4 does such a search before switching to prediction from the sequence.\n",
    "\n",
    "The Stevens and Boucher approach is to use the sequence in the neighbourhood of a residue as a basis for a neural network prediction.  A predictive network is trained using data on known secondary structures of k-mers of 5 amino-acids taken from a set of PDB structures. Three secondary structure states are defined: H, C, and S. H and S are helix and strand respectively while C is for coil which is a range of structures not with regular H-bonding pattern. The DSSP program is used to assess the secondary structure of PDB structures. For more details see [6aam_secondary_structure_from_PDB.ipynb](6aam_secondary_structure_from_PDB.ipynb) where DSSP data is used to get test data for this exercise.\n",
    "\n",
    "\n",
    "The network here is a simple three layer feed-forward one.\n",
    "\n",
    "<img src=\"https://www.researchgate.net/publication/259519062/figure/fig10/AS:667910238384136@1536253618458/A-3-layer-feed-forward-neural-network_W640.jpg\" width=300>\n",
    "\n",
    "The number of nodes in the hidden layer can be defined by the programmer. But the number of input and output nodes is defined by the sizes of the input and output data vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to import numpy \n",
    "from numpy import tanh, ones, append, array, zeros, random, sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network function takes input data for the first layer of Network nodes, applies the the first weighted connections to pass the signal to the hidden layer of nodes, then applies the second weights to produce output. \n",
    "\n",
    "The output may not be optimized as the function also operates on the weighting during training. However after training the function gives predictions so takes its name from that. \n",
    "\n",
    "The weightsIn values define the strength of connection between the input nodes and the hidden nodes. Similarly weightsOut define the strengths of connection between the hidden and the output nodes. \n",
    "\n",
    "The weights are given as matrices with the rows indexing the nodes in a layer and the columns indexing the nodes in the other layer. \n",
    "\n",
    "The signalIn vector is the input features and an extra value of 1.0. This additional value is called the bias node which is used to tune the baseline response of the network. The baseline is the level without a meaningful signal. \n",
    "\n",
    "Setting the bias node value happens during training to adapt to the values in the input data. This means the input data don't need to pre-prepared with a mean of zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this cell to define the neuralNetPredict function\n",
    "def neuralNetPredict(inputVec, weightsIn, weightsOut):\n",
    "    \"\"\" uses the current weights in a neural network\n",
    "    to make a prediction from an input vector\n",
    "    all input and output are numpy data structures\"\"\" \n",
    "    signalIn = append(inputVec, 1.0) # input layer\n",
    "\n",
    "    prod = signalIn * weightsIn.T\n",
    "    sums = sum(prod, axis=1)\n",
    "    signalHid = tanh(sums)    # hidden    layer\n",
    "\n",
    "    prod = signalHid * weightsOut.T\n",
    "    sums = sum(prod, axis=1)\n",
    "    signalOut = tanh(sums)    # output    layer\n",
    "\n",
    "    return signalIn, signalHid, signalOut\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the numpy `.T` methodn gives the transpose of a matrix - that is the matrix with the columns turned into rows and the rows turned into columns. \n",
    "\n",
    "This is used so that the input signal gets applied to all the hidden nodes.\n",
    "\n",
    "The network applies the hyperbolic tangent function (tanh) to get the signal output from all the nodes in layer. Hyperbolic tan is a sigmoidal function that varies from -1 to 1, so it is much better than ordinary tan that runs off to infinity. \n",
    "\n",
    "<img src=\"https://mathworld.wolfram.com/images/interactive/TanhReal.gif\" width=300></img> \n",
    "\n",
    "The tanh function defines the output of that node given an input or set of inputs. As a nod to the output of neurones, which depends on an activation level across their cell membrane, the output function is called the *Activation* function.\n",
    "\n",
    "In operation only the signalOut from the output layer is of interest. But during training the response signals from the other layers are also needed to adjust the weighting scheme.\n",
    "\n",
    "### Training \n",
    "\n",
    "The weighting scheme (and gain) will be optimized by using a training dataset. \n",
    "\n",
    "The training data will be an input feature vector and a known output vector. The order of the data will be randomly shuffled to avoid bias. The number of hidden nodes needs to be specified and the number of optimization cycles. \n",
    "\n",
    "After each cycle the 'error' between the output signal of the network and the known training set output is used to adjust the network weights. The difference is combined with the *gradient* in the signal values - calculated from the tanh activation function (conveniently the gradient of tanh(sig) is 1-sig<sup>2</sup> or 1 - {sig x sig}). \n",
    "\n",
    "Early in training large difference can make the network go haywire so the speed of weight changing is damped down by a 'rate' and 'momentum' multipliers (usually the default values of 0.5 and 0.2 are good enough). \n",
    "\n",
    "More damping would mean that many more cycles would be needed for the weights to converge. \n",
    "\n",
    "The training will work back from the value of the error to adjust the weighting scheme of the network. This is called *back propagation*. \n",
    "\n",
    "The use of the gradient is crucial as it means initial adjustments will be large but then finer adjustments will be made as the optimum is approached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this cell to define the neuralNetTrain function\n",
    "def neuralNetTrain(trainData, numHid, steps=100, rate=0.5, momentum=0.2, silent=False):\n",
    "    \"\"\" uses training data to set the weights in a simple\n",
    "    neural network, number of hidden nodes is specified\"\"\"\n",
    "    numInp = len(trainData[0][0])\n",
    "    numOut = len(trainData[0][1])\n",
    "    numInp += 1\n",
    "    minError = None\n",
    "\n",
    "    sigInp = ones(numInp)\n",
    "    sigHid = ones(numHid)\n",
    "    sigOut = ones(numOut)\n",
    "    \n",
    "    wInp = random.random((numInp, numHid))-0.5\n",
    "    wOut = random.random((numHid, numOut))-0.5\n",
    "    \n",
    "    bestWeightMatrices = (wInp, wOut)\n",
    "\n",
    "    cInp = zeros((numInp, numHid))\n",
    "    cOut = zeros((numHid, numOut))\n",
    "\n",
    "    for x, (inputs, knownOut) in enumerate(trainData):\n",
    "        trainData[x] = (array(inputs), array(knownOut))\n",
    " \n",
    "    for step in range(steps):  \n",
    "        random.shuffle(trainData) # Important to avoid bias\n",
    "        error = 0.0\n",
    " \n",
    "        for inputs, knownOut in trainData:\n",
    "            sigIn, sigHid, sigOut = neuralNetPredict(inputs, wInp, wOut)\n",
    "\n",
    "            diff = knownOut - sigOut\n",
    "            error += sum(diff * diff)\n",
    "\n",
    "            gradient = ones(numOut) - (sigOut*sigOut)\n",
    "            outAdjust = gradient * diff \n",
    "\n",
    "            diff = sum(outAdjust * wOut, axis=1)\n",
    "            gradient = ones(numHid) - (sigHid*sigHid)\n",
    "            hidAdjust = gradient * diff \n",
    "\n",
    "            # update output \n",
    "            change = outAdjust * sigHid.reshape(numHid, 1)\n",
    "            wOut += (rate * change) + (momentum * cOut)\n",
    "            cOut = change\n",
    " \n",
    "            # update input \n",
    "            change = hidAdjust * sigIn.reshape(numInp, 1)\n",
    "            wInp += (rate * change) + (momentum * cInp)\n",
    "            cInp = change\n",
    " \n",
    "        if (minError is None) or (error < minError):\n",
    "            minError = error\n",
    "            bestWeightMatrices = (wInp.copy(), wOut.copy())\n",
    "            if not silent:\n",
    "                print(\"Step: %d Error: %f\" % (step, error))\n",
    "    \n",
    "    return bestWeightMatrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the functions\n",
    "Simple data to test a network can be binary input vectors with the desired output being an 'exclusive OR' (EOR) response https://en.wikipedia.org/wiki/Exclusive_or. This responds `True` if any input is `True` but False is both inputs are `True` or `False. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run this cell to define the test data\n",
    "testEORdata = [[[0,0], [0]],\n",
    "               [[0,1], [1]], \n",
    "               [[1,0], [1]], \n",
    "               [[1,1], [0]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network test uses two hidden nodes - in real use several values would be tried to find the best performance.\n",
    "Run the cell below to see if the training converges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Error: 2.155768\n",
      "Step: 1 Error: 1.391256\n",
      "Step: 2 Error: 1.174905\n",
      "Step: 5 Error: 1.077870\n",
      "Step: 8 Error: 1.036137\n",
      "Step: 13 Error: 0.974505\n",
      "Step: 15 Error: 0.968126\n",
      "Step: 17 Error: 0.963763\n",
      "Step: 18 Error: 0.943993\n",
      "Step: 21 Error: 0.918696\n",
      "Step: 23 Error: 0.910035\n",
      "Step: 25 Error: 0.904738\n",
      "Step: 27 Error: 0.904109\n",
      "Step: 28 Error: 0.886103\n",
      "Step: 30 Error: 0.880164\n",
      "Step: 34 Error: 0.875396\n",
      "Step: 39 Error: 0.870171\n",
      "Step: 40 Error: 0.869399\n",
      "Step: 49 Error: 0.866733\n",
      "Step: 50 Error: 0.853426\n",
      "Step: 52 Error: 0.849690\n",
      "Step: 56 Error: 0.845411\n",
      "Step: 58 Error: 0.832798\n",
      "Step: 73 Error: 0.821713\n",
      "Step: 102 Error: 0.818708\n",
      "Step: 117 Error: 0.810672\n",
      "Step: 121 Error: 0.808864\n",
      "Step: 134 Error: 0.802754\n",
      "Step: 193 Error: 0.790337\n",
      "Step: 198 Error: 0.777828\n",
      "Step: 252 Error: 0.770825\n",
      "Step: 304 Error: 0.758034\n",
      "Step: 319 Error: 0.739479\n",
      "Step: 321 Error: 0.721897\n",
      "Step: 323 Error: 0.614169\n",
      "Step: 324 Error: 0.603006\n",
      "Step: 326 Error: 0.470133\n",
      "Step: 328 Error: 0.422640\n",
      "Step: 329 Error: 0.287381\n",
      "Step: 330 Error: 0.260929\n",
      "Step: 333 Error: 0.236760\n",
      "Step: 334 Error: 0.190738\n",
      "Step: 336 Error: 0.135877\n",
      "Step: 338 Error: 0.095719\n",
      "Step: 340 Error: 0.083300\n",
      "Step: 354 Error: 0.069318\n",
      "Step: 358 Error: 0.061420\n",
      "Step: 360 Error: 0.036486\n",
      "Step: 371 Error: 0.034097\n",
      "Step: 382 Error: 0.031194\n",
      "Step: 386 Error: 0.020696\n",
      "Step: 394 Error: 0.020589\n",
      "Step: 406 Error: 0.018656\n",
      "Step: 410 Error: 0.018320\n",
      "Step: 412 Error: 0.018007\n",
      "Step: 413 Error: 0.014903\n",
      "Step: 418 Error: 0.014737\n",
      "Step: 422 Error: 0.013042\n",
      "Step: 425 Error: 0.011050\n",
      "Step: 430 Error: 0.010705\n",
      "Step: 443 Error: 0.009960\n",
      "Step: 448 Error: 0.009866\n",
      "Step: 453 Error: 0.008263\n",
      "Step: 465 Error: 0.007193\n",
      "Step: 477 Error: 0.006980\n",
      "Step: 481 Error: 0.005914\n",
      "Step: 496 Error: 0.005620\n",
      "Step: 498 Error: 0.004862\n",
      "Step: 512 Error: 0.004658\n",
      "Step: 520 Error: 0.004351\n",
      "Step: 532 Error: 0.004114\n",
      "Step: 535 Error: 0.003709\n",
      "Step: 540 Error: 0.003619\n",
      "Step: 559 Error: 0.003330\n",
      "Step: 574 Error: 0.003181\n",
      "Step: 576 Error: 0.003034\n",
      "Step: 588 Error: 0.002998\n",
      "Step: 589 Error: 0.002926\n",
      "Step: 591 Error: 0.002810\n",
      "Step: 599 Error: 0.002752\n",
      "Step: 605 Error: 0.002708\n",
      "Step: 609 Error: 0.002654\n",
      "Step: 620 Error: 0.002578\n",
      "Step: 621 Error: 0.002539\n",
      "Step: 625 Error: 0.002419\n",
      "Step: 630 Error: 0.002229\n",
      "Step: 648 Error: 0.002224\n",
      "Step: 650 Error: 0.002048\n",
      "Step: 665 Error: 0.002041\n",
      "Step: 668 Error: 0.001989\n",
      "Step: 681 Error: 0.001961\n",
      "Step: 685 Error: 0.001836\n",
      "Step: 692 Error: 0.001830\n",
      "Step: 696 Error: 0.001773\n",
      "Step: 701 Error: 0.001771\n",
      "Step: 710 Error: 0.001758\n",
      "Step: 716 Error: 0.001653\n",
      "Step: 736 Error: 0.001618\n",
      "Step: 742 Error: 0.001535\n",
      "Step: 746 Error: 0.001488\n",
      "Step: 749 Error: 0.001476\n",
      "Step: 753 Error: 0.001452\n",
      "Step: 771 Error: 0.001421\n",
      "Step: 780 Error: 0.001374\n",
      "Step: 788 Error: 0.001366\n",
      "Step: 796 Error: 0.001349\n",
      "Step: 798 Error: 0.001344\n",
      "Step: 802 Error: 0.001303\n",
      "Step: 804 Error: 0.001277\n",
      "Step: 808 Error: 0.001267\n",
      "Step: 810 Error: 0.001246\n",
      "Step: 822 Error: 0.001238\n",
      "Step: 831 Error: 0.001237\n",
      "Step: 832 Error: 0.001224\n",
      "Step: 835 Error: 0.001159\n",
      "Step: 844 Error: 0.001138\n",
      "Step: 855 Error: 0.001124\n",
      "Step: 868 Error: 0.001120\n",
      "Step: 876 Error: 0.001059\n",
      "Step: 892 Error: 0.001009\n",
      "Step: 922 Error: 0.000984\n",
      "Step: 931 Error: 0.000961\n",
      "Step: 942 Error: 0.000941\n",
      "Step: 946 Error: 0.000930\n",
      "Step: 957 Error: 0.000914\n",
      "Step: 967 Error: 0.000872\n",
      "Step: 979 Error: 0.000866\n",
      "Step: 988 Error: 0.000862\n",
      "Step: 994 Error: 0.000849\n",
      "Step: 999 Error: 0.000825\n"
     ]
    }
   ],
   "source": [
    "## Run this cell to train the network\n",
    "wMatrixIn, wMatrixOut = neuralNetTrain(testEORdata, 2, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should find good convergence has occurred and there is no oscillation. Perhaps you can see that the initial steps are giving large changes in the error while later on there are smaller and smaller changes. This is owing to the effect of the gradient calculation. The changes in the actual weights are not printed, but will follow the same trend.\n",
    "\n",
    "The output weight matrices will now be stored in the kernel as `wMatrixIn` and `wMatrixOut` as can then be run on test data for evaluation. \n",
    "\n",
    "Run the following cell to compare the output of the network with the actual values of the training set outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input [1 1]  should have output  [0] actual output 0.01\n",
      "input [1 0]  should have output  [1] actual output 0.98\n",
      "input [0 1]  should have output  [1] actual output 0.98\n",
      "input [0 0]  should have output  [0] actual output -0.00\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to test the neural network produces the expected output for the training set\n",
    "for inputs, knownOut in testEORdata:\n",
    "    sIn, sHid, sOut =    neuralNetPredict(array(inputs), wMatrixIn, wMatrixOut)\n",
    "    print('input', inputs, ' should have output ', knownOut, 'actual output {:.2f}'.format(sOut[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the neural net produces actual output that matches the training set data.\n",
    "\n",
    "This is only a limited test because to properly test data should be used inputs with known output but which were not included in the training set. In this case it is not possible to give any new data for the EOR function as the training set covered all possible responses!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply the neural net to secondary structure prediction\n",
    "\n",
    "Here is a very limited set of training data that will be used to start with. It shows the raw format which is a 5-mer string and the secondary structure that was observed for the central residue of this in at least one PDB structure. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run this cell to define the training set\n",
    "small_training_set = [('ADTLL','S'),\n",
    "                      ('DTLLI','S'),\n",
    "                      ('TLLIL','S'),\n",
    "                      ('LLILG','S'),\n",
    "                      ('LILGD','S'),\n",
    "                      ('ILGDS','S'),\n",
    "                      ('LGDSL','C'),\n",
    "                      ('GDSLS','H'),\n",
    "                      ('DSLSA','H'),\n",
    "                      ('SLSAG','H'),\n",
    "                      ('LSAGY','H'),\n",
    "                      ('SAGYR','C'),\n",
    "                      ('AGYRM','C'),\n",
    "                      ('GYRMS','C'),\n",
    "                      ('YRMSA','C'),\n",
    "                      ('RMSAS','C')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple feature vectors for sequence data\n",
    "\n",
    "The neural net needs its data to provided in a numeric form as a vector like that used in the EOR example.\n",
    "A simple numbering scheme is used to convert to the sequence alphabet to a numeric form as an input vector. For proteins that is number from 0 to 19 from the list of one-letter codes.\n",
    "\n",
    "k-mers with k=5 are used. It should be noted that the secondary structure is that associated with the middle residue of the 5 mer. The possible outputs are also coded as integers for the more restricted alphabet of H, C, and S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run this cell to define the dictionaries for the vectors\n",
    "aminoAcids = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "aaIndexDict = {}\n",
    "for i, aa in enumerate(aminoAcids):\n",
    "        aaIndexDict[aa] = i\n",
    "\n",
    "ssIndexDict = {}\n",
    "ssCodes = 'HCS'\n",
    "for i, code in enumerate(ssCodes):\n",
    "        ssIndexDict[code] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The actual structure is a simplified output from the DSSP program mentioned in the introduction. DSSP acutally distinguishes more structures that the three here - for example there are other kinds of helix. But these complications are not dealt with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data has to be converted to the numerical code. Here is a function to to that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run this cell to define the function\n",
    "def convertSeqToVector(seq, indexDict):\n",
    "    \"\"\"converts a one-letter sequence to numerical\n",
    "    coding for neural network calculations\"\"\"   \n",
    "    numLetters = len(indexDict)\n",
    "    vector = [0.0] * len(seq) * numLetters\n",
    "    for pos, letter in enumerate(seq):\n",
    "        index = pos * numLetters + indexDict[letter]    \n",
    "        vector[index] = 1.0\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data is prepared with the `convertSeqToVector`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run this cell to create the training data\n",
    "small_training_vector = []\n",
    "for seq, ss in small_training_set:\n",
    "        inputVec = convertSeqToVector(seq, aaIndexDict)\n",
    "        outputVec = convertSeqToVector(ss, ssIndexDict)\n",
    "        small_training_vector.append( (inputVec, outputVec) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then the network is trained. Here there are 3 hidden nodes specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Error: 18.923801\n",
      "Step: 1 Error: 11.275540\n",
      "Step: 3 Error: 8.667626\n",
      "Step: 4 Error: 5.158456\n",
      "Step: 5 Error: 3.388506\n",
      "Step: 6 Error: 3.183800\n",
      "Step: 7 Error: 2.260478\n",
      "Step: 8 Error: 1.728604\n",
      "Step: 9 Error: 0.837363\n",
      "Step: 10 Error: 0.441825\n",
      "Step: 11 Error: 0.418651\n",
      "Step: 13 Error: 0.372228\n",
      "Step: 14 Error: 0.279643\n",
      "Step: 21 Error: 0.173488\n",
      "Step: 24 Error: 0.106576\n",
      "Step: 37 Error: 0.060687\n",
      "Step: 41 Error: 0.060271\n",
      "Step: 76 Error: 0.051542\n",
      "Step: 77 Error: 0.050923\n",
      "Step: 84 Error: 0.049465\n",
      "Step: 86 Error: 0.033449\n",
      "Step: 87 Error: 0.021676\n",
      "Step: 91 Error: 0.017242\n",
      "Step: 132 Error: 0.011145\n",
      "Step: 152 Error: 0.010751\n",
      "Step: 153 Error: 0.008127\n",
      "Step: 156 Error: 0.007079\n",
      "Step: 178 Error: 0.005158\n",
      "Step: 308 Error: 0.004664\n",
      "Step: 309 Error: 0.004587\n",
      "Step: 312 Error: 0.004350\n",
      "Step: 317 Error: 0.004331\n",
      "Step: 320 Error: 0.004158\n",
      "Step: 321 Error: 0.002978\n",
      "Step: 372 Error: 0.002732\n",
      "Step: 378 Error: 0.002606\n",
      "Step: 382 Error: 0.002406\n",
      "Step: 383 Error: 0.002281\n",
      "Step: 389 Error: 0.001885\n",
      "Step: 428 Error: 0.001830\n",
      "Step: 433 Error: 0.001522\n",
      "Step: 585 Error: 0.001240\n",
      "Step: 599 Error: 0.001234\n",
      "Step: 612 Error: 0.001029\n",
      "Step: 628 Error: 0.000979\n",
      "Step: 630 Error: 0.000899\n",
      "Step: 829 Error: 0.000766\n",
      "Step: 832 Error: 0.000625\n",
      "Step: 838 Error: 0.000552\n",
      "Step: 845 Error: 0.000526\n",
      "Step: 851 Error: 0.000506\n",
      "Step: 859 Error: 0.000474\n",
      "Step: 874 Error: 0.000457\n",
      "Step: 889 Error: 0.000450\n"
     ]
    }
   ],
   "source": [
    "small_training_matrix_in, small_training_matrix_out = neuralNetTrain(small_training_vector, 3, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see for this small training set the neural net converges quickly. Lets test how the neural net performs.\n",
    "\n",
    "The following function can be used to make a predict for any given 5-mer sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to define predict_seq function.\n",
    "def predict_seq(seq, w_matrix_in, w_matrix_out):\n",
    "    \"\"\"\n",
    "    returns a prediction either 'H', 'S' or 'C' for the input 5-mer sequence seq using\n",
    "    the neuralNetPredict funciton with matrix weights w_matrix_in, w_matrix_out\n",
    "    \"\"\"\n",
    "    vector_seq = convertSeqToVector(seq, indexDict=aaIndexDict)\n",
    "    array_seq = array([vector_seq,])\n",
    "    sIn, sHid, sOut =    neuralNetPredict(array_seq, w_matrix_in, w_matrix_out)\n",
    "    index = sOut.argmax()\n",
    "    return ssCodes[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How well does the neural network do for the `small_training_set` data that was used to train it?\n",
    "\n",
    "The first sequence in this was `'ADTLL` that was a strand `S`. Let us 'predict' what seconary the neural network expects for this sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neural net predicts ADTLL will be S\n"
     ]
    }
   ],
   "source": [
    "test_seq = 'ADTLL'\n",
    "predict = predict_seq(test_seq, small_training_matrix_in, small_training_matrix_out)\n",
    "print('neural net predicts', test_seq, 'will be', predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see that `ADTLL` is predicted to be `S` just like in the input set. But what of the other cases in `small_training_set`? Write python to find out the percentage success rate for the neural network for `small_training_set`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your turn write Python to find out percentage the success rate for small_training_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage success for small_training_set 100.0 %\n"
     ]
    }
   ],
   "source": [
    "# ANSWER REMOVE BEFORE PUBLICATION\n",
    "success = 0\n",
    "total = 0\n",
    "for test_seq, known_ss in small_training_set:\n",
    "    predict = predict_seq(test_seq, small_training_matrix_in, small_training_matrix_out)\n",
    "    if predict == known_ss:\n",
    "        success += 1\n",
    "    total += 1\n",
    "print('percentage success for small_training_set {:.1f} %'.format(100.*success/total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have found the success rate for `predicting` the data used for training is 100% or close to it. But the real test of a neural net is that it should be able to make predictions for input data it has not seen before.\n",
    "\n",
    "To test the predictive power of the neural net trained on the `small_training_set` use the `test_data_from_6aam` (see  [6aam_secondary_structure_from_PDB.ipynb](6aam_secondary_structure_from_PDB.ipynb) for its derivation): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data from PDB entry 6AAM\n",
    "test_data_from_6aam = [('FHKRY', 'C'), ('DLGEG', 'C'), ('SLYCY', 'S'), ('GTGEM', 'C'),\n",
    "                       ('LKADA', 'C'), ('SGWKQ', 'H'), ('RTLYH', 'C'), ('YKGCC', 'S'),\n",
    "                       ('ASLQL', 'S'), ('PLGSL', 'C'), ('RHSIG', 'C'), ('LFAQQ', 'H'),\n",
    "                       ('AYLHA', 'H'), ('RNLAA', 'C'), ('DNDRL', 'C'), ('FGLAK', 'C'),\n",
    "                       ('HEYYR', 'C'), ('DSPVF', 'C'), ('CLKEY', 'H'), ('SDVWS', 'H'),\n",
    "                       ('YELLT', 'H'), ('QSPPT', 'H'), ('IGLAQ', 'S'), ('LRLTE', 'H'),\n",
    "                       ('ERLPR', 'C'), ('AEVYH', 'H'), ('WETEA', 'S'), ('FENLI', 'H'), \n",
    "                       ('VHEKY', 'H')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your turn write Python to find out percentage the success rate of the\n",
    "# neural_net trained on small_training_set when test on test_data_from_6aam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage success for test_data_from_6aam  27.6 %\n"
     ]
    }
   ],
   "source": [
    "# ANSWER REMOVE BEFORE PUBLICATION\n",
    "success = 0\n",
    "total = 0\n",
    "for test_seq, known_ss in test_data_from_6aam:\n",
    "    predict = predict_seq(test_seq, small_training_matrix_in, small_training_matrix_out)\n",
    "    if predict == known_ss:\n",
    "        success += 1\n",
    "    total += 1\n",
    "print('percentage success for test_data_from_6aam  {:.1f} %'.format(100.*success/total))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should find the success rate is low - in fact below 30%. Considering there are only 3 states possible a reasonable comparison is to 'predict' that every residue is coil 'C'. This produces a success rate of 45%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage success predicting all coil 44.8%\n"
     ]
    }
   ],
   "source": [
    "n_coil = sum([ss == 'C' for _, ss in  test_data_from_6aam])\n",
    "percent_all_coil = 100.*n_coil/len(test_data_from_6aam)\n",
    "print('percentage success predicting all coil {:.1f}%'.format(percent_all_coil))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the success rate for neural net trained on a set of 16 examples is below the all-coil benchmark. It has no real predictive power. This is not really surprising as for example the amino acid proline (one letter code `P`) does not appear anywhere in the `small_training_set` but appears a number of times in `test_data_from_6aam`\n",
    "\n",
    "But do not worry as Stevens and Boucher provide secondary structure data for 26242 5-mers and confidentally assert 'Using this as as an input would give a vastly superior result'. So let's load this dataset and checkout whether more training data helps in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading a larger training set\n",
    "\n",
    "The  much dataset from Stevens and Boucher is provided as a comma separated file called:\n",
    "[PDB_protein_secondary_5mers.csv](PDB_protein_secondary_5mers.csv) that is supplied alongside this Notebook.\n",
    "\n",
    "You should now read the data from [PDB_protein_secondary_5mers.csv](PDB_protein_secondary_5mers.csv) into a list `large_data_set`.\n",
    "\n",
    "The Python `csv` module `reader` method should be used for this. See https://pymotw.com/3/csv/ or Google for another tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your turn read PDB_protein_secondary_5mers.csv into the list large_data_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have read 26242 lines from csv file\n",
      "first record ['MGKMY', 'S'] last record ['LAMKL', 'H']\n"
     ]
    }
   ],
   "source": [
    "# ANSWER REMOVE BEFORE PUBLICATION\n",
    "from csv import reader\n",
    "with open('PDB_protein_secondary_5mers.csv') as csv_file:\n",
    "    csv_reader = reader(csv_file, delimiter=',')\n",
    "    large_data_set = list(csv_reader)\n",
    "print('have read', len(large_data_set), 'lines from csv file')\n",
    "print('first record', large_data_set[0], 'last record', large_data_set[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to check that list large_data_set has the data from PDB_protein_secondary_5mers.csv\n",
    "assert type(large_data_set) is list\n",
    "assert len(large_data_set) == 26242\n",
    "assert large_data_set[-1] == ['LAMKL', 'H']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have read in the `large_data_set` you go ahead and train the neural network on all this data. In practice you would find this does not work well, running slowly with big convergence issues. Instead lets try using a subset of the data to train the neural network. Lets start with the first 100 records:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a selection of data from the `large_data_set` as larger_training_set\n",
    "larger_training_set = large_data_set[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to create the larger_training_vector from larger_training_set\n",
    "larger_training_vector = []\n",
    "for seq, ss in larger_training_set:\n",
    "        inputVec = convertSeqToVector(seq, aaIndexDict)\n",
    "        outputVec = convertSeqToVector(ss, ssIndexDict)\n",
    "        larger_training_vector.append( (inputVec, outputVec) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Error: 104.244087\n",
      "Step: 3 Error: 89.086720\n",
      "Step: 5 Error: 82.723386\n",
      "Step: 6 Error: 77.121099\n",
      "Step: 7 Error: 68.926343\n",
      "Step: 8 Error: 66.370467\n",
      "Step: 9 Error: 65.753416\n",
      "Step: 10 Error: 45.793486\n",
      "Step: 11 Error: 40.858301\n",
      "Step: 13 Error: 35.876907\n",
      "Step: 15 Error: 29.079697\n",
      "Step: 18 Error: 27.140895\n",
      "Step: 19 Error: 23.189806\n",
      "Step: 20 Error: 22.541265\n",
      "Step: 21 Error: 22.387350\n",
      "Step: 23 Error: 19.873106\n",
      "Step: 29 Error: 15.813635\n",
      "Step: 37 Error: 13.787603\n",
      "Step: 44 Error: 13.707655\n",
      "Step: 90 Error: 11.271791\n",
      "Step: 114 Error: 10.930608\n",
      "Step: 118 Error: 10.887742\n",
      "Step: 128 Error: 10.501612\n",
      "Step: 236 Error: 10.174853\n",
      "Step: 256 Error: 9.752790\n",
      "Step: 323 Error: 9.740842\n",
      "Step: 407 Error: 6.406704\n",
      "Step: 434 Error: 5.355145\n",
      "Step: 520 Error: 4.602612\n",
      "Step: 583 Error: 3.628552\n"
     ]
    }
   ],
   "source": [
    "# now train the neural network on the larger training vector.\n",
    "larger_training_matrix_in, larger_training_matrix_out = neuralNetTrain(larger_training_vector, 3, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to define a function to find percentage_sucess\n",
    "def percentage_success(test_set, matrix_in, matrix_out):\n",
    "    \"\"\"\n",
    "    returns the percentage_success for the test_set using \n",
    "    neural network defined by matrix_in and matrix_out\n",
    "    \"\"\"\n",
    "    predictions = [predict_seq(ts, matrix_in, matrix_out) for ts, _ in test_set]\n",
    "    known = [k for _, k in test_set]\n",
    "    percentage = 100.*sum([p == k for p, k in zip(predictions, known)])/len(test_set)\n",
    "    return percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful prediction rates:\n",
      "   training_set data:       98.0%\n",
      "   test data from PDB 6AAM: 41.4%\n"
     ]
    }
   ],
   "source": [
    "# run this cell to assess percentage success rate on training and test data\n",
    "train_percent = percentage_success(larger_training_set, larger_training_matrix_in, larger_training_matrix_out)\n",
    "independent_percent = percentage_success(test_data_from_6aam, larger_training_matrix_in, larger_training_matrix_out)\n",
    "print('Successful prediction rates:')\n",
    "print('   training_set data:       {:.1f}%'.format(train_percent))\n",
    "print('   test data from PDB 6AAM: {:.1f}%'.format(independent_percent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results you will get above will vary. Sometimes the neural network converges with small error whereas sometimes it fails to converge. Lets try running the process 5 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successful prediction rates:\n",
      "   training_set data:       100.0%\n",
      "   test data from PDB 6AAM: 51.7%\n",
      "Successful prediction rates:\n",
      "   training_set data:       98.0%\n",
      "   test data from PDB 6AAM: 41.4%\n",
      "Successful prediction rates:\n",
      "   training_set data:       98.0%\n",
      "   test data from PDB 6AAM: 37.9%\n",
      "Successful prediction rates:\n",
      "   training_set data:       100.0%\n",
      "   test data from PDB 6AAM: 44.8%\n",
      "Successful prediction rates:\n",
      "   training_set data:       84.0%\n",
      "   test data from PDB 6AAM: 41.4%\n"
     ]
    }
   ],
   "source": [
    "# run this cell run the training process a number of times - may be slow.\n",
    "for _ in range(5): # number of repeats\n",
    "    larger_training_matrix_in, larger_training_matrix_out = neuralNetTrain(larger_training_vector, 3, 1000, \n",
    "                                                                           silent=True)\n",
    "    train_percent = percentage_success(larger_training_set, larger_training_matrix_in, larger_training_matrix_out)\n",
    "    independent_percent = percentage_success(test_data_from_6aam, larger_training_matrix_in, larger_training_matrix_out)\n",
    "    print('Successful prediction rates:')\n",
    "    print('   training_set data:       {:.1f}%'.format(train_percent))\n",
    "    print('   test data from PDB 6AAM: {:.1f}%'.format(independent_percent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was the result I got:\n",
    "```\n",
    "Successful prediction rates:\n",
    "   training_set data:       100.0%\n",
    "   test data from PDB 6AAM: 51.7%\n",
    "Successful prediction rates:\n",
    "   training_set data:       98.0%\n",
    "   test data from PDB 6AAM: 41.4%\n",
    "Successful prediction rates:\n",
    "   training_set data:       98.0%\n",
    "   test data from PDB 6AAM: 37.9%\n",
    "Successful prediction rates:\n",
    "   training_set data:       100.0%\n",
    "   test data from PDB 6AAM: 44.8%\n",
    "Successful prediction rates:\n",
    "   training_set data:       84.0%\n",
    "   test data from PDB 6AAM: 41.4%\n",
    "```\n",
    "You will set that the results vary. Sometime the neural net converges and can 'predict' the input data sometimes not. Results for 6AAM data are between 38% to 51% success. Compare this to around 28% for the small training set with 16 examples and 42% for an 'all coil' prediction. So the conclusion is that this will a data set size 200 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is your turn. \n",
    "* Try training with datasets size 200, 400 and 800 \n",
    "* what is the best success rate for 6AAM you can obtain? \n",
    "* try altering the values of `rate` `momentum` to help the convergence\n",
    "Keep your results in the table below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of results (to be completed)\n",
    "\n",
    "\n",
    "| train data set size | %prediction on training data | %prediction 6aam data        |\n",
    "| ------------------- | -----------------------------| -----------------------------|\n",
    "|      100            |  100, 98, 98,  100, 84       | 51.7, 41.4, 37.9, 44.8, 41.4 |\n",
    "|      200            | | |\n",
    "|      400            | | |\n",
    "|      800            | | |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_data_size: 16  train: 100.0 6AAM: 31.0\n",
      "training_data_size: 16  train: 100.0 6AAM: 37.9\n",
      "training_data_size: 16  train: 100.0 6AAM: 37.9\n",
      "training_data_size: 16  train: 100.0 6AAM: 37.9\n",
      "training_data_size: 16  train: 100.0 6AAM: 31.0\n",
      "16 100.0,100.0,100.0,100.0,100.0 31.0,37.9,37.9,37.9,31.0\n",
      "training_data_size: 100  train: 99.0 6AAM: 37.9\n",
      "training_data_size: 100  train: 100.0 6AAM: 41.4\n",
      "training_data_size: 100  train: 99.0 6AAM: 34.5\n",
      "training_data_size: 100  train: 99.0 6AAM: 48.3\n",
      "training_data_size: 100  train: 99.0 6AAM: 41.4\n",
      "100 99.0,100.0,99.0,99.0,99.0 37.9,41.4,34.5,48.3,41.4\n",
      "training_data_size: 200  train: 95.0 6AAM: 48.3\n",
      "training_data_size: 200  train: 95.5 6AAM: 48.3\n",
      "training_data_size: 200  train: 94.0 6AAM: 41.4\n",
      "training_data_size: 200  train: 94.0 6AAM: 37.9\n",
      "training_data_size: 200  train: 94.5 6AAM: 55.2\n",
      "200 95.0,95.5,94.0,94.0,94.5 48.3,48.3,41.4,37.9,55.2\n",
      "training_data_size: 400  train: 88.8 6AAM: 41.4\n",
      "training_data_size: 400  train: 87.8 6AAM: 58.6\n",
      "training_data_size: 400  train: 85.0 6AAM: 48.3\n",
      "training_data_size: 400  train: 88.2 6AAM: 62.1\n",
      "training_data_size: 400  train: 66.0 6AAM: 55.2\n",
      "400 88.8,87.8,85.0,88.2,66.0 41.4,58.6,48.3,62.1,55.2\n",
      "training_data_size: 800  train: 80.1 6AAM: 41.4\n",
      "training_data_size: 800  train: 67.9 6AAM: 48.3\n",
      "training_data_size: 800  train: 62.2 6AAM: 58.6\n",
      "training_data_size: 800  train: 78.2 6AAM: 37.9\n",
      "training_data_size: 800  train: 77.6 6AAM: 37.9\n",
      "800 80.1,67.9,62.2,78.2,77.6 41.4,48.3,58.6,37.9,37.9\n",
      "training_data_size: 1600  train: 68.7 6AAM: 51.7\n",
      "training_data_size: 1600  train: 70.9 6AAM: 48.3\n",
      "training_data_size: 1600  train: 39.5 6AAM: 34.5\n",
      "training_data_size: 1600  train: 73.0 6AAM: 48.3\n",
      "training_data_size: 1600  train: 72.6 6AAM: 51.7\n",
      "1600 68.7,70.9,39.5,73.0,72.6 51.7,48.3,34.5,48.3,51.7\n"
     ]
    }
   ],
   "source": [
    "# ANSWER\n",
    "for train_data_size in 16, 100, 200, 400, 800, 1600:\n",
    "    larger_training_set = large_data_set[:train_data_size]\n",
    "    larger_training_vector = []\n",
    "    for seq, ss in larger_training_set:\n",
    "        inputVec = convertSeqToVector(seq, aaIndexDict)\n",
    "        outputVec = convertSeqToVector(ss, ssIndexDict)\n",
    "        larger_training_vector.append( (inputVec, outputVec) )\n",
    "    train_percents = []\n",
    "    independent_percents = []\n",
    "    for _ in range(5): # number of repeats\n",
    "        larger_training_matrix_in, larger_training_matrix_out = neuralNetTrain(larger_training_vector, 3, 1000, \n",
    "                                                                           silent=True,\n",
    "                                                                           rate=0.1)\n",
    "        train_percent = percentage_success(larger_training_set, larger_training_matrix_in, larger_training_matrix_out)\n",
    "        independent_percent = percentage_success(test_data_from_6aam, larger_training_matrix_in, larger_training_matrix_out)\n",
    "        print(f'training_data_size: {train_data_size}  train: {train_percent:.1f} 6AAM: {independent_percent:.1f}')\n",
    "        train_percents.append(f'{train_percent:.1f}')\n",
    "        independent_percents.append(f'{independent_percent:.1f}')\n",
    "    print(train_data_size, ','.join(train_percents), ','.join(independent_percents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWER - larger sets\n",
    "for train_data_size in 3200, 6400, 12800:\n",
    "    larger_training_set = large_data_set[:train_data_size]\n",
    "    larger_training_vector = []\n",
    "    for seq, ss in larger_training_set:\n",
    "        inputVec = convertSeqToVector(seq, aaIndexDict)\n",
    "        outputVec = convertSeqToVector(ss, ssIndexDict)\n",
    "        larger_training_vector.append( (inputVec, outputVec) )\n",
    "    train_percents = []\n",
    "    independent_percents = []\n",
    "    for _ in range(5): # number of repeats\n",
    "        larger_training_matrix_in, larger_training_matrix_out = neuralNetTrain(larger_training_vector, 3, 1000, \n",
    "                                                                           silent=True,\n",
    "                                                                           rate=0.1)\n",
    "        train_percent = percentage_success(larger_training_set, larger_training_matrix_in, larger_training_matrix_out)\n",
    "        independent_percent = percentage_success(test_data_from_6aam, larger_training_matrix_in, larger_training_matrix_out)\n",
    "        print(f'training_data_size: {train_data_size}  train: {train_percent:.1f} 6AAM: {independent_percent:.1f}')\n",
    "        train_percents.append(f'{train_percent:.1f}')\n",
    "        independent_percents.append(f'{independent_percent:.1f}')\n",
    "    print(train_data_size, ','.join(train_percents), ','.join(independent_percents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My table of results\n",
    "\n",
    "Run with `rate=0.1`\n",
    "\n",
    "\n",
    "| train data set size | %prediction on training data | %prediction 6aam data        |\n",
    "| ------------------- | -----------------------------| -----------------------------|\n",
    "| 16 |  100.0,100.0,100.0,100.0,100.0 | 31.0,37.9,37.9,37.9,31.0 |\n",
    "|      100            |  100, 98, 98,  100, 84       | 51.7, 41.4, 37.9, 44.8, 41.4 |\n",
    "|      200            | | |\n",
    "|      400            | | |\n",
    "|      800            | | |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions \n",
    "\n",
    "Ideas:\n",
    "* look at data - is there a single secondary structure for each 5-mer? \n",
    "* Helix and sheet formation is cooperative - not explicitly covered in this module\n",
    "* Chou and Fasman.\n",
    "* State of the art compare to JPRED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chou & Fasman\n",
    "\n",
    "Server http://www.biogem.org/tool/chou-fasman/index.php\n",
    "\n",
    "Produces result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from server http://www.biogem.org/tool/chou-fasman/index.php using\n",
    "# using fasta from\n",
    "chou_fasman = \"\"\"\n",
    "Struc 1   CCCTCCCCHHHHHHHHHHCCTTCHHCEEEEECCTCCTCCCHHHHHHHHHHHCCTCCCTHHHHHEEEEEEH 70 \n",
    "Struc 71  HHHHHHHHCCCCHHHHHHHEEHHHEEEHCCCCCCCCCCCCEHHHEEHHHHEHEHHHHHHHHHHHHHCTCH 140 \n",
    "Struc 141 HHHHHHHHTHHHHHHHHHHHHHHHHHCCCCCCCCTCCCTCEEEHHTHHHHHEEEEEEEEEEEEEEEHHHE 210 \n",
    "Struc 211 EEECCTCCCTCEHHHHEEHHHHTEEEEEHHHHHHHTHHCCCTCTHHHHHEEHHHETEHHHHHHCCTEHHE 280 \n",
    "Struc 281 EEEEEEHHHHHHHTHCCC 298 \n",
    "\"\"\"\n",
    "import re\n",
    "chou_fasman = re.sub(r'[^HTEC]+', '', chou_fasman)\n",
    "chou_fasman = chou_fasman.replace('T', 'S')\n",
    "chou_fasman = chou_fasman.replace('E', 'S')\n",
    "assert len(chou_fasman) == 298  # must be as long as 6aam\n",
    "assert set(chou_fasman) == set('HSC')  # must have only H, S or C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6aam sequence from https://www.ebi.ac.uk/pdbe/entry/pdb/6aam/protein/1\n",
    "sequence_6aam = ('GPGDPTVFHKRYLKKIRDLGEGHFGKVSLYCYDPTNDGTGEMVAVKALKADAGP'\n",
    "                 'QHRSGWKQEIDILRTLYHEHIIKYKGCCEDAGAASLQLVMEYVPLGSLRDYLPR'\n",
    "                 'HSIGLAQLLLFAQQICEGMAYLHAQHYIHRNLAARNVLLDNDRLVKIGDFGLAK'\n",
    "                 'AVPEGHEYYRVREDGDSPVFWYAPECLKEYKFYYASDVWSFGVTLYELLTHCDS'\n",
    "                 'SQSPPTKFLELIGLAQGQMTVLRLTELLERGERLPRPDKCPAEVYHLMKNCWET'\n",
    "                 'EASFRPTFENLIPILKTVHEKYQGQAPS')\n",
    "print(len(sequence_6aam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for test_5_mer, code in test_data_6aam:\n",
    "    where_in_6aam = sequence_6aam.find(test_5_mer)\n",
    "    cf_predict = chou_fasman[where_in_6aam+2]\n",
    "    print(test_5_mer, code, cf_predict)\n",
    "    if code == cf_predict:\n",
    "        correct += 1\n",
    "print('Success CF prediction for 6AAM test data is {:.1f}%'.format(100.*correct/len(test_data_6aam)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# jPred4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jnetpred = '-,-,-,-,-,-,-,-,-,-,-,-,-,E,E,-,-,-,-,-,-,-,-,-,-,E,E,E,E,E,E,E,E,-,-,-,-,-,-,-,E,E,E,E,E,E,E,E,-,-,-,-,-,H,H,H,H,H,H,H,H,H,H,H,H,H,H,H,H,-,-,-,-,-,-,E,E,E,E,E,E,E,E,E,-,-,-,-,E,E,E,E,E,E,E,-,-,-,-,-,-,-,H,H,H,-,-,-,-,-,-,-,H,H,H,H,H,H,H,H,H,H,H,H,H,H,H,H,H,H,-,-,-,-,-,E,E,E,-,-,-,-,-,-,E,-,-,-,-,-,-,E,E,E,E,-,-,-,-,-,-,-,-,-,-,-,-,-,E,E,E,E,E,E,-,-,-,E,-,-,-,-,-,-,-,H,H,H,H,-,-,-,-,-,-,-,-,-,-,H,H,H,H,H,H,H,H,H,H,H,H,H,-,-,-,-,-,-,-,-,-,-,-,-,-,E,H,H,H,H,-,-,-,-,-,-,H,H,H,H,H,H,H,H,H,-,-,-,-,-,-,-,-,-,-,-,H,H,H,H,H,H,H,H,H,H,H,H,H,-,-,-,-,-,-,-,-,H,H,H,H,H,H,H,H,H,H,H,H,H,H,-,-,-,-,-,-,-,'\n",
    "translation = str.maketrans('HE-', 'HSC', ',')\n",
    "jnetpred = jnetpred.translate(translation)\n",
    "assert len(jnetpred) == 298  # must be as long as 6aam\n",
    "assert set(jnetpred) == set('HSC')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for test_5_mer, code in test_data_6aam:\n",
    "    where_in_6aam = sequence_6aam.find(test_5_mer)\n",
    "    jp_predict = jnetpred[where_in_6aam+2]\n",
    "    #print(test_5_mer, code, jp_predict)\n",
    "    if code == jp_predict:\n",
    "        correct += 1\n",
    "print('Success JPred4 prediction for 6AAM test data is {:.1f}%'.format(100.*correct/len(test_data_6aam)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maybe try 3-mer rather than 5mer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run this cell to define the training set\n",
    "small_training_set = [('ADTLL','S'),\n",
    "                      ('DTLLI','S'),\n",
    "                      ('TLLIL','S'),\n",
    "                      ('LLILG','S'),\n",
    "                      ('LILGD','S'),\n",
    "                      ('ILGDS','S'),\n",
    "                      ('LGDSL','C'),\n",
    "                      ('GDSLS','H'),\n",
    "                      ('DSLSA','H'),\n",
    "                      ('SLSAG','H'),\n",
    "                      ('LSAGY','H'),\n",
    "                      ('SAGYR','C'),\n",
    "                      ('AGYRM','C'),\n",
    "                      ('GYRMS','C'),\n",
    "                      ('YRMSA','C'),\n",
    "                      ('RMSAS','C')]\n",
    "small_training_set_3 = []\n",
    "for k, v in small_training_set:\n",
    "    small_training_set_3.append((k[1:4],v))\n",
    "print(small_training_set_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run this cell to create the training data\n",
    "small_training_vector = []\n",
    "for seq, ss in small_training_set_3:\n",
    " \n",
    "        inputVec = convertSeqToVector(seq, aaIndexDict)\n",
    "        outputVec = convertSeqToVector(ss, ssIndexDict)\n",
    "\n",
    "        small_training_vector.append( (inputVec, outputVec) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wMatrixIn, wMatrixOut = neuralNetTrain(small_training_vector, 3, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to define \n",
    "def predict_3_mer(seq):\n",
    "    \"\"\"\n",
    "    returns a prediction either 'H', 'S' or 'C' for the input sequence of 5 amino acids\n",
    "    \"\"\"\n",
    "    global wMatrixIn  # produced by previous training\n",
    "    global wMatrixOut\n",
    "    vector_seq = convertSeqToVector(seq, indexDict=aaIndexDict)\n",
    "    array_seq = array([vector_seq,])\n",
    "    sIn, sHid, sOut =    neuralNetPredict(array_seq, wMatrixIn, wMatrixOut)\n",
    "    index = sOut.argmax()\n",
    "    return ssCodes[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for test_3_mer, code in small_training_set_3:\n",
    "    predict = predict_3_mer(test_3_mer)\n",
    "    if predict == code:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "    print(test_3_mer, 'input ', code, ' predict ', predict)\n",
    "print('success \"prediction\" for training data is {:.1f}%'.format(100.*correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = small_training_set + data[:100]\n",
    "##Run this cell to create the training data\n",
    "training_set_3 = [(k[1:4], v) for (k, v) in training_set]\n",
    "training_vector = []\n",
    "for seq, ss in training_set_3:\n",
    "        inputVec = convertSeqToVector(seq, aaIndexDict)\n",
    "        outputVec = convertSeqToVector(ss, ssIndexDict)\n",
    "        training_vector.append( (inputVec, outputVec) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MatrixIn, wMatrixOut = neuralNetTrain(training_vector, 4, 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for test_3_mer, code in training_set_3:\n",
    "    #print(test_3_mer, code)\n",
    "    predict = predict_3_mer(test_3_mer)\n",
    "    if predict == code:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "print('success \"prediction\" for training data is {:.1f}%'.format(100.*correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for test_5_mer, code in test_data_6aam:\n",
    "    test_3_mer = test_5_mer[1:4]\n",
    "    predict = predict_5_mer(test_3_mer)\n",
    "    if predict == code:\n",
    "        correct += 1\n",
    "print('success prediction for 6aan test data is {:.1f}%'.format(100.*correct/len(test_data_6aam)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lookup closest\n",
    "\n",
    "idea lookup 'closest' value in large dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_match_5 = {}\n",
    "match_central_3 = {}\n",
    "match_central_1 = {}\n",
    "for k, v in small_training_set + data:\n",
    "    exact_match_5[k] = v\n",
    "    match_central_3[k[1:4]] = v\n",
    "    match_central_1[k[3]] = v\n",
    "    \n",
    "def predict_closest(seq5):\n",
    "    if seq5 in exact_match_5:\n",
    "        print('match 5')\n",
    "        return exact_match_5[seq5]\n",
    "    if seq5[1:4] in match_central_3:\n",
    "        print('match 3')\n",
    "        return match_central_3[seq5[1:4]]\n",
    "    if seq5[3] in match_central_1:\n",
    "        print('match 1')\n",
    "        return match_central_1[seq5[3]]\n",
    "    print('no match')\n",
    "    return 'C'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for test_5_mer, code in small_training_set:\n",
    "    predict = predict_closest(test_5_mer)\n",
    "    if predict == code:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "    print(test_5_mer, 'input ', code, ' predict ', predict)\n",
    "print('success \"prediction\" for training data is {:.1f}%'.format(100.*correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for test_5_mer, code in test_data_6aam:\n",
    "    predict = predict_closest(test_5_mer)\n",
    "    if predict == code:\n",
    "        correct += 1\n",
    "print('success prediction for 6AAM test data is {:.1f}%'.format(100.*correct/len(test_data_6aam)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = {}\n",
    "for k, v in small_training_set + data:\n",
    "    if k not in values:\n",
    "        values[k] = [v]\n",
    "    else:\n",
    "        values[k] = values[k] + [v]\n",
    "for k, v in values.items():\n",
    "    if len(v) > 4:\n",
    "        print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
