{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A feed-forward Neural Network for secondary structure prediction\n",
    "This notebook looks at a Neural Network based on code from \n",
    "[Stevens and Boucher (2014, Python Programming for Biology CUP)](https://www.amazon.co.uk/Python-Programming-Biology-Bioinformatics-Beyond/dp/0521720095). The aim is to predict the secondary structure of a protein from its sequence. \n",
    "A Predictive network is trained using data on known secondary structure of k-mers of 5 amino-acids taken from a set of PDB structures. Three secondary structure states are defined: H, C, and S. H and S are helix and strand respectively while C is for coil which is a range of structures not with regular H-bonding pattern. In practice, secondary structure prediction has many uses, for instance in helping in the identification of functional domains ([Drozdetskiy et al., 2015](https://doi.org/10.1093/nar/gkv332)) and can be easily acheived using the JPRED4 server http://www.compbio.dundee.ac.uk/jpred4\n",
    "\n",
    "Secondary structure is much more complicated than indicated by the simple classification of this data - full details are available from the analysis of the H-bonding arrangements. The program DSSP (https://swift.cmbi.umcn.nl/gv/dssp/DSSP_3.html) is a well-tested approach to this problem. This produces a description of the secondary structure in a known protein structure. For historical reasons DSSP uses E for Strands. A related DSSR program gives RNA secondary structure.\n",
    "\n",
    "It is useful to be able to predict the secondary structure of a protein for which there is only sequence available. One approach would be to align it with homologous sequences where the structure is known. The approach here is to use the sequence in the neighbourhood of a residue as a basis for a neural network prediction. \n",
    "\n",
    "The network here is a simple three layer feed-forward one. The number of nodes in the hidden layer can be defined by the programmer. But the number of input and output nodes is defined by the sizes of the input and output data vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run this cell to import numpy \n",
    "from numpy import tanh, ones, append, array, zeros, random, sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network function takes input data for the first layer of Network nodes, applies the the first weighted connections to pass the signal to the hidden layer of nodes, then applies the second weights to produce output. \n",
    "\n",
    "The output may not be optimized as the function also operates on the weighting during training. However after training the function gives predictions so takes its name from that. \n",
    "\n",
    "The weightsIn values define the strength of connection between the input nodes and the hidden nodes. Similarly weightsOut define the strengths of connection between the hidden and the output nodes. \n",
    "\n",
    "The weights are given as matrices with the rows indexing the nodes in a layer and the columns indexing the nodes in the other layer. \n",
    "\n",
    "The signalIn vector is the input features and an extra value of 1.0. This additional value is called the bias node which is used to tune the baseline response of the network. The baseline is the level without a meaningful signal. \n",
    "\n",
    "Setting the bias node value happens during training to adapt to the values in the input data. This means the input data don't need to pre-prepared with a mean of zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this cell to define the function\n",
    "def neuralNetPredict(inputVec, weightsIn, weightsOut):\n",
    "    \"\"\" uses the current weights in a neural network\n",
    "    to make a prediction from an input vector\n",
    "    all input and output are numpy data structures\"\"\" \n",
    "    signalIn = append(inputVec, 1.0) # input layer\n",
    "\n",
    "    prod = signalIn * weightsIn.T\n",
    "    sums = sum(prod, axis=1)\n",
    "    signalHid = tanh(sums)    # hidden    layer\n",
    "\n",
    "    prod = signalHid * weightsOut.T\n",
    "    sums = sum(prod, axis=1)\n",
    "    signalOut = tanh(sums)    # output    layer\n",
    "\n",
    "    return signalIn, signalHid, signalOut\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the numpy `.T` methodn gives the transpose of a matrix - that is the matrix with the columns turned into rows and the rows turned into columns. \n",
    "\n",
    "This is used so that the input signal gets applied to all the hidden nodes.\n",
    "\n",
    "The network applies the hyperbolic tangent function (tanh) to get the signal output from all the nodes in layer. Hyperbolic tan is a sigmoidal function that varies from -1 to 1, so it is much better than ordinary tan that runs off to infinity. \n",
    "\n",
    "<img src=\"https://mathworld.wolfram.com/images/interactive/TanhReal.gif\" width=300></img> \n",
    "\n",
    "The tanh function defines the output of that node given an input or set of inputs. As a nod to the output of neurones, which depends on an activation level across their cell membrane, the output function is called the *Activation* function.\n",
    "\n",
    "In operation only the signalOut from the output layer is of interest. But during training the response signals from the other layers are also needed to adjust the weighting scheme.\n",
    "\n",
    "### Training \n",
    "\n",
    "The weighting scheme (and gain) will be optimized by using a training dataset. \n",
    "\n",
    "The training data will be an input feature vector and a known output vector. The order of the data will be randomly shuffled to avoid bias. The number of hidden nodes needs to be specified and the number of optimization cycles. \n",
    "\n",
    "After each cycle the 'error' between the output signal of the network and the known training set output is used to adjust the network weights. The difference is combined with the *gradient* in the signal values - calculated from the tanh activation function (conveniently the gradient of tanh(sig) is 1-sig<sup>2</sup> or 1 - {sig x sig}). \n",
    "\n",
    "Early in training large difference can make the network go haywire so the speed of weight changing is damped down by a 'rate' and 'momentum' multipliers (usually the default values of 0.5 and 0.2 are good enough). \n",
    "\n",
    "More damping would mean that many more cycles would be needed for the weights to converge. \n",
    "\n",
    "The training will work back from the value of the error to adjust the weighting scheme of the network. This is called *back propagation*. \n",
    "\n",
    "The use of the gradient is crucial as it means initial adjustments will be large but then finer adjustments will be made as the optimum is approached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this cell to define the neuralNetTrain function\n",
    "def neuralNetTrain(trainData, numHid, steps=100, rate=0.5, momentum=0.2, wInp=None, wOut=None):\n",
    "    \"\"\" uses training data to set the weights in a simple\n",
    "    neural network, number of hidden nodes is specified\"\"\"\n",
    "    numInp = len(trainData[0][0])\n",
    "    numOut = len(trainData[0][1])\n",
    "    numInp += 1\n",
    "    minError = None\n",
    "\n",
    "    sigInp = ones(numInp)\n",
    "    sigHid = ones(numHid)\n",
    "    sigOut = ones(numOut)\n",
    "    \n",
    "    if wInp is None:\n",
    "        wInp = random.random((numInp, numHid))-0.5\n",
    "    \n",
    "    if wOut is None:\n",
    "        wOut = random.random((numHid, numOut))-0.5\n",
    "    bestWeightMatrices = (wInp, wOut)\n",
    "\n",
    "    cInp = zeros((numInp, numHid))\n",
    "    cOut = zeros((numHid, numOut))\n",
    "\n",
    "    for x, (inputs, knownOut) in enumerate(trainData):\n",
    "        trainData[x] = (array(inputs), array(knownOut))\n",
    " \n",
    "    for step in range(steps):  \n",
    "        random.shuffle(trainData) # Important to avoid bias\n",
    "        error = 0.0\n",
    " \n",
    "        for inputs, knownOut in trainData:\n",
    "            sigIn, sigHid, sigOut = neuralNetPredict(inputs, wInp, wOut)\n",
    "\n",
    "            diff = knownOut - sigOut\n",
    "            error += sum(diff * diff)\n",
    "\n",
    "            gradient = ones(numOut) - (sigOut*sigOut)\n",
    "            outAdjust = gradient * diff \n",
    "\n",
    "            diff = sum(outAdjust * wOut, axis=1)\n",
    "            gradient = ones(numHid) - (sigHid*sigHid)\n",
    "            hidAdjust = gradient * diff \n",
    "\n",
    "            # update output \n",
    "            change = outAdjust * sigHid.reshape(numHid, 1)\n",
    "            wOut += (rate * change) + (momentum * cOut)\n",
    "            cOut = change\n",
    " \n",
    "            # update input \n",
    "            change = hidAdjust * sigIn.reshape(numInp, 1)\n",
    "            wInp += (rate * change) + (momentum * cInp)\n",
    "            cInp = change\n",
    " \n",
    "        if (minError is None) or (error < minError):\n",
    "            minError = error\n",
    "            bestWeightMatrices = (wInp.copy(), wOut.copy())\n",
    "            print(\"Step: %d Error: %f\" % (step, error))\n",
    "    \n",
    "    return bestWeightMatrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the functions\n",
    "Simple data to test a network can be binary input vectors with the desired output being an 'exclusive OR' (EOR) response https://en.wikipedia.org/wiki/Exclusive_or. This responds True if any input is true but False is both are together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run this cell to define the test data\n",
    "testEORdata = [[[0,0], [0]],\n",
    "               [[0,1], [1]], \n",
    "               [[1,0], [1]], \n",
    "               [[1,1], [0]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network test uses two hidden nodes - in real use several values would be tried to find the best performance.\n",
    "Run the cell below to see if the training converges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Error: 2.117528\n",
      "Step: 1 Error: 1.392060\n",
      "Step: 2 Error: 1.365615\n",
      "Step: 3 Error: 1.280868\n",
      "Step: 4 Error: 1.239094\n",
      "Step: 5 Error: 1.096176\n",
      "Step: 6 Error: 0.970600\n",
      "Step: 10 Error: 0.909827\n",
      "Step: 11 Error: 0.901503\n",
      "Step: 13 Error: 0.899322\n",
      "Step: 14 Error: 0.842626\n",
      "Step: 17 Error: 0.829305\n",
      "Step: 18 Error: 0.776297\n",
      "Step: 22 Error: 0.750190\n",
      "Step: 24 Error: 0.709475\n",
      "Step: 29 Error: 0.678282\n",
      "Step: 31 Error: 0.643846\n",
      "Step: 33 Error: 0.635477\n",
      "Step: 34 Error: 0.627824\n",
      "Step: 35 Error: 0.570631\n",
      "Step: 39 Error: 0.511134\n",
      "Step: 40 Error: 0.505995\n",
      "Step: 41 Error: 0.457139\n",
      "Step: 42 Error: 0.331631\n",
      "Step: 48 Error: 0.329306\n",
      "Step: 50 Error: 0.205766\n",
      "Step: 55 Error: 0.203626\n",
      "Step: 56 Error: 0.118081\n",
      "Step: 68 Error: 0.111239\n",
      "Step: 70 Error: 0.088335\n",
      "Step: 77 Error: 0.079027\n",
      "Step: 82 Error: 0.077963\n",
      "Step: 84 Error: 0.057786\n",
      "Step: 92 Error: 0.042400\n",
      "Step: 100 Error: 0.041747\n",
      "Step: 102 Error: 0.037621\n",
      "Step: 108 Error: 0.029122\n",
      "Step: 116 Error: 0.028673\n",
      "Step: 119 Error: 0.027827\n",
      "Step: 125 Error: 0.023189\n",
      "Step: 129 Error: 0.016571\n",
      "Step: 152 Error: 0.014270\n",
      "Step: 162 Error: 0.013250\n",
      "Step: 164 Error: 0.011680\n",
      "Step: 171 Error: 0.009900\n",
      "Step: 174 Error: 0.008896\n",
      "Step: 181 Error: 0.008008\n",
      "Step: 199 Error: 0.007920\n",
      "Step: 202 Error: 0.007679\n",
      "Step: 203 Error: 0.006327\n",
      "Step: 207 Error: 0.006147\n",
      "Step: 217 Error: 0.005756\n",
      "Step: 233 Error: 0.005456\n",
      "Step: 240 Error: 0.004758\n",
      "Step: 248 Error: 0.004677\n",
      "Step: 257 Error: 0.004579\n",
      "Step: 259 Error: 0.004260\n",
      "Step: 266 Error: 0.004002\n",
      "Step: 275 Error: 0.003969\n",
      "Step: 276 Error: 0.003814\n",
      "Step: 288 Error: 0.003730\n",
      "Step: 290 Error: 0.003639\n",
      "Step: 294 Error: 0.003348\n",
      "Step: 297 Error: 0.003285\n",
      "Step: 311 Error: 0.002835\n",
      "Step: 328 Error: 0.002780\n",
      "Step: 333 Error: 0.002682\n",
      "Step: 335 Error: 0.002514\n",
      "Step: 355 Error: 0.002513\n",
      "Step: 359 Error: 0.002510\n",
      "Step: 360 Error: 0.002507\n",
      "Step: 363 Error: 0.002369\n",
      "Step: 364 Error: 0.002323\n",
      "Step: 374 Error: 0.002262\n",
      "Step: 380 Error: 0.002017\n",
      "Step: 411 Error: 0.001942\n",
      "Step: 416 Error: 0.001904\n",
      "Step: 418 Error: 0.001757\n",
      "Step: 431 Error: 0.001756\n",
      "Step: 436 Error: 0.001704\n",
      "Step: 450 Error: 0.001669\n",
      "Step: 454 Error: 0.001609\n",
      "Step: 467 Error: 0.001584\n",
      "Step: 470 Error: 0.001557\n",
      "Step: 472 Error: 0.001546\n",
      "Step: 477 Error: 0.001501\n",
      "Step: 480 Error: 0.001500\n",
      "Step: 492 Error: 0.001443\n",
      "Step: 498 Error: 0.001438\n",
      "Step: 502 Error: 0.001395\n",
      "Step: 504 Error: 0.001384\n",
      "Step: 512 Error: 0.001310\n",
      "Step: 536 Error: 0.001307\n",
      "Step: 539 Error: 0.001282\n",
      "Step: 544 Error: 0.001264\n",
      "Step: 549 Error: 0.001233\n",
      "Step: 551 Error: 0.001169\n",
      "Step: 578 Error: 0.001158\n",
      "Step: 582 Error: 0.001090\n",
      "Step: 597 Error: 0.001063\n",
      "Step: 608 Error: 0.001062\n",
      "Step: 613 Error: 0.001052\n",
      "Step: 627 Error: 0.001050\n",
      "Step: 629 Error: 0.001045\n",
      "Step: 634 Error: 0.001038\n",
      "Step: 636 Error: 0.001033\n",
      "Step: 638 Error: 0.001016\n",
      "Step: 640 Error: 0.000970\n",
      "Step: 657 Error: 0.000951\n",
      "Step: 666 Error: 0.000942\n",
      "Step: 669 Error: 0.000941\n",
      "Step: 673 Error: 0.000932\n",
      "Step: 681 Error: 0.000902\n",
      "Step: 683 Error: 0.000901\n",
      "Step: 693 Error: 0.000891\n",
      "Step: 695 Error: 0.000875\n",
      "Step: 703 Error: 0.000851\n",
      "Step: 727 Error: 0.000845\n",
      "Step: 739 Error: 0.000845\n",
      "Step: 740 Error: 0.000812\n",
      "Step: 752 Error: 0.000805\n",
      "Step: 761 Error: 0.000785\n",
      "Step: 770 Error: 0.000757\n",
      "Step: 796 Error: 0.000752\n",
      "Step: 802 Error: 0.000746\n",
      "Step: 804 Error: 0.000729\n",
      "Step: 817 Error: 0.000722\n",
      "Step: 829 Error: 0.000719\n",
      "Step: 838 Error: 0.000713\n",
      "Step: 840 Error: 0.000698\n",
      "Step: 848 Error: 0.000689\n",
      "Step: 861 Error: 0.000685\n",
      "Step: 862 Error: 0.000676\n",
      "Step: 864 Error: 0.000668\n",
      "Step: 870 Error: 0.000667\n",
      "Step: 873 Error: 0.000657\n",
      "Step: 882 Error: 0.000652\n",
      "Step: 891 Error: 0.000632\n",
      "Step: 907 Error: 0.000628\n",
      "Step: 917 Error: 0.000627\n",
      "Step: 923 Error: 0.000627\n",
      "Step: 932 Error: 0.000612\n",
      "Step: 939 Error: 0.000601\n",
      "Step: 957 Error: 0.000600\n",
      "Step: 963 Error: 0.000586\n",
      "Step: 966 Error: 0.000585\n",
      "Step: 977 Error: 0.000577\n",
      "Step: 986 Error: 0.000574\n",
      "Step: 993 Error: 0.000566\n"
     ]
    }
   ],
   "source": [
    "## Run this cell to train the network\n",
    "wMatrixIn, wMatrixOut = neuralNetTrain(testEORdata, 2, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here quite good convergence has occurred and there is no oscillation. Perhaps you can see that the initial steps are giving large changes in the error while later on there are smaller and smaller changes. This is owing to the effect of the gradient calculation. The changes in the actual weights are not printed, but will follow the same trend.\n",
    "\n",
    "The output weight matrices can then be run on test data for evaluation. Test data should be inputs with known output but which were not included in the training set. \n",
    "\n",
    "Obviously it is not possible to give any new data for the EOR function as the training set covered all possible responses!\n",
    "\n",
    "But the trained network should be able to do a reasonable job on the training set.\n",
    "Run the following cell to compare the output of the network with the actual values of the training set outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input [0 1]  should have output  [1] actual output 0.984\n",
      "input [1 1]  should have output  [0] actual output 0.006\n",
      "input [1 0]  should have output  [1] actual output 0.984\n",
      "input [0 0]  should have output  [0] actual output -0.002\n"
     ]
    }
   ],
   "source": [
    "##Run this cell to test the network\n",
    "for inputs, knownOut in testEORdata:\n",
    "    sIn, sHid, sOut =    neuralNetPredict(array(inputs), wMatrixIn, wMatrixOut)\n",
    "    print('input', inputs, ' should have output ', knownOut, 'actual output {:.3f}'.format(sOut[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple feature vectors for sequence data\n",
    "A simple numbering scheme is used to convert to the sequence alphabet to a numeric form as an input vector. For proteins that is number from 1 to 20 from the list of one-letter codes.\n",
    "\n",
    "k-mers with k=5 are used. Only the output for the middle residue is required but the network will use the neighbours to predict the secondary structure of the middle one.  \n",
    "\n",
    "Although static k-mer are used for training in practice a prediction in a moving 5-mer window could be implemented. \n",
    "\n",
    "The possible outputs are also coded as integers for the more restricted alphabet of H, C, and S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run this cell to define the dictionaries for the vectors\n",
    "aminoAcids = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "aaIndexDict = {}\n",
    "for i, aa in enumerate(aminoAcids):\n",
    "        aaIndexDict[aa] = i\n",
    "\n",
    "ssIndexDict = {}\n",
    "ssCodes = 'HCS'\n",
    "for i, code in enumerate(ssCodes):\n",
    "        ssIndexDict[code] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a very limited set of training data. It shows the raw format which is a 5-mer string and the secondary structure that was observed for the central residue of this in at least one PDB structure. \n",
    "\n",
    "The actual structure is a simplified output from the DSSP program mentioned in the introduction. DSSP acutally distinguishes more structures that the three here - for example there are other kinds of helix. But these complications are not dealt with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run this cell to define the training set\n",
    "small_training_set = [('ADTLL','S'),\n",
    "                      ('DTLLI','S'),\n",
    "                      ('TLLIL','S'),\n",
    "                      ('LLILG','S'),\n",
    "                      ('LILGD','S'),\n",
    "                      ('ILGDS','S'),\n",
    "                      ('LGDSL','C'),\n",
    "                      ('GDSLS','H'),\n",
    "                      ('DSLSA','H'),\n",
    "                      ('SLSAG','H'),\n",
    "                      ('LSAGY','H'),\n",
    "                      ('SAGYR','C'),\n",
    "                      ('AGYRM','C'),\n",
    "                      ('GYRMS','C'),\n",
    "                      ('YRMSA','C'),\n",
    "                      ('RMSAS','C')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data has to be converted to the numerical code. Here is a function to to that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run this cell to define the function\n",
    "def convertSeqToVector(seq, indexDict):\n",
    "    \"\"\"converts a one-letter sequence to numerical\n",
    "    coding for neural network calculations\"\"\"   \n",
    "    numLetters = len(indexDict)\n",
    "    vector = [0.0] * len(seq) * numLetters\n",
    "\n",
    "    for pos, letter in enumerate(seq):\n",
    "        index = pos * numLetters + indexDict[letter]    \n",
    "        vector[index] = 1.0\n",
    "\n",
    "    return vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data is prepared with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run this cell to create the training data\n",
    "small_training_vector = []\n",
    "for seq, ss in small_training_set:\n",
    " \n",
    "        inputVec = convertSeqToVector(seq, aaIndexDict)\n",
    "        outputVec = convertSeqToVector(ss, ssIndexDict)\n",
    "\n",
    "        small_training_vector.append( (inputVec, outputVec) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then the network is trained. Here there are 3 hidden nodes specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Error: 19.635992\n",
      "Step: 1 Error: 11.546471\n",
      "Step: 2 Error: 11.348317\n",
      "Step: 3 Error: 7.996287\n",
      "Step: 4 Error: 5.794609\n",
      "Step: 5 Error: 5.535160\n",
      "Step: 6 Error: 2.135310\n",
      "Step: 7 Error: 1.445940\n",
      "Step: 9 Error: 1.253434\n",
      "Step: 10 Error: 0.705480\n",
      "Step: 11 Error: 0.675753\n",
      "Step: 13 Error: 0.404313\n",
      "Step: 18 Error: 0.396442\n",
      "Step: 19 Error: 0.256821\n",
      "Step: 27 Error: 0.219662\n",
      "Step: 34 Error: 0.154109\n",
      "Step: 37 Error: 0.116712\n",
      "Step: 51 Error: 0.108171\n",
      "Step: 67 Error: 0.103497\n",
      "Step: 69 Error: 0.048243\n",
      "Step: 83 Error: 0.045614\n",
      "Step: 85 Error: 0.043765\n",
      "Step: 88 Error: 0.030492\n",
      "Step: 97 Error: 0.023526\n",
      "Step: 100 Error: 0.016722\n",
      "Step: 173 Error: 0.012254\n",
      "Step: 174 Error: 0.009497\n",
      "Step: 232 Error: 0.007960\n",
      "Step: 237 Error: 0.005484\n",
      "Step: 238 Error: 0.004232\n",
      "Step: 322 Error: 0.004194\n",
      "Step: 328 Error: 0.003167\n",
      "Step: 380 Error: 0.002579\n",
      "Step: 408 Error: 0.001786\n",
      "Step: 522 Error: 0.001459\n",
      "Step: 579 Error: 0.001129\n",
      "Step: 874 Error: 0.001025\n",
      "Step: 879 Error: 0.000980\n",
      "Step: 886 Error: 0.000974\n",
      "Step: 894 Error: 0.000924\n",
      "Step: 899 Error: 0.000660\n",
      "Step: 902 Error: 0.000617\n",
      "Step: 903 Error: 0.000563\n",
      "Step: 908 Error: 0.000525\n"
     ]
    }
   ],
   "source": [
    "wMatrixIn, wMatrixOut = neuralNetTrain(small_training_vector, 3, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see that this training has converged nicely. The only problem is that it was for a very restricted set of sequence data. \n",
    "\n",
    "There are 3 x 20 = 60 theoretical combinations of amino acid residues with secondary structure states. But for particular residues some of these are favoured and others disfavoured. \n",
    "\n",
    "For each residue there will be 20^4 = 160 000 different contexts that then could possibly occur in. Although some of the resulting 5-mers are actually quite rare in structured proteins. \n",
    "\n",
    "All the same, it would be good to have a larger training set. It is better to have some rare examples of residue state combinations although, of course, the network will not have to predict them frequently.\n",
    "\n",
    "One thing to remember is to retain some test examples - where the answer is known but which are not in the training set. \n",
    "\n",
    "In its current, poorly-trained state, the network is still able to make a reasonable predictions. But only if the test is clearly related to examples that it has seen. testSecStrucSeq here is very similar to examples in the seqSecStrucData training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to define \n",
    "def predict_seq(seq, w_matrix_in, w_matrix_out):\n",
    "    \"\"\"\n",
    "    returns a prediction either 'H', 'S' or 'C' for the input sequence\n",
    "    \"\"\"\n",
    "    vector_seq = convertSeqToVector(seq, indexDict=aaIndexDict)\n",
    "    array_seq = array([vector_seq,])\n",
    "    sIn, sHid, sOut =    neuralNetPredict(array_seq, w_matrix_in, w_matrix_out)\n",
    "    index = sOut.argmax()\n",
    "    return ssCodes[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADTLL input  S  predict  S\n",
      "DTLLI input  S  predict  S\n",
      "TLLIL input  S  predict  S\n",
      "LLILG input  S  predict  S\n",
      "LILGD input  S  predict  S\n",
      "ILGDS input  S  predict  S\n",
      "LGDSL input  C  predict  C\n",
      "GDSLS input  H  predict  H\n",
      "DSLSA input  H  predict  H\n",
      "SLSAG input  H  predict  H\n",
      "LSAGY input  H  predict  H\n",
      "SAGYR input  C  predict  C\n",
      "AGYRM input  C  predict  C\n",
      "GYRMS input  C  predict  C\n",
      "YRMSA input  C  predict  C\n",
      "RMSAS input  C  predict  C\n",
      "success \"prediction\" for training data is 100.0%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for test_5_mer, code in small_training_set:\n",
    "    predict = predict_seq(test_5_mer, wMatrixIn, wMatrixOut)\n",
    "    if predict == code:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "    print(test_5_mer, 'input ', code, ' predict ', predict)\n",
    "print('success \"prediction\" for training data is {:.1f}%'.format(100.*correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediction for DLLSA is H\n"
     ]
    }
   ],
   "source": [
    "test_expect_h = 'DLLSA'\n",
    "print('prediction for', test_expect_h, 'is', predict_seq(test_expect_h, wMatrixIn, wMatrixOut))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data from PDB structure 6aam, divided into 5-mers\n",
    "test_data_6aam = [('DPTVF', 'C'), ('HKRYL', 'C'), ('KKIRD', 'S'), ('LGEGH', 'C'), \n",
    "                  ('FGKVS', 'S'), ('LYCYD', 'S'), ('PTNDG', 'C'), ('TGEMV', 'S'), \n",
    "                  ('AVKAL', 'S'), ('KADAG', 'C'), ('PQHRS', 'H'), ('GWKQE', 'H'), \n",
    "                  ('IDILR', 'H'), ('TLYHE', 'C'), ('HIIKY', 'C'), ('KGCCE', 'S'), \n",
    "                  ('DAGAA', 'C'), ('SLQLV', 'S'), ('MEYVP', 'C'), ('LGSLR', 'S'), \n",
    "                  ('DYLPR', 'C'), ('HSIGL', 'C'), ('AQLLL', 'H'), ('FAQQI', 'H'), \n",
    "                  ('CEGMA', 'H'), ('YLHAQ', 'H'), ('HYIHR', 'S'), ('NLAAR', 'S'), \n",
    "                  ('NVLLD', 'S'), ('NDRLV', 'C'), ('KIGDF', 'C'), ('GLAKA', 'C'), \n",
    "                  ('VPEGH', 'C'), ('EYYRV', 'C'), ('REDGD', 'C'), ('SPVFW', 'C'), \n",
    "                  ('YAPEC', 'H'), ('LKEYK', 'H'), ('FYYAS', 'H'), ('DVWSF', 'H'), \n",
    "                  ('GVTLY', 'H'), ('ELLTH', 'H'), ('CDSSQ', 'C'), ('SPPTK', 'H'), \n",
    "                  ('FLELI', 'H'), ('GLAQG', 'C'), ('QMTVL', 'H'), ('RLTEL', 'H'), \n",
    "                  ('LERGE', 'C'), ('RLPRP', 'C'), ('DKCPA', 'C'), ('EVYHL', 'H'), \n",
    "                  ('MKNCW', 'H'), ('ETEAS', 'S'), ('FRPTF', 'C'), ('ENLIP', 'H'), \n",
    "                  ('ILKTV', 'H'), ('HEKYQ', 'H'), ('GQAPS', 'C')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success prediction for test data is 27.1%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for test_5_mer, code in test_data_6aam:\n",
    "    predict = predict_seq(test_5_mer, wMatrixIn, wMatrixOut)\n",
    "    if predict == code:\n",
    "        correct += 1\n",
    "print('success prediction for 6AAM test data is {:.1f}%'.format(100.*correct/len(test_data_6aam)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# making a prediction of the secondary structure for PDB 6aam\n",
    "\n",
    "Lets use the recent PDB structure 6AAM \"Non-receptor tyrosine-protein kinase TYK2\" \n",
    "https://www.ebi.ac.uk/pdbe/entry/pdb/6aam/\n",
    "    \n",
    "as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6aam sequence from https://www.ebi.ac.uk/pdbe/entry/pdb/6aam/protein/1\n",
    "sequence_6aam = ('GPGDPTVFHKRYLKKIRDLGEGHFGKVSLYCYDPTNDGTGEMVAVKALKADAGP'\n",
    "                 'QHRSGWKQEIDILRTLYHEHIIKYKGCCEDAGAASLQLVMEYVPLGSLRDYLPR'\n",
    "                 'HSIGLAQLLLFAQQICEGMAYLHAQHYIHRNLAARNVLLDNDRLVKIGDFGLAK'\n",
    "                 'AVPEGHEYYRVREDGDSPVFWYAPECLKEYKFYYASDVWSFGVTLYELLTHCDS'\n",
    "                 'SQSPPTKFLELIGLAQGQMTVLRLTELLERGERLPRPDKCPAEVYHLMKNCWET'\n",
    "                 'EASFRPTFENLIPILKTVHEKYQGQAPS')\n",
    "print(len(sequence_6aam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://cdn.rcsb.org/etl/kabschSander/ss_dis.txt.gz\n",
    "dssp_result_for_6aam  = \"\"\">6AAM:A:secstr\n",
    "      B  GGGEEEEEE       EEEEEEE TT     EEEEEEE      TTHHHHHHHHHHHHHH   TTB\n",
    "  EEEEEEEGGGTEEEEEEE  TT BHHHHGGGS   HHHHHHHHHHHHHHHHHHHHTTEE S  SGGGEEEEET\n",
    "TEEEE   TT EE                 GGG  HHHHHH    HHHHHHHHHHHHHHHHTTT GGGSHHHHHH\n",
    "HHH S  TT HHHHHHHHHHTT      TT  HHHHHHHHHHT SSGGGS  HHHHHHHHHHHHHHHH     \n",
    "\"\"\"\n",
    "dssp_result_for_6aam = dssp_result_for_6aam.splitlines()\n",
    "dssp_result_for_6aam.pop(0)\n",
    "dssp_result_for_6aam = ''.join(dssp_result_for_6aam)\n",
    "# need to convert DSSP code to the 3-category helix, strand, coil.\n",
    "# Use mapping \n",
    "# helices H, C, I go to H\n",
    "# strands E & bridges B go to S\n",
    "# everything else got to C\n",
    "translation = str.maketrans('HCIEB GT', 'HHHSSCCC')\n",
    "dssp_result_for_6aam = dssp_result_for_6aam.translate(translation)\n",
    "print(len(dssp_result_for_6aam))\n",
    "print(dssp_result_for_6aam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_from_6aam = []\n",
    "for ires, dssp in enumerate(dssp_result_for_6aam):\n",
    "    if ires>1 and ires%5 == 0:\n",
    "        fivemer = sequence_6aam[ires-2:ires+3]\n",
    "        test_data_from_6aam.append((fivemer, dssp))\n",
    "print(test_data_from_6aam)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_for_sequence(sequence):\n",
    "    prediction = []\n",
    "    for ires, residue in enumerate(sequence):\n",
    "        if ires < 2 or ires > len(sequence) - 3:\n",
    "            this_prediction = '.'\n",
    "        else:\n",
    "            fivemer = sequence[ires-2:ires+3]\n",
    "            testSecStrucVec = convertSeqToVector(fivemer, aaIndexDict)\n",
    "            testSecStrucArray = array( [testSecStrucVec,] )\n",
    "            sIn, sHid, sOut =    neuralNetPredict(testSecStrucArray, wMatrixIn, wMatrixOut)\n",
    "            index = sOut.argmax()\n",
    "            this_prediction = ssCodes[index]\n",
    "        prediction.append(this_prediction)\n",
    "    return ''.join(prediction)\n",
    "\n",
    "initial_predict_6aam = predict_for_sequence(sequence_6aam)\n",
    "print(initial_predict_6aam)\n",
    "print(len(initial_predict_6aam))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_line(first_seq, second_seq):\n",
    "    \"\"\" \n",
    "    for the two sequences returns a line where matching letters are \n",
    "    highlighted with | except if the letter are a gap\n",
    "    \"\"\"\n",
    "    joins = ['|' if a == b and a != '-' else ' ' for a, b in zip(first_seq, second_seq)]\n",
    "    return ''.join(joins)\n",
    "def print_alignment(seq_a, seq_b):\n",
    "    len_split = 50\n",
    "    n_splits = len(seq_a)//len_split + 1\n",
    "    for i_split in range(n_splits):\n",
    "        start = len_split*i_split\n",
    "        end = start + len_split\n",
    "        part_a = seq_a[start:end]\n",
    "        part_b = seq_b[start:end]\n",
    "        print(part_a)\n",
    "        print(highlight_line(part_a, part_b))\n",
    "        print(part_b)\n",
    "        print()\n",
    "\n",
    "print_alignment(dssp_result_for_6aam, initial_predict_6aam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def success(seq_a, seq_b):\n",
    "    same = 0\n",
    "    different = 0\n",
    "    for let_a, let_b in zip(seq_a, seq_b):\n",
    "        if let_a == let_b:\n",
    "            same += 1\n",
    "        else:\n",
    "            different += 1\n",
    "    return same/(same + different)\n",
    "print('percentage correct predictions: {:.1f}%'.format(100*success(dssp_result_for_6aam, initial_predict_6aam)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inporting a larger training set\n",
    "A much larger training set is provided as a comma separated file called:\n",
    "\"PDB_protein_secondary_5mers.csv\". \n",
    "\n",
    "    from csv import reader #may help here\n",
    "Can you create a training set vector data structure from this and use it to train the network?\n",
    "If you would like some examples of test data, here are some examples from the recently-determined PDB 6aam.pdb\n",
    "\n",
    "    S: EMVAV, KVSKY, YKGCC\n",
    "    H: LAQLL, ICEGM, ASVDW\n",
    "    C: ERLPR, GDFGL, YKFYY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have read 26242 lines from csv file\n",
      "first 5 lines [['MGKMY', 'S'], ['YGIPQ', 'C'], ['KMWTY', 'H'], ['YRLRK', 'H'], ['NSVSV', 'S']]\n"
     ]
    }
   ],
   "source": [
    "# answer\n",
    "from csv import reader\n",
    "with open('PDB_protein_secondary_5mers.csv') as csv_file:\n",
    "    csv_reader = reader(csv_file, delimiter=',')\n",
    "    data = list(csv_reader)\n",
    "print('have read', len(data), 'lines from csv file')\n",
    "print('first 5 lines', data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = small_training_set + data[:2000]\n",
    "##Run this cell to create the training data\n",
    "training_vector = []\n",
    "for seq, ss in training_set:\n",
    "        inputVec = convertSeqToVector(seq, aaIndexDict)\n",
    "        outputVec = convertSeqToVector(ss, ssIndexDict)\n",
    "        training_vector.append( (inputVec, outputVec) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Error: 2227.156243\n",
      "Step: 4 Error: 2213.111653\n",
      "Step: 13 Error: 2198.422190\n",
      "Step: 24 Error: 2191.190179\n",
      "Step: 32 Error: 2187.640857\n",
      "Step: 51 Error: 2185.208241\n",
      "Step: 56 Error: 2170.163848\n",
      "Step: 69 Error: 2162.817387\n",
      "Step: 94 Error: 2157.230469\n",
      "Step: 102 Error: 2156.136502\n",
      "Step: 277 Error: 2140.591012\n",
      "Step: 798 Error: 2126.489703\n",
      "Step: 898 Error: 2095.296813\n"
     ]
    }
   ],
   "source": [
    "wMatrixIn, wMatrixOut = neuralNetTrain(training_vector, 3, 1000, wInp=wMatrixIn, wOut=wMatrixOut)\n",
    "#\n",
    "#wMatrixIn, wMatrixOut = neuralNetTrain(training_vector, 3, 1000, wInp=wMatrixIn, wOut=wMatrixOut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADTLL input  S  predict  H\n",
      "DTLLI input  S  predict  S\n",
      "TLLIL input  S  predict  H\n",
      "LLILG input  S  predict  H\n",
      "LILGD input  S  predict  S\n",
      "ILGDS input  S  predict  S\n",
      "LGDSL input  C  predict  S\n",
      "GDSLS input  H  predict  S\n",
      "DSLSA input  H  predict  H\n",
      "SLSAG input  H  predict  S\n",
      "LSAGY input  H  predict  H\n",
      "SAGYR input  C  predict  S\n",
      "AGYRM input  C  predict  S\n",
      "GYRMS input  C  predict  S\n",
      "YRMSA input  C  predict  S\n",
      "RMSAS input  C  predict  C\n",
      "success \"prediction\" for training data is 37.5%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for test_5_mer, code in small_training_set:\n",
    "    predict = predict_seq(test_5_mer, wMatrixIn, wMatrixOut)\n",
    "    if predict == code:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "    print(test_5_mer, 'input ', code, ' predict ', predict)\n",
    "print('success \"prediction\" for training data is {:.1f}%'.format(100.*correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "success prediction for 6AAM test data is 25.4%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "for test_5_mer, code in test_data_6aam:\n",
    "    predict = predict_seq(test_5_mer, wMatrixIn, wMatrixOut)\n",
    "    if predict == code:\n",
    "        correct += 1\n",
    "print('success prediction for 6AAM test data is {:.1f}%'.format(100.*correct/len(test_data_6aam)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Maybe try 3-mer rather than 5mer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run this cell to define the training set\n",
    "small_training_set = [('ADTLL','S'),\n",
    "                      ('DTLLI','S'),\n",
    "                      ('TLLIL','S'),\n",
    "                      ('LLILG','S'),\n",
    "                      ('LILGD','S'),\n",
    "                      ('ILGDS','S'),\n",
    "                      ('LGDSL','C'),\n",
    "                      ('GDSLS','H'),\n",
    "                      ('DSLSA','H'),\n",
    "                      ('SLSAG','H'),\n",
    "                      ('LSAGY','H'),\n",
    "                      ('SAGYR','C'),\n",
    "                      ('AGYRM','C'),\n",
    "                      ('GYRMS','C'),\n",
    "                      ('YRMSA','C'),\n",
    "                      ('RMSAS','C')]\n",
    "small_training_set_3 = []\n",
    "for k, v in small_training_set:\n",
    "    small_training_set_3.append((k[1:4],v))\n",
    "print(small_training_set_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run this cell to create the training data\n",
    "small_training_vector = []\n",
    "for seq, ss in small_training_set_3:\n",
    " \n",
    "        inputVec = convertSeqToVector(seq, aaIndexDict)\n",
    "        outputVec = convertSeqToVector(ss, ssIndexDict)\n",
    "\n",
    "        small_training_vector.append( (inputVec, outputVec) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wMatrixIn, wMatrixOut = neuralNetTrain(small_training_vector, 3, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this cell to define \n",
    "def predict_3_mer(seq):\n",
    "    \"\"\"\n",
    "    returns a prediction either 'H', 'S' or 'C' for the input sequence of 5 amino acids\n",
    "    \"\"\"\n",
    "    global wMatrixIn  # produced by previous training\n",
    "    global wMatrixOut\n",
    "    vector_seq = convertSeqToVector(seq, indexDict=aaIndexDict)\n",
    "    array_seq = array([vector_seq,])\n",
    "    sIn, sHid, sOut =    neuralNetPredict(array_seq, wMatrixIn, wMatrixOut)\n",
    "    index = sOut.argmax()\n",
    "    return ssCodes[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for test_3_mer, code in small_training_set_3:\n",
    "    predict = predict_3_mer(test_3_mer)\n",
    "    if predict == code:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "    print(test_3_mer, 'input ', code, ' predict ', predict)\n",
    "print('success \"prediction\" for training data is {:.1f}%'.format(100.*correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = small_training_set + data[:100]\n",
    "##Run this cell to create the training data\n",
    "training_set_3 = [(k[1:4], v) for (k, v) in training_set]\n",
    "training_vector = []\n",
    "for seq, ss in training_set_3:\n",
    "        inputVec = convertSeqToVector(seq, aaIndexDict)\n",
    "        outputVec = convertSeqToVector(ss, ssIndexDict)\n",
    "        training_vector.append( (inputVec, outputVec) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MatrixIn, wMatrixOut = neuralNetTrain(training_vector, 4, 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for test_3_mer, code in training_set_3:\n",
    "    #print(test_3_mer, code)\n",
    "    predict = predict_3_mer(test_3_mer)\n",
    "    if predict == code:\n",
    "        correct += 1\n",
    "    total += 1\n",
    "print('success \"prediction\" for training data is {:.1f}%'.format(100.*correct/total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = 0\n",
    "for test_5_mer, code in test_data_6aam:\n",
    "    test_3_mer = test_5_mer[1:4]\n",
    "    predict = predict_5_mer(test_3_mer)\n",
    "    if predict == code:\n",
    "        correct += 1\n",
    "print('success prediction for 6aan test data is {:.1f}%'.format(100.*correct/len(test_data_6aam)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lookup closest\n",
    "\n",
    "idea lookup 'closest' value in large dataset\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_match_5 = {}\n",
    "match_central_3 = {}\n",
    "for k, v in data:\n",
    "    exact_match_5[k] = v\n",
    "    match_central_3[k[1:4]] =\n",
    "for k, v in data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
