{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A feed-forward Neural Network for secondary structure prediction\n",
    "This notebook uses a Neural Network based on code from \n",
    "[Stevens and Boucher (2014, Python Programming for Biology CUP)](https://www.amazon.co.uk/Python-Programming-Biology-Bioinformatics-Beyond/dp/0521720095). The aim is to predict the secondary structure of a protein from its sequence. \n",
    "A Predictive network is trained using data on known secondary structure of k-mers of 5 amino-acids taken from a set of PDB structures. Three secondary structure states are defined: H, C, and S. H and S are helix and strand respectively while C is for coil which is a range of structures not with regular H-bonding pattern. In practice, secondary structure prediction has many uses, for instance in helping in the identification of functional domains ([Drozdetskiy et al., 2015](https://doi.org/10.1093/nar/gkv332)) and can be easily acheived using the JPRED4 server http://www.compbio.dundee.ac.uk/jpred4\n",
    "\n",
    "Secondary structure is much more complicated than indicated by the simple classification of this data - full details are available from the analysis of the H-bonding arrangements. The program DSSP (https://swift.cmbi.umcn.nl/gv/dssp/DSSP_3.html) is a well-tested approach to this problem. This produces a description of the secondary structure in a known protein structure. For historical reasons DSSP uses E for Strands. A related DSSR program gives RNA secondary structure.\n",
    "\n",
    "It is useful to be able to predict the secondary structure of a protein for which there is only sequence available. One approach would be to align it with homologous sequences where the structure is known. The approach here is to use the sequence in the neighbourhood of a residue as a basis for a neural network prediction. \n",
    "\n",
    "The network here is a simple three layer feed-forward one. The number of nodes in the hidden layer can be defined by the programmer. But the number of input and output nodes is defined by the sizes of the input and output data vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run this cell to import numpy \n",
    "from numpy import tanh, ones, append, array, zeros, random, sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The neural network function takes input data for the first layer of Network nodes, applies the the first weighted connections to pass the signal to the hidden layer of nodes, then applies the second weights to produce output. \n",
    "\n",
    "The output may not be optimized as the function also operates on the weighting during training. However after training the function gives predictions so takes its name from that. \n",
    "\n",
    "The weightsIn values define the strength of connection between the input nodes and the hidden nodes. Similarly weightsOut define the strengths of connection between the hidden and the output nodes. \n",
    "\n",
    "The wieghts are given as matrices with the rows indexing the nodes in a layer and the columns indexing the nodes in the other layer. \n",
    "\n",
    "The signalIn vector is the input features and an extra value of 1.0. This additional value is called the bias node which is used to tune the baseline response of the network. The baseline is the level without a meaningful signal. \n",
    "\n",
    "Setting the bias node value happens during training to adapt to the values in the input data. This means the input data don't need to pre-prepared with a mean of zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this cell to define the function\n",
    "def neuralNetPredict(inputVec, weightsIn, weightsOut):\n",
    "    \"\"\" uses the current weights in a neural network\n",
    "    to make a prediction from an input vector\n",
    "    all input and output are numpy data structures\"\"\" \n",
    "    signalIn = append(inputVec, 1.0) # input layer\n",
    "\n",
    "    prod = signalIn * weightsIn.T\n",
    "    sums = sum(prod, axis=1)\n",
    "    signalHid = tanh(sums)    # hidden    layer\n",
    "\n",
    "    prod = signalHid * weightsOut.T\n",
    "    sums = sum(prod, axis=1)\n",
    "    signalOut = tanh(sums)    # output    layer\n",
    "\n",
    "    return signalIn, signalHid, signalOut\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .T function gives the transpose of a matrix - that is the matrix with the columns turned into rows and the rows turned into columns. \n",
    "\n",
    "This is used so that the input signal gets applied to all the hidden nodes.\n",
    "\n",
    "The network applies the hyperbolic tangent function (tanh) to get the signal output from all the nodes in layer. Hyperbolic tan is a sigmoidal function that varies from -1 to 1, so it is much better than ordinary tan that runs off to infinity. \n",
    "\n",
    "<img src=\"https://mathworld.wolfram.com/images/interactive/TanhReal.gif\" width=300></img> \n",
    "\n",
    "The tanh function defines the output of that node given an input or set of inputs. As a nod to the output of neurones, which depends on an activation level across their cell membrane, the output function is called the *Activation* function.\n",
    "\n",
    "In operation only the signalOut from the output layer is of interest. But during training the response signals from the other layers are also needed to adjust the weighting scheme.\n",
    "\n",
    "### Training \n",
    "\n",
    "The weighting scheme (and gain) will be optimized by using a training dataset. \n",
    "\n",
    "The training data will be an input feature vector and a known output vector. The order of the data will be randomly shuffled to avoid bias. The number of hidden nodes needs to be specified and the number of optimization cycles. \n",
    "\n",
    "After each cycle the 'error' between the output signal of the network and the known training set output is used to adjust the network weights. The difference is combined with the *gradient* in the signal values - calculated from the tanh activation function (conveniently the gradient of tanh(sig) is 1-sig<sup>2</sup> or 1 - {sig x sig}). \n",
    "\n",
    "Early in training large difference can make the network go haywire so the speed of weight changing is damped down by a 'rate' and 'momentum' multipliers (usually the default values of 0.5 and 0.2 are good enough). \n",
    "\n",
    "More damping would mean that many more cycles would be needed for the weights to converge. \n",
    "\n",
    "The training will work back from the value of the error to adjust the weighting scheme of the network. This is called *back propagation*. \n",
    "\n",
    "The use of the gradient is crucial as it means initial adjustments will be large but then finer adjustments will be made as the optimum is approached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Run this cell to define the function\n",
    "def neuralNetTrain(trainData, numHid, steps=100, rate=0.5, momentum=0.2):\n",
    "    \"\"\" uses training data to set the weights in a simple\n",
    "    neural network, number of hidden nodes is specified\"\"\"\n",
    "    numInp = len(trainData[0][0])\n",
    "    numOut = len(trainData[0][1])\n",
    "    numInp += 1\n",
    "    minError = None\n",
    "\n",
    "    sigInp = ones(numInp)\n",
    "    sigHid = ones(numHid)\n",
    "    sigOut = ones(numOut)\n",
    "\n",
    "    wInp = random.random((numInp, numHid))-0.5\n",
    "    wOut = random.random((numHid, numOut))-0.5\n",
    "    bestWeightMatrices = (wInp, wOut)\n",
    "\n",
    "    cInp = zeros((numInp, numHid))\n",
    "    cOut = zeros((numHid, numOut))\n",
    "\n",
    "    for x, (inputs, knownOut) in enumerate(trainData):\n",
    "        trainData[x] = (array(inputs), array(knownOut))\n",
    " \n",
    "    for step in range(steps):  \n",
    "        random.shuffle(trainData) # Important to avoid bias\n",
    "        error = 0.0\n",
    " \n",
    "        for inputs, knownOut in trainData:\n",
    "            sigIn, sigHid, sigOut = neuralNetPredict(inputs, wInp, wOut)\n",
    "\n",
    "            diff = knownOut - sigOut\n",
    "            error += sum(diff * diff)\n",
    "\n",
    "            gradient = ones(numOut) - (sigOut*sigOut)\n",
    "            outAdjust = gradient * diff \n",
    "\n",
    "            diff = sum(outAdjust * wOut, axis=1)\n",
    "            gradient = ones(numHid) - (sigHid*sigHid)\n",
    "            hidAdjust = gradient * diff \n",
    "\n",
    "            # update output \n",
    "            change = outAdjust * sigHid.reshape(numHid, 1)\n",
    "            wOut += (rate * change) + (momentum * cOut)\n",
    "            cOut = change\n",
    " \n",
    "            # update input \n",
    "            change = hidAdjust * sigIn.reshape(numInp, 1)\n",
    "            wInp += (rate * change) + (momentum * cInp)\n",
    "            cInp = change\n",
    " \n",
    "        if (minError is None) or (error < minError):\n",
    "            minError = error\n",
    "            bestWeightMatrices = (wInp.copy(), wOut.copy())\n",
    "            print(\"Step: %d Error: %f\" % (step, error))\n",
    "    \n",
    "    return bestWeightMatrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the functions\n",
    "Simple data to test a network can be binary input vectors with the desired output being an 'exclusive OR' (EOR) response. This responds True if any input is true but False is both are together. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run this cell to define the test data\n",
    "testEORdata = [[[0,0], [0]],\n",
    "               [[0,1], [1]], \n",
    "               [[1,0], [1]], \n",
    "               [[1,1], [0]]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network test uses two hidden nodes - in real use several values would be tried to find the best performance.\n",
    "Run the cell below to see if the training converges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Error: 1.996738\n",
      "Step: 1 Error: 1.440903\n",
      "Step: 2 Error: 1.265039\n",
      "Step: 4 Error: 1.169860\n",
      "Step: 22 Error: 1.118598\n",
      "Step: 28 Error: 1.083179\n",
      "Step: 32 Error: 0.994186\n",
      "Step: 35 Error: 0.975448\n",
      "Step: 37 Error: 0.935896\n",
      "Step: 38 Error: 0.900493\n",
      "Step: 39 Error: 0.893845\n",
      "Step: 40 Error: 0.861554\n",
      "Step: 44 Error: 0.823851\n",
      "Step: 45 Error: 0.821889\n",
      "Step: 51 Error: 0.817290\n",
      "Step: 53 Error: 0.796468\n",
      "Step: 56 Error: 0.789788\n",
      "Step: 59 Error: 0.750791\n",
      "Step: 62 Error: 0.731600\n",
      "Step: 65 Error: 0.711318\n",
      "Step: 66 Error: 0.706699\n",
      "Step: 70 Error: 0.677616\n",
      "Step: 71 Error: 0.638384\n",
      "Step: 73 Error: 0.589361\n",
      "Step: 80 Error: 0.492021\n",
      "Step: 83 Error: 0.469774\n",
      "Step: 85 Error: 0.442459\n",
      "Step: 86 Error: 0.298744\n",
      "Step: 92 Error: 0.226552\n",
      "Step: 95 Error: 0.166673\n",
      "Step: 106 Error: 0.137360\n",
      "Step: 107 Error: 0.087597\n",
      "Step: 113 Error: 0.067742\n",
      "Step: 116 Error: 0.059237\n",
      "Step: 125 Error: 0.057300\n",
      "Step: 134 Error: 0.054089\n",
      "Step: 137 Error: 0.050076\n",
      "Step: 140 Error: 0.045679\n",
      "Step: 141 Error: 0.037000\n",
      "Step: 149 Error: 0.029754\n",
      "Step: 152 Error: 0.024830\n",
      "Step: 159 Error: 0.021912\n",
      "Step: 168 Error: 0.019060\n",
      "Step: 173 Error: 0.017731\n",
      "Step: 175 Error: 0.014968\n",
      "Step: 192 Error: 0.011636\n",
      "Step: 194 Error: 0.010591\n",
      "Step: 204 Error: 0.009710\n",
      "Step: 213 Error: 0.009191\n",
      "Step: 216 Error: 0.009104\n",
      "Step: 219 Error: 0.008842\n",
      "Step: 220 Error: 0.008637\n",
      "Step: 225 Error: 0.008365\n",
      "Step: 226 Error: 0.008220\n",
      "Step: 228 Error: 0.007902\n",
      "Step: 230 Error: 0.007537\n",
      "Step: 232 Error: 0.006057\n",
      "Step: 247 Error: 0.005991\n",
      "Step: 248 Error: 0.005423\n",
      "Step: 252 Error: 0.005100\n",
      "Step: 269 Error: 0.004988\n",
      "Step: 272 Error: 0.004242\n",
      "Step: 291 Error: 0.004200\n",
      "Step: 296 Error: 0.003978\n",
      "Step: 301 Error: 0.003608\n",
      "Step: 325 Error: 0.003486\n",
      "Step: 326 Error: 0.003193\n",
      "Step: 329 Error: 0.002942\n",
      "Step: 337 Error: 0.002848\n",
      "Step: 349 Error: 0.002698\n",
      "Step: 366 Error: 0.002573\n",
      "Step: 372 Error: 0.002501\n",
      "Step: 376 Error: 0.002298\n",
      "Step: 396 Error: 0.002282\n",
      "Step: 406 Error: 0.002167\n",
      "Step: 409 Error: 0.002009\n",
      "Step: 429 Error: 0.001981\n",
      "Step: 441 Error: 0.001899\n",
      "Step: 446 Error: 0.001899\n",
      "Step: 451 Error: 0.001865\n",
      "Step: 456 Error: 0.001809\n",
      "Step: 462 Error: 0.001752\n",
      "Step: 468 Error: 0.001745\n",
      "Step: 471 Error: 0.001739\n",
      "Step: 474 Error: 0.001654\n",
      "Step: 483 Error: 0.001592\n",
      "Step: 491 Error: 0.001564\n",
      "Step: 507 Error: 0.001504\n",
      "Step: 522 Error: 0.001492\n",
      "Step: 525 Error: 0.001490\n",
      "Step: 526 Error: 0.001455\n",
      "Step: 528 Error: 0.001449\n",
      "Step: 532 Error: 0.001448\n",
      "Step: 536 Error: 0.001446\n",
      "Step: 537 Error: 0.001433\n",
      "Step: 540 Error: 0.001416\n",
      "Step: 543 Error: 0.001353\n",
      "Step: 547 Error: 0.001307\n",
      "Step: 559 Error: 0.001300\n",
      "Step: 564 Error: 0.001293\n",
      "Step: 572 Error: 0.001190\n",
      "Step: 604 Error: 0.001172\n",
      "Step: 622 Error: 0.001151\n",
      "Step: 623 Error: 0.001126\n",
      "Step: 627 Error: 0.001097\n",
      "Step: 636 Error: 0.001073\n",
      "Step: 640 Error: 0.001065\n",
      "Step: 648 Error: 0.001017\n",
      "Step: 658 Error: 0.000985\n",
      "Step: 672 Error: 0.000959\n",
      "Step: 687 Error: 0.000934\n",
      "Step: 707 Error: 0.000933\n",
      "Step: 709 Error: 0.000910\n",
      "Step: 720 Error: 0.000893\n",
      "Step: 734 Error: 0.000891\n",
      "Step: 738 Error: 0.000878\n",
      "Step: 746 Error: 0.000868\n",
      "Step: 750 Error: 0.000846\n",
      "Step: 756 Error: 0.000841\n",
      "Step: 759 Error: 0.000811\n",
      "Step: 793 Error: 0.000806\n",
      "Step: 804 Error: 0.000761\n",
      "Step: 817 Error: 0.000748\n",
      "Step: 823 Error: 0.000743\n",
      "Step: 844 Error: 0.000727\n",
      "Step: 858 Error: 0.000727\n",
      "Step: 862 Error: 0.000711\n",
      "Step: 887 Error: 0.000710\n",
      "Step: 888 Error: 0.000691\n",
      "Step: 899 Error: 0.000674\n",
      "Step: 905 Error: 0.000665\n",
      "Step: 913 Error: 0.000656\n",
      "Step: 916 Error: 0.000641\n",
      "Step: 940 Error: 0.000618\n",
      "Step: 980 Error: 0.000615\n",
      "Step: 993 Error: 0.000610\n",
      "Step: 997 Error: 0.000599\n"
     ]
    }
   ],
   "source": [
    "## Run this cell to train the network\n",
    "wMatrixIn, wMatrixOut = neuralNetTrain(testEORdata, 2, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here quite good convergence has occurred and there is no oscillation. Perhaps you can see that the initial steps are giving large changes in the error while later on there are smaller and smaller changes. This is owing to the effect of the gradient calculation. The changes in the actual weights are not printed, but will follow the same trend.\n",
    "\n",
    "The output weight matrices can then be run on test data for evaluation. Test data should be inputs with known output but which were not included in the training set. \n",
    "\n",
    "Obviously it is not possible to give any new data for the EOR function as the training set covered all possible responses!\n",
    "\n",
    "But the trained network should be able to do a reasonable job on the training set.\n",
    "Run the following cell to compare the output of the network with the actual values of the training set outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input [0 0]  should have output  [0] actual output -0.000\n",
      "input [1 1]  should have output  [0] actual output 0.014\n",
      "input [0 1]  should have output  [1] actual output 0.984\n",
      "input [1 0]  should have output  [1] actual output 0.984\n"
     ]
    }
   ],
   "source": [
    "##Run this cell to test the network\n",
    "for inputs, knownOut in testEORdata:\n",
    "    sIn, sHid, sOut =    neuralNetPredict(array(inputs), wMatrixIn, wMatrixOut)\n",
    "    print('input', inputs, ' should have output ', knownOut, 'actual output {:.3f}'.format(sOut[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple feature vectors for sequence data\n",
    "A simple numbering scheme is used to convert to the sequence alphabet to a numeric form as an input vector. For proteins that is number from 1 to 20 from the list of one-letter codes.\n",
    "\n",
    "k-mers with k=5 are used. Only the output for the middle residue is required but the network will use the neighbours to predict the secondary structure of the middle one.  \n",
    "\n",
    "Although static k-mer are used for training in practice a prediction in a moving 5-mer window could be implemented. \n",
    "\n",
    "The possible outputs are also coded as integers for the more restricted alphabet of H, C, and S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run this cell to define the dictionaries for the vectors\n",
    "aminoAcids = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "aaIndexDict = {}\n",
    "for i, aa in enumerate(aminoAcids):\n",
    "        aaIndexDict[aa] = i\n",
    "\n",
    "ssIndexDict = {}\n",
    "ssCodes = 'HCS'\n",
    "for i, code in enumerate(ssCodes):\n",
    "        ssIndexDict[code] = i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a very limited set of training data. It shows the raw format which is a 5-mer string and the secondary structure that was observed for the central residue of this in at least one PDB structure. \n",
    "\n",
    "The actual structure is a simplified output from the DSSP program mentioned in the introduction. DSSP acutally distinguishes more structures that the three here - for example there are other kinds of helix. But these complications are not dealt with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run this cell to define the training set\n",
    "seqSecStrucData = [('ADTLL','S'),\n",
    "                   ('DTLLI','S'),\n",
    "                   ('TLLIL','S'),\n",
    "                   ('LLILG','S'),\n",
    "                   ('LILGD','S'),\n",
    "                   ('ILGDS','S'),\n",
    "                   ('LGDSL','C'),\n",
    "                   ('GDSLS','H'),\n",
    "                   ('DSLSA','H'),\n",
    "                   ('SLSAG','H'),\n",
    "                   ('LSAGY','H'),\n",
    "                   ('SAGYR','C'),\n",
    "                   ('AGYRM','C'),\n",
    "                   ('GYRMS','C'),\n",
    "                   ('YRMSA','C'),\n",
    "                   ('RMSAS','C')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data has to be converted to the numerical code. Here is a function to to that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run this cell to define the function\n",
    "def convertSeqToVector(seq, indexDict):\n",
    "    \"\"\"converts a one-letter sequence to numerical\n",
    "    coding for neural network calculations\"\"\"   \n",
    "    numLetters = len(indexDict)\n",
    "    vector = [0.0] * len(seq) * numLetters\n",
    "\n",
    "    for pos, letter in enumerate(seq):\n",
    "        index = pos * numLetters + indexDict[letter]    \n",
    "        vector[index] = 1.0\n",
    "\n",
    "    return vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data is prepared with this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Run this cell to create the training data\n",
    "trainingData = []\n",
    "for seq, ss in seqSecStrucData:\n",
    " \n",
    "        inputVec = convertSeqToVector(seq, aaIndexDict)\n",
    "        outputVec = convertSeqToVector(ss, ssIndexDict)\n",
    " \n",
    "        trainingData.append( (inputVec, outputVec) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then the network is trained. Here there are 3 hidden nodes specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Error: 19.738710\n",
      "Step: 1 Error: 16.156241\n",
      "Step: 2 Error: 12.451463\n",
      "Step: 3 Error: 8.126981\n",
      "Step: 6 Error: 7.768595\n",
      "Step: 7 Error: 6.606724\n",
      "Step: 8 Error: 1.961691\n",
      "Step: 11 Error: 1.001516\n",
      "Step: 13 Error: 0.793545\n",
      "Step: 15 Error: 0.525485\n",
      "Step: 16 Error: 0.500134\n",
      "Step: 17 Error: 0.460263\n",
      "Step: 18 Error: 0.387424\n",
      "Step: 25 Error: 0.309206\n",
      "Step: 26 Error: 0.239182\n",
      "Step: 27 Error: 0.137848\n",
      "Step: 31 Error: 0.124080\n",
      "Step: 33 Error: 0.108376\n",
      "Step: 48 Error: 0.076003\n",
      "Step: 53 Error: 0.072243\n",
      "Step: 54 Error: 0.049964\n",
      "Step: 55 Error: 0.049776\n",
      "Step: 59 Error: 0.032444\n",
      "Step: 103 Error: 0.021625\n",
      "Step: 134 Error: 0.016265\n",
      "Step: 135 Error: 0.011543\n",
      "Step: 164 Error: 0.009952\n",
      "Step: 168 Error: 0.009446\n",
      "Step: 221 Error: 0.007421\n",
      "Step: 224 Error: 0.006656\n",
      "Step: 225 Error: 0.004523\n",
      "Step: 299 Error: 0.004444\n",
      "Step: 311 Error: 0.004356\n",
      "Step: 315 Error: 0.004092\n",
      "Step: 317 Error: 0.004017\n",
      "Step: 320 Error: 0.003200\n",
      "Step: 349 Error: 0.002690\n",
      "Step: 377 Error: 0.002686\n",
      "Step: 378 Error: 0.002642\n",
      "Step: 379 Error: 0.002212\n",
      "Step: 383 Error: 0.001852\n",
      "Step: 418 Error: 0.001754\n",
      "Step: 427 Error: 0.001672\n",
      "Step: 437 Error: 0.001643\n",
      "Step: 708 Error: 0.001364\n",
      "Step: 709 Error: 0.000969\n",
      "Step: 758 Error: 0.000869\n",
      "Step: 888 Error: 0.000827\n",
      "Step: 894 Error: 0.000699\n"
     ]
    }
   ],
   "source": [
    "   wMatrixIn, wMatrixOut = neuralNetTrain(trainingData, 3, 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You will see that this training has converged nicely. The only problem is that it was for a very restricted set of sequence data. \n",
    "\n",
    "There are 3 x 20 = 60 theoretical combinations of amino acid residues with secondary structure states. But for particular residues some of these are favoured and others disfavoured. \n",
    "\n",
    "For each residue there will be 20^4 = 160 000 different contexts that then could possibly occur in. Although some of the resulting 5-mers are actually quite rare in structured proteins. \n",
    "\n",
    "All the same, it would be good to have a larger training set. It is better to have some rare examples of residue state combinations although, of course, the network will not have to predict them frequently.\n",
    "\n",
    "One thing to remember is to retain some test examples - where the answer is known but which are not in the training set. \n",
    "\n",
    "In its current, poorly-trained state, the network is still able to make a reasonable predictions. But only if the test is clearly related to examples that it has seen. testSecStrucSeq here is very similar to examples in the seqSecStrucData training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test prediction: H\n"
     ]
    }
   ],
   "source": [
    "##Run this cell to test for a new 5-mer\n",
    "testSecStrucSeq = 'DLLSA'\n",
    "testSecStrucVec = convertSeqToVector(testSecStrucSeq, aaIndexDict)\n",
    "testSecStrucArray = array( [testSecStrucVec,] )\n",
    "\n",
    "sIn, sHid, sOut =    neuralNetPredict(testSecStrucArray, wMatrixIn, wMatrixOut)\n",
    "index = sOut.argmax()\n",
    "print(\"Test prediction: %s\" % ssCodes[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# making a prediction of the secondary structure for PDB 6aam\n",
    "\n",
    "Lets use the recent PDB structure 6AAM \"Non-receptor tyrosine-protein kinase TYK2\" \n",
    "https://www.ebi.ac.uk/pdbe/entry/pdb/6aam/\n",
    "    \n",
    "as an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "298\n"
     ]
    }
   ],
   "source": [
    "# 6aam sequence from https://www.ebi.ac.uk/pdbe/entry/pdb/6aam/protein/1\n",
    "sequence_6aam = ('GPGDPTVFHKRYLKKIRDLGEGHFGKVSLYCYDPTNDGTGEMVAVKALKADAGP'\n",
    "                 'QHRSGWKQEIDILRTLYHEHIIKYKGCCEDAGAASLQLVMEYVPLGSLRDYLPR'\n",
    "                 'HSIGLAQLLLFAQQICEGMAYLHAQHYIHRNLAARNVLLDNDRLVKIGDFGLAK'\n",
    "                 'AVPEGHEYYRVREDGDSPVFWYAPECLKEYKFYYASDVWSFGVTLYELLTHCDS'\n",
    "                 'SQSPPTKFLELIGLAQGQMTVLRLTELLERGERLPRPDKCPAEVYHLMKNCWET'\n",
    "                 'EASFRPTFENLIPILKTVHEKYQGQAPS')\n",
    "print(len(sequence_6aam))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..SCSSSSCCCCSSCCSSSSCSSCSCCHHHCCCSSSCSSCCHCSCCSSSHCHCCSSCHCCCCSSSSSSSSSCCSCSCSCCCCHCHCCCSSSSSCCSCSSSHHHCSSSCCHHSSSSSSSSSCSSSCCCCSSSCCCCCCCSCSCCCSSSSSCSSSSCSSHHCHCCCSSSCCCCCCCSSSCHHSHCCCCSSSHCCCCCCCCHHCHHSCSSSSSSSSCCHHHHHSSSSSSSSSSHCSCSSSSSSSSSSSCCCSCSCSSHHCCSCSCCSCSCCCSCCCCCCSCCSSSSSSSSSCSCSCSCH..\n",
      "298\n"
     ]
    }
   ],
   "source": [
    "def predict_for_sequence(sequence):\n",
    "    prediction = []\n",
    "    for ires, residue in enumerate(sequence):\n",
    "        if ires < 2 or ires > len(sequence) - 3:\n",
    "            this_prediction = '.'\n",
    "        else:\n",
    "            fivemer = sequence[ires-2:ires+3]\n",
    "            testSecStrucVec = convertSeqToVector(fivemer, aaIndexDict)\n",
    "            testSecStrucArray = array( [testSecStrucVec,] )\n",
    "            sIn, sHid, sOut =    neuralNetPredict(testSecStrucArray, wMatrixIn, wMatrixOut)\n",
    "            index = sOut.argmax()\n",
    "            this_prediction = ssCodes[index]\n",
    "        prediction.append(this_prediction)\n",
    "    return ''.join(prediction)\n",
    "\n",
    "initial_predict_6aam = predict_for_sequence(sequence_6aam)\n",
    "print(initial_predict_6aam)\n",
    "print(len(initial_predict_6aam))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "298\n",
      "CCCCCCSCCCCCSSSSSSCCCCCCCSSSSSSSCCCCCCCCSSSSSSSCCCCCCCCHHHHHHHHHHHHHHCCCCCSCCSSSSSSSCCCCSSSSSSSCCCCCSHHHHCCCSCCCHHHHHHHHHHHHHHHHHHHHCCSSCSCCSCCCSSSSSCCSSSSCCCCCCSSCCCCCCCCCCCCCCCCCCCCCCHHHHHHCCCCHHHHHHHHHHHHHHHHCCCCCCCSHHHHHHHHHCSCCCCCHHHHHHHHHHCCCCCCCCCCCCHHHHHHHHHHCCSSCCCSCCHHHHHHHHHHHHHHHHCCCCC\n"
     ]
    }
   ],
   "source": [
    "# from https://cdn.rcsb.org/etl/kabschSander/ss_dis.txt.gz\n",
    "dssp_result_for_6aam  = \"\"\">6AAM:A:secstr\n",
    "      B  GGGEEEEEE       EEEEEEE TT     EEEEEEE      TTHHHHHHHHHHHHHH   TTB\n",
    "  EEEEEEEGGGTEEEEEEE  TT BHHHHGGGS   HHHHHHHHHHHHHHHHHHHHTTEE S  SGGGEEEEET\n",
    "TEEEE   TT EE                 GGG  HHHHHH    HHHHHHHHHHHHHHHHTTT GGGSHHHHHH\n",
    "HHH S  TT HHHHHHHHHHTT      TT  HHHHHHHHHHT SSGGGS  HHHHHHHHHHHHHHHH     \n",
    "\"\"\"\n",
    "dssp_result_for_6aam = dssp_result_for_6aam.splitlines()\n",
    "dssp_result_for_6aam.pop(0)\n",
    "dssp_result_for_6aam = ''.join(dssp_result_for_6aam)\n",
    "# need to convert DSSP code to the 3-category helix, strand, coil.\n",
    "# Use mapping \n",
    "# helices H, C, I go to H\n",
    "# strands E & bridges B go to S\n",
    "# everything else got to C\n",
    "translation = str.maketrans('HCIEB GT', 'HHHSSCCC')\n",
    "dssp_result_for_6aam = dssp_result_for_6aam.translate(translation)\n",
    "print(len(dssp_result_for_6aam))\n",
    "print(dssp_result_for_6aam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CCCCCCSCCCCCSSSSSSCCCCCCCSSSSSSSCCCCCCCCSSSSSSSCCC\n",
      "   |  | ||||||  ||  |  |        |   |  |   |  |   \n",
      "..SCSSSSCCCCSSCCSSSSCSSCSCCHHHCCCSSSCSSCCHCSCCSSSH\n",
      "\n",
      "CCCCCHHHHHHHHHHHHHHCCCCCSCCSSSSSSSCCCCSSSSSSSCCCCC\n",
      "| ||   |             ||   ||       ||||||||   |   \n",
      "CHCCSSCHCCCCSSSSSSSSSCCSCSCSCCCCHCHCCCSSSSSCCSCSSS\n",
      "\n",
      "SHHHHCCCSCCCHHHHHHHHHHHHHHHHHHHHCCSSCSCCSCCCSSSSSC\n",
      " ||    |                        ||  |  |||||||||||\n",
      "HHHCSSSCCHHSSSSSSSSSCSSSCCCCSSSCCCCCCCSCSCCCSSSSSC\n",
      "\n",
      "CSSSSCCCCCCSSCCCCCCCCCCCCCCCCCCCCCCHHHHHHCCCCHHHHH\n",
      " |||     |   |   |||||||   |    |||    | ||||   ||\n",
      "SSSSCSSHHCHCCCSSSCCCCCCCSSSCHHSHCCCCSSSHCCCCCCCCHH\n",
      "\n",
      "HHHHHHHHHHHCCCCCCCSHHHHHHHHHCSCCCCCHHHHHHHHHHCCCCC\n",
      " ||          ||    |         | | |           ||| |\n",
      "CHHSCSSSSSSSSCCHHHHHSSSSSSSSSSHCSCSSSSSSSSSSSCCCSC\n",
      "\n",
      "CCCCCCCHHHHHHHHHHCCSSCCCSCCHHHHHHHHHHHHHHHHCCCCC\n",
      " |    |          ||| ||| |                  |   \n",
      "SCSSHHCCSCSCCSCSCCCSCCCCCCSCCSSSSSSSSSCSCSCSCH..\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def highlight_line(first_seq, second_seq):\n",
    "    \"\"\" \n",
    "    for the two sequences returns a line where matching letters are \n",
    "    highlighted with | except if the letter are a gap\n",
    "    \"\"\"\n",
    "    joins = ['|' if a == b and a != '-' else ' ' for a, b in zip(first_seq, second_seq)]\n",
    "    return ''.join(joins)\n",
    "def print_alignment(seq_a, seq_b):\n",
    "    len_split = 50\n",
    "    n_splits = len(seq_a)//len_split + 1\n",
    "    for i_split in range(n_splits):\n",
    "        start = len_split*i_split\n",
    "        end = start + len_split\n",
    "        part_a = seq_a[start:end]\n",
    "        part_b = seq_b[start:end]\n",
    "        print(part_a)\n",
    "        print(highlight_line(part_a, part_b))\n",
    "        print(part_b)\n",
    "        print()\n",
    "\n",
    "print_alignment(dssp_result_for_6aam, initial_predict_6aam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage correct predictions: 32.2%\n"
     ]
    }
   ],
   "source": [
    "def success(seq_a, seq_b):\n",
    "    same = 0\n",
    "    different = 0\n",
    "    for let_a, let_b in zip(seq_a, seq_b):\n",
    "        if let_a == let_b:\n",
    "            same += 1\n",
    "        else:\n",
    "            different += 1\n",
    "    return same/(same + different)\n",
    "print('percentage correct predictions: {:.1f}%'.format(100*success(dssp_result_for_6aam, initial_predict_6aam)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inporting a larger training set\n",
    "A much larger training set is provided as a comma separated file called:\n",
    "\"PDB_protein_secondary_5mers.csv\". \n",
    "\n",
    "    from csv import reader #may help here\n",
    "Can you create a training set vector data structure from this and use it to train the network?\n",
    "If you would like some examples of test data, here are some examples from the recently-determined PDB 6aam.pdb\n",
    "\n",
    "    S: EMVAV, KVSKY, YKGCC\n",
    "    H: LAQLL, ICEGM, ASVDW\n",
    "    C: ERLPR, GDFGL, YKFYY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "have read 26242 lines from csv file\n",
      "first 5 lines [['MGKMY', 'S'], ['YGIPQ', 'C'], ['KMWTY', 'H'], ['YRLRK', 'H'], ['NSVSV', 'S']]\n"
     ]
    }
   ],
   "source": [
    "# answer\n",
    "from csv import reader\n",
    "with open('PDB_protein_secondary_5mers.csv') as csv_file:\n",
    "    csv_reader = reader(csv_file, delimiter=',')\n",
    "    data = list(csv_reader)\n",
    "print('have read', len(data), 'lines from csv file')\n",
    "print('first 5 lines', data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = seqSecStrucData + data[:200]\n",
    "##Run this cell to create the training data\n",
    "trainingData = []\n",
    "for seq, ss in training_set:\n",
    " \n",
    "        inputVec = convertSeqToVector(seq, aaIndexDict)\n",
    "        outputVec = convertSeqToVector(ss, ssIndexDict)\n",
    " \n",
    "        trainingData.append( (inputVec, outputVec) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Error: 250.602497\n",
      "Step: 15 Error: 248.776633\n",
      "Step: 16 Error: 238.671113\n",
      "Step: 17 Error: 233.067781\n",
      "Step: 18 Error: 219.627317\n",
      "Step: 21 Error: 188.787130\n",
      "Step: 32 Error: 188.786146\n",
      "Step: 33 Error: 180.054240\n",
      "Step: 37 Error: 173.220385\n",
      "Step: 38 Error: 160.614172\n",
      "Step: 44 Error: 159.688608\n",
      "Step: 46 Error: 152.873979\n",
      "Step: 71 Error: 148.366355\n",
      "Step: 79 Error: 140.795987\n",
      "Step: 109 Error: 138.691307\n",
      "Step: 125 Error: 135.592580\n",
      "Step: 128 Error: 134.971765\n",
      "Step: 133 Error: 129.121363\n",
      "Step: 155 Error: 127.287103\n",
      "Step: 184 Error: 123.494047\n",
      "Step: 193 Error: 119.126797\n",
      "Step: 237 Error: 118.153780\n",
      "Step: 243 Error: 117.131650\n",
      "Step: 356 Error: 114.343184\n",
      "Step: 521 Error: 111.513306\n",
      "Step: 622 Error: 110.264015\n",
      "Step: 1331 Error: 106.479068\n"
     ]
    }
   ],
   "source": [
    " wMatrixIn, wMatrixOut = neuralNetTrain(trainingData, 4, 3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test prediction: C\n"
     ]
    }
   ],
   "source": [
    "##Run this cell to test for a new 5-mer\n",
    "testSecStrucSeq = 'DLLSA'\n",
    "testSecStrucVec = convertSeqToVector(testSecStrucSeq, aaIndexDict)\n",
    "testSecStrucArray = array( [testSecStrucVec,] )\n",
    "\n",
    "sIn, sHid, sOut =    neuralNetPredict(testSecStrucArray, wMatrixIn, wMatrixOut)\n",
    "index = sOut.argmax()\n",
    "print(\"Test prediction: %s\" % ssCodes[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..CCCCCCHHCHHHHHCCCCCCCCCCCHHHCCCCCCCCCCCHHCCHHHHCCCCCCCCCHCCHCHCCCCHHHCCHHCCCHCCCCCCCCCHCHCCCCCCCCCCCCCCCCCCHCCCCCCHCHHCCCCCCHCCCHCCCHCCCHCHCCCCCHCCCCCHCCCCCCCHCHCCCCCCHCCHCCCCCCCCCHCCCCCHHCHHCHCCCHCCCCCCCCCCHHCCHCHCCCCCCCHHHCCCCCCCCCCCCHCHCHHCCCCCCCCCCCCCCCHCCHCHCHCCHCHCCCCCCHHCCCCCCCHCHHCCCCC..\n",
      "percentage correct predictions: 47.3%\n"
     ]
    }
   ],
   "source": [
    "def predict_for_sequence(sequence):\n",
    "    prediction = []\n",
    "    for ires, residue in enumerate(sequence):\n",
    "        if ires < 2 or ires > len(sequence) - 3:\n",
    "            this_prediction = '.'\n",
    "        else:\n",
    "            fivemer = sequence[ires-2:ires+3]\n",
    "            testSecStrucVec = convertSeqToVector(fivemer, aaIndexDict)\n",
    "            testSecStrucArray = array( [testSecStrucVec,] )\n",
    "            sIn, sHid, sOut =    neuralNetPredict(testSecStrucArray, wMatrixIn, wMatrixOut)\n",
    "            index = sOut.argmax()\n",
    "            this_prediction = ssCodes[index]\n",
    "        prediction.append(this_prediction)\n",
    "    return ''.join(prediction)\n",
    "\n",
    "second_predict_6aam = predict_for_sequence(sequence_6aam)\n",
    "print(second_predict_6aam)\n",
    "print('percentage correct predictions: {:.1f}%'.format(100*success(dssp_result_for_6aam, second_predict_6aam)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
